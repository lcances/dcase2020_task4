{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:15<00:00, 477.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    validation_ratio=0.2,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "Loading dataset: train, subset: weak\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "# manager.add_subset(\"synthetic20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> Creating new train / validation split\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> validation ratio : 0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.split_train_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%  setup augmentation and create pytorch dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MultipleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(MultipleDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        for d in self.datasets:\n",
    "            assert not isinstance(d, IterableDataset), \"ConcatDataset does not support IterableDataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [d[sample_idx] for d in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 243)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): ReLU6()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_features.Conv2d_3         [32, 32, 1, 1]    [1, 32, 1, 53]  1.056k   \n",
      "16_features.ReLU6_4                       -    [1, 32, 1, 53]       -   \n",
      "17_classifier.Flatten_0                   -         [1, 1696]       -   \n",
      "18_classifier.Dropout_1                   -         [1, 1696]       -   \n",
      "19_classifier.Linear_2           [1696, 10]           [1, 10]  16.97k   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_features.Conv2d_3           54.272k  \n",
      "16_features.ReLU6_4                  -  \n",
      "17_classifier.Flatten_0              -  \n",
      "18_classifier.Dropout_1              -  \n",
      "19_classifier.Linear_2          16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             37.034k\n",
      "Trainable params         37.034k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.663008M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "# criterion & optimizers\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "binacc_func = BinaryAccuracy()\n",
    "f_func = FScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak   - metrics:  Weak acc  | Weak F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<10.10}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"metrics: \", \"Weak acc \", \"Weak F1 \", \"Time\"\n",
    ")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    binacc_func.reset()\n",
    "    f_func.reset()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X_weak, y_weak) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X_weak, y_weak = X_weak.cuda().float(), y_weak[0].cuda().float()\n",
    "\n",
    "        logits = model(X_weak)\n",
    "        \n",
    "        loss = criterion(logits, y_weak)\n",
    "        \n",
    "        # calc metrics\n",
    "        pred = torch.sigmoid(logits)\n",
    "        binacc = binacc_func(pred, y_weak)\n",
    "        fscore = f_func(pred, y_weak)\n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        # logs\n",
    "        print(value_form.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            int(100 * (i + 1) / len(training_loader)),\n",
    "            \"\", loss.item(),\n",
    "            \"\", binacc, fscore,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")\n",
    "        \n",
    "    # tensorboard logs\n",
    "    tensorboard.add_scalar(\"train/loss\", loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", binacc, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/precision\", f_func.precision, epoch)\n",
    "    tensorboard.add_scalar(\"train/recall\", f_func.recall, epoch)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "    binacc_func.reset()\n",
    "    f_func.reset()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X_weak, y_weak) in enumerate(val_loader):\n",
    "            X_weak, y_weak = X_weak.cuda().float(), y_weak[0].cuda().float()\n",
    "\n",
    "            logits = model(X_weak)\n",
    "\n",
    "            loss = criterion(logits, y_weak)\n",
    "\n",
    "            # calc metrics\n",
    "            pred = torch.sigmoid(logits)\n",
    "            binacc = binacc_func(pred, y_weak)\n",
    "            fscore = f_func(pred, y_weak)\n",
    "\n",
    "            # logs\n",
    "            print(value_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", loss.item(),\n",
    "                \"\", binacc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/loss\", loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/acc\", binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/precision\", f_func.precision, epoch)\n",
    "        tensorboard.add_scalar(\"val/recall\", f_func.recall, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak   - metrics:  Weak acc  | Weak F1  - Time  \n",
      "\n",
      "Training 1      - 100    -          0.3416 -           0.8454    | 0.0565   - 14.1074\n",
      "--------------------------------------------------     0.8503    | 0.0573   - 3.0254\n",
      "\n",
      "Training 2      - 100    -          0.3938 -           0.8554    | 0.0923   - 0.4879\n",
      "--------------------------------------------------     0.8565    | 0.2980   - 0.0390\n",
      "\n",
      "Training 3      - 100    -          0.2879 -           0.8590    | 0.1447   - 0.4591\n",
      "--------------------------------------------------     0.8549    | 0.1423   - 0.0392\n",
      "\n",
      "Training 4      - 100    -          0.2753 -           0.8668    | 0.2402   - 0.4597\n",
      "--------------------------------------------------     0.8698    | 0.3746   - 0.0389\n",
      "\n",
      "Training 5      - 100    -          0.3121 -           0.8705    | 0.3019   - 0.4508\n",
      "--------------------------------------------------     0.8794    | 0.4096   - 0.0364\n",
      "\n",
      "Training 6      - 100    -          0.2963 -           0.8765    | 0.3555   - 0.4465\n",
      "--------------------------------------------------     0.8638    | 0.2883   - 0.0363\n",
      "\n",
      "Training 7      - 100    -          0.4298 -           0.8798    | 0.3820   - 0.4513\n",
      "--------------------------------------------------     0.8846    | 0.4625   - 0.0362\n",
      "\n",
      "Training 8      - 100    -          0.3148 -           0.8809    | 0.4132   - 0.4491\n",
      "--------------------------------------------------     0.8795    | 0.4438   - 0.0365\n",
      "\n",
      "Training 9      - 100    -          0.3830 -           0.8827    | 0.4245   - 0.4469\n",
      "--------------------------------------------------     0.8747    | 0.4035   - 0.0364\n",
      "\n",
      "Training 10     - 100    -          0.2836 -           0.8883    | 0.4662   - 0.4470\n",
      "--------------------------------------------------     0.8828    | 0.4421   - 0.0372\n",
      "\n",
      "Training 11     - 100    -          0.2575 -           0.8928    | 0.5040   - 0.4488\n",
      "--------------------------------------------------     0.8692    | 0.4678   - 0.0359\n",
      "\n",
      "Training 12     - 100    -          0.3300 -           0.8895    | 0.4854   - 0.4462\n",
      "--------------------------------------------------     0.8664    | 0.4516   - 0.0366\n",
      "\n",
      "Training 13     - 100    -          0.3081 -           0.8934    | 0.4958   - 0.4474\n",
      "--------------------------------------------------     0.8751    | 0.4694   - 0.0363\n",
      "\n",
      "Training 14     - 100    -          0.1956 -           0.8925    | 0.5060   - 0.4478\n",
      "--------------------------------------------------     0.8862    | 0.5458   - 0.0365\n",
      "\n",
      "Training 15     - 100    -          0.2440 -           0.8954    | 0.5254   - 0.4455\n",
      "--------------------------------------------------     0.8925    | 0.5356   - 0.0361\n",
      "\n",
      "Training 16     - 100    -          0.2293 -           0.8951    | 0.5225   - 0.4588\n",
      "--------------------------------------------------     0.8881    | 0.5389   - 0.0400\n",
      "\n",
      "Training 17     - 100    -          0.3364 -           0.8998    | 0.5536   - 0.4623\n",
      "--------------------------------------------------     0.8887    | 0.5116   - 0.0388\n",
      "\n",
      "Training 18     - 100    -          0.2441 -           0.8996    | 0.5512   - 0.4662\n",
      "--------------------------------------------------     0.8883    | 0.4866   - 0.0399\n",
      "\n",
      "Training 19     - 100    -          0.3105 -           0.8988    | 0.5505   - 0.4627\n",
      "--------------------------------------------------     0.8796    | 0.4639   - 0.0390\n",
      "\n",
      "Training 20     - 100    -          0.2530 -           0.8986    | 0.5400   - 0.4559\n",
      "--------------------------------------------------     0.8957    | 0.5398   - 0.0363\n",
      "\n",
      "Training 21     - 100    -          0.2631 -           0.9025    | 0.5643   - 0.4560\n",
      "--------------------------------------------------     0.8902    | 0.5245   - 0.0364\n",
      "\n",
      "Training 22     - 100    -          0.3511 -           0.9015    | 0.5730   - 0.4535\n",
      "--------------------------------------------------     0.8872    | 0.4680   - 0.0368\n",
      "\n",
      "Training 23     - 100    -          0.2209 -           0.9029    | 0.5715   - 0.4532\n",
      "--------------------------------------------------     0.8823    | 0.5103   - 0.0372\n",
      "\n",
      "Training 24     - 100    -          0.2685 -           0.9034    | 0.5751   - 0.4531\n",
      "--------------------------------------------------     0.8846    | 0.5040   - 0.0370\n",
      "\n",
      "Training 25     - 100    -          0.2639 -           0.9060    | 0.5985   - 0.4538\n",
      "--------------------------------------------------     0.8956    | 0.5576   - 0.0364\n",
      "\n",
      "Training 26     - 100    -          0.1784 -           0.9083    | 0.6070   - 0.4529\n",
      "--------------------------------------------------     0.8881    | 0.5292   - 0.0365\n",
      "\n",
      "Training 27     - 100    -          0.2458 -           0.9027    | 0.5741   - 0.4536\n",
      "--------------------------------------------------     0.8911    | 0.5422   - 0.0372\n",
      "\n",
      "Training 28     - 100    -          0.3042 -           0.9101    | 0.6206   - 0.4576\n",
      "--------------------------------------------------     0.8871    | 0.5315   - 0.0364\n",
      "\n",
      "Training 29     - 100    -          0.2069 -           0.9088    | 0.6167   - 0.4537\n",
      "--------------------------------------------------     0.8863    | 0.5161   - 0.0373\n",
      "\n",
      "Training 30     - 100    -          0.3470 -           0.9125    | 0.6280   - 0.4533\n",
      "--------------------------------------------------     0.8977    | 0.5475   - 0.0376\n",
      "\n",
      "Training 31     - 100    -          0.2092 -           0.9118    | 0.6230   - 0.4513\n",
      "--------------------------------------------------     0.8956    | 0.5356   - 0.0371\n",
      "\n",
      "Training 32     - 100    -          0.1714 -           0.9165    | 0.6481   - 0.4680\n",
      "--------------------------------------------------     0.8884    | 0.5549   - 0.0412\n",
      "\n",
      "Training 33     - 100    -          0.1751 -           0.9125    | 0.6343   - 0.4560\n",
      "--------------------------------------------------     0.8885    | 0.5220   - 0.0365\n",
      "\n",
      "Training 34     - 100    -          0.2999 -           0.9157    | 0.6477   - 0.4489\n",
      "--------------------------------------------------     0.8945    | 0.5601   - 0.0365\n",
      "\n",
      "Training 35     - 100    -          0.1634 -           0.9165    | 0.6530   - 0.4527\n",
      "--------------------------------------------------     0.8987    | 0.5589   - 0.0366\n",
      "\n",
      "Training 36     - 100    -          0.2977 -           0.9166    | 0.6624   - 0.4488\n",
      "--------------------------------------------------     0.8942    | 0.5573   - 0.0370\n",
      "\n",
      "Training 37     - 100    -          0.2364 -           0.9155    | 0.6513   - 0.4514\n",
      "--------------------------------------------------     0.8991    | 0.5739   - 0.0375\n",
      "\n",
      "Training 38     - 100    -          0.3004 -           0.9152    | 0.6473   - 0.4518\n",
      "--------------------------------------------------     0.8977    | 0.5655   - 0.0371\n",
      "\n",
      "Training 39     - 100    -          0.1259 -           0.9202    | 0.6713   - 0.4504\n",
      "--------------------------------------------------     0.9010    | 0.5911   - 0.0369\n",
      "\n",
      "Training 40     - 100    -          0.1940 -           0.9165    | 0.6549   - 0.4500\n",
      "--------------------------------------------------     0.8980    | 0.5769   - 0.0372\n",
      "\n",
      "Training 41     - 100    -          0.1348 -           0.9219    | 0.6829   - 0.4525\n",
      "--------------------------------------------------     0.8980    | 0.5885   - 0.0375\n",
      "\n",
      "Training 42     - 100    -          0.2205 -           0.9268    | 0.7014   - 0.4532\n",
      "--------------------------------------------------     0.8893    | 0.4921   - 0.0374\n",
      "\n",
      "Training 43     - 100    -          0.2356 -           0.9207    | 0.6745   - 0.4516\n",
      "--------------------------------------------------     0.8970    | 0.5709   - 0.0382\n",
      "\n",
      "Training 44     - 100    -          0.1798 -           0.9208    | 0.6794   - 0.4493\n",
      "--------------------------------------------------     0.9025    | 0.5977   - 0.0366\n",
      "\n",
      "Training 45     - 100    -          0.1491 -           0.9239    | 0.6887   - 0.4497\n",
      "--------------------------------------------------     0.8997    | 0.5690   - 0.0369\n",
      "\n",
      "Training 46     - 100    -          0.2043 -           0.9266    | 0.7008   - 0.4484\n",
      "--------------------------------------------------     0.8984    | 0.5820   - 0.0363\n",
      "\n",
      "Training 47     - 100    -          0.1872 -           0.9220    | 0.6830   - 0.4501\n",
      "--------------------------------------------------     0.9023    | 0.5865   - 0.0369\n",
      "\n",
      "Training 48     - 100    -          0.2985 -           0.9216    | 0.6798   - 0.4500\n",
      "--------------------------------------------------     0.8905    | 0.5107   - 0.0366\n",
      "\n",
      "Training 49     - 100    -          0.2466 -           0.9267    | 0.7023   - 0.4544\n",
      "--------------------------------------------------     0.8979    | 0.5890   - 0.0370\n",
      "\n",
      "Training 50     - 100    -          0.1979 -           0.9292    | 0.7158   - 0.4506\n",
      "--------------------------------------------------     0.9042    | 0.6076   - 0.0365\n",
      "\n",
      "Training 51     - 100    -          0.1544 -           0.9271    | 0.7072   - 0.4499\n",
      "--------------------------------------------------     0.9064    | 0.6086   - 0.0367\n",
      "\n",
      "Training 52     - 100    -          0.1654 -           0.9245    | 0.7001   - 0.4490\n",
      "--------------------------------------------------     0.8997    | 0.5956   - 0.0367\n",
      "\n",
      "Training 53     - 100    -          0.1227 -           0.9286    | 0.7171   - 0.4514\n",
      "--------------------------------------------------     0.8991    | 0.5827   - 0.0373\n",
      "\n",
      "Training 54     - 100    -          0.1876 -           0.9267    | 0.7095   - 0.4489\n",
      "--------------------------------------------------     0.8895    | 0.5675   - 0.0366\n",
      "\n",
      "Training 55     - 100    -          0.4326 -           0.9289    | 0.7157   - 0.4490\n",
      "--------------------------------------------------     0.8853    | 0.4870   - 0.0366\n",
      "\n",
      "Training 56     - 100    -          0.1666 -           0.9256    | 0.7021   - 0.4516\n",
      "--------------------------------------------------     0.9010    | 0.5831   - 0.0375\n",
      "\n",
      "Training 57     - 100    -          0.2054 -           0.9294    | 0.7187   - 0.4510\n",
      "--------------------------------------------------     0.9044    | 0.5965   - 0.0372\n",
      "\n",
      "Training 58     - 100    -          0.0719 -           0.9320    | 0.7297   - 0.4631\n",
      "--------------------------------------------------     0.8969    | 0.5592   - 0.0401\n",
      "\n",
      "Training 59     - 100    -          0.1268 -           0.9345    | 0.7425   - 0.4639\n",
      "--------------------------------------------------     0.8999    | 0.5849   - 0.0392\n",
      "\n",
      "Training 60     - 100    -          0.2651 -           0.9329    | 0.7355   - 0.4629\n",
      "--------------------------------------------------     0.8964    | 0.5847   - 0.0393\n",
      "\n",
      "Training 61     - 100    -          0.2390 -           0.9324    | 0.7318   - 0.4691\n",
      "--------------------------------------------------     0.8941    | 0.5677   - 0.0394\n",
      "\n",
      "Training 62     - 100    -          0.1779 -           0.9285    | 0.7192   - 0.4631\n",
      "--------------------------------------------------     0.9075    | 0.6204   - 0.0389\n",
      "\n",
      "Training 63     - 100    -          0.2422 -           0.9255    | 0.7058   - 0.4667\n",
      "--------------------------------------------------     0.8891    | 0.5768   - 0.0394\n",
      "\n",
      "Training 64     - 100    -          0.2456 -           0.9263    | 0.7101   - 0.4646\n",
      "--------------------------------------------------     0.8996    | 0.5749   - 0.0393\n",
      "\n",
      "Training 65     - 100    -          0.1509 -           0.9327    | 0.7338   - 0.4667\n",
      "--------------------------------------------------     0.8962    | 0.5663   - 0.0391\n",
      "\n",
      "Training 66     - 100    -          0.1799 -           0.9331    | 0.7337   - 0.4648\n",
      "--------------------------------------------------     0.9017    | 0.5906   - 0.0388\n",
      "\n",
      "Training 67     - 100    -          0.2043 -           0.9348    | 0.7464   - 0.4648\n",
      "--------------------------------------------------     0.8932    | 0.5732   - 0.0387\n",
      "\n",
      "Training 68     - 100    -          0.1596 -           0.9336    | 0.7347   - 0.4627\n",
      "--------------------------------------------------     0.8964    | 0.5863   - 0.0392\n",
      "\n",
      "Training 69     - 100    -          0.2222 -           0.9366    | 0.7518   - 0.4632\n",
      "--------------------------------------------------     0.9027    | 0.5854   - 0.0391\n",
      "\n",
      "Training 70     - 100    -          0.1744 -           0.9311    | 0.7330   - 0.4692\n",
      "--------------------------------------------------     0.9014    | 0.6053   - 0.0394\n",
      "\n",
      "Training 71     - 100    -          0.1911 -           0.9314    | 0.7323   - 0.4596\n",
      "--------------------------------------------------     0.9014    | 0.5949   - 0.0391\n",
      "\n",
      "Training 72     - 100    -          0.2850 -           0.9329    | 0.7371   - 0.4591\n",
      "--------------------------------------------------     0.9004    | 0.5876   - 0.0373\n",
      "\n",
      "Training 73     - 100    -          0.2335 -           0.9305    | 0.7221   - 0.4605\n",
      "--------------------------------------------------     0.8990    | 0.5815   - 0.0407\n",
      "\n",
      "Training 74     - 100    -          0.1638 -           0.9325    | 0.7311   - 0.4647\n",
      "--------------------------------------------------     0.8962    | 0.5663   - 0.0376\n",
      "\n",
      "Training 75     - 100    -          0.1461 -           0.9344    | 0.7439   - 0.4567\n",
      "--------------------------------------------------     0.8713    | 0.4513   - 0.0377\n",
      "\n",
      "Training 76     - 100    -          0.2201 -           0.9332    | 0.7366   - 0.4564\n",
      "--------------------------------------------------     0.8973    | 0.5646   - 0.0367\n",
      "\n",
      "Training 77     - 100    -          0.2410 -           0.9347    | 0.7470   - 0.4543\n",
      "--------------------------------------------------     0.8970    | 0.5957   - 0.0374\n",
      "\n",
      "Training 78     - 100    -          0.2154 -           0.9351    | 0.7487   - 0.4552\n",
      "--------------------------------------------------     0.8982    | 0.5892   - 0.0382\n",
      "\n",
      "Training 79     - 100    -          0.2111 -           0.9324    | 0.7321   - 0.4545\n",
      "--------------------------------------------------     0.9009    | 0.6102   - 0.0368\n",
      "\n",
      "Training 80     - 100    -          0.1478 -           0.9381    | 0.7598   - 0.4610\n",
      "--------------------------------------------------     0.9033    | 0.6104   - 0.0388\n",
      "\n",
      "Training 81     - 100    -          0.2086 -           0.9343    | 0.7434   - 0.4538\n",
      "--------------------------------------------------     0.8943    | 0.5657   - 0.0375\n",
      "\n",
      "Training 82     - 100    -          0.1682 -           0.9371    | 0.7558   - 0.4533\n",
      "--------------------------------------------------     0.8979    | 0.6014   - 0.0365\n",
      "\n",
      "Training 83     - 100    -          0.1629 -           0.9390    | 0.7628   - 0.4539\n",
      "--------------------------------------------------     0.8980    | 0.5703   - 0.0374\n",
      "\n",
      "Training 84     - 100    -          0.0840 -           0.9413    | 0.7731   - 0.4540\n",
      "--------------------------------------------------     0.9022    | 0.6054   - 0.0371\n",
      "\n",
      "Training 85     - 100    -          0.1669 -           0.9405    | 0.7692   - 0.4536\n",
      "--------------------------------------------------     0.9026    | 0.6070   - 0.0376\n",
      "\n",
      "Training 86     - 100    -          0.1412 -           0.9415    | 0.7758   - 0.4533\n",
      "--------------------------------------------------     0.9009    | 0.5913   - 0.0369\n",
      "\n",
      "Training 87     - 100    -          0.1647 -           0.9380    | 0.7584   - 0.4592\n",
      "--------------------------------------------------     0.9012    | 0.6117   - 0.0375\n",
      "\n",
      "Training 88     - 100    -          0.1118 -           0.9380    | 0.7622   - 0.4546\n",
      "--------------------------------------------------     0.8987    | 0.5909   - 0.0366\n",
      "\n",
      "Training 89     - 100    -          0.2613 -           0.9379    | 0.7586   - 0.4535\n",
      "--------------------------------------------------     0.9013    | 0.6069   - 0.0375\n",
      "\n",
      "Training 90     - 100    -          0.0943 -           0.9382    | 0.7607   - 0.4529\n",
      "--------------------------------------------------     0.8938    | 0.5709   - 0.0373\n",
      "\n",
      "Training 91     - 100    -          0.0786 -           0.9406    | 0.7678   - 0.4536\n",
      "--------------------------------------------------     0.8975    | 0.5833   - 0.0377\n",
      "\n",
      "Training 92     - 100    -          0.1599 -           0.9373    | 0.7597   - 0.4536\n",
      "--------------------------------------------------     0.9001    | 0.6048   - 0.0365\n",
      "\n",
      "Training 93     - 100    -          0.1441 -           0.9405    | 0.7728   - 0.4578\n",
      "--------------------------------------------------     0.9013    | 0.6103   - 0.0374\n",
      "\n",
      "Training 94     - 100    -          0.2343 -           0.9402    | 0.7735   - 0.4583\n",
      "--------------------------------------------------     0.8835    | 0.5506   - 0.0377\n",
      "\n",
      "Training 95     - 100    -          0.1061 -           0.9389    | 0.7641   - 0.4523\n",
      "--------------------------------------------------     0.8994    | 0.5965   - 0.0370\n",
      "\n",
      "Training 96     - 100    -          0.1746 -           0.9417    | 0.7757   - 0.4536\n",
      "--------------------------------------------------     0.9041    | 0.6178   - 0.0370\n",
      "\n",
      "Training 97     - 100    -          0.1986 -           0.9383    | 0.7647   - 0.4536\n",
      "--------------------------------------------------     0.9020    | 0.5987   - 0.0374\n",
      "\n",
      "Training 98     - 100    -          0.2220 -           0.9411    | 0.7729   - 0.4540\n",
      "--------------------------------------------------     0.9038    | 0.6163   - 0.0368\n",
      "\n",
      "Training 99     - 100    -          0.1484 -           0.9415    | 0.7721   - 0.4540\n",
      "--------------------------------------------------     0.9004    | 0.5930   - 0.0368\n",
      "\n",
      "Training 100    - 100    -          0.2817 -           0.9383    | 0.7613   - 0.4536\n",
      "--------------------------------------------------     0.9050    | 0.6237   - 0.0373\n"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
