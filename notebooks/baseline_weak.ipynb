{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:15<00:00, 483.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    validation_ratio=0.2,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "Loading dataset: train, subset: weak\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "# manager.add_subset(\"synthetic20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> Creating new train / validation split\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> validation ratio : 0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.split_train_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%  setup augmentation and create pytorch dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MultipleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(MultipleDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        for d in self.datasets:\n",
    "            assert not isinstance(d, IterableDataset), \"ConcatDataset does not support IterableDataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [d[sample_idx] for d in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 243)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): ReLU6()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_features.Conv2d_3         [32, 32, 1, 1]    [1, 32, 1, 53]  1.056k   \n",
      "16_features.ReLU6_4                       -    [1, 32, 1, 53]       -   \n",
      "17_classifier.Flatten_0                   -         [1, 1696]       -   \n",
      "18_classifier.Dropout_1                   -         [1, 1696]       -   \n",
      "19_classifier.Linear_2           [1696, 10]           [1, 10]  16.97k   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_features.Conv2d_3           54.272k  \n",
      "16_features.ReLU6_4                  -  \n",
      "17_classifier.Flatten_0              -  \n",
      "18_classifier.Dropout_1              -  \n",
      "19_classifier.Linear_2          16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             37.034k\n",
      "Trainable params         37.034k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.663008M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "# criterion & optimizers\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "binacc_func = BinaryAccuracy()\n",
    "f_func = FScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak   - metrics:  Weak acc  | Weak F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<10.10}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"metrics: \", \"Weak acc \", \"Weak F1 \", \"Time\"\n",
    ")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    binacc_func.reset()\n",
    "    f_func.reset()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X_weak, y_weak) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X_weak, y_weak = X_weak.cuda().float(), y_weak[0].cuda().float()\n",
    "\n",
    "        logits = model(X_weak)\n",
    "        \n",
    "        \n",
    "        loss = criterion(logits, y_weak)\n",
    "        \n",
    "        # calc metrics\n",
    "        binacc = binacc_func(logits, y_weak)\n",
    "        fscore = f_func(logits, y_weak)\n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        # logs\n",
    "        print(value_form.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            int(100 * (i + 1) / len(training_loader)),\n",
    "            \"\", loss.item(),\n",
    "            \"\", binacc, fscore,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")\n",
    "        \n",
    "    # tensorboard logs\n",
    "    tensorboard.add_scalar(\"train/loss\", loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/acc\", binacc, epoch)\n",
    "    tensorboard.add_scalar(\"train/f1\", fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/precision\", f_func.precision, epoch)\n",
    "    tensorboard.add_scalar(\"train/recall\", f_func.recall, epoch)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "    binacc_func.reset()\n",
    "    f_func.reset()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X_weak, y_weak) in enumerate(val_loader):\n",
    "            X_weak, y_weak = X_weak.cuda().float(), y_weak[0].cuda().float()\n",
    "\n",
    "            logits = model(X_weak)\n",
    "\n",
    "            loss = criterion(logits, y_weak)\n",
    "\n",
    "            # calc metrics\n",
    "            binacc = binacc_func(logits, y_weak)\n",
    "            fscore = f_func(logits, y_weak)\n",
    "\n",
    "            # logs\n",
    "            print(value_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / len(val_loader)),\n",
    "                \"\", loss.item(),\n",
    "                \"\", binacc, fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/loss\", loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/acc\", binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/f1\", fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/precision\", f_func.precision, epoch)\n",
    "        tensorboard.add_scalar(\"val/recall\", f_func.recall, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak   - metrics:  Weak acc  | Weak F1  - Time  \n",
      "\n",
      "Training 1      - 100    -          0.0921 -           0.9281    | 0.7344   - 0.4555\n",
      "Training 1      - 100    -          0.3142 -           0.8917    | 0.7371   - 0.0369\n",
      "Training 2      - 100    -          0.1383 -           0.9278    | 0.7396   - 0.4531\n",
      "Training 2      - 100    -          0.3018 -           0.8958    | 0.7424   - 0.0380\n",
      "Training 3      - 100    -          0.1463 -           0.9287    | 0.7453   - 0.4651\n",
      "Training 3      - 100    -          0.3019 -           0.8952    | 0.7483   - 0.0407\n",
      "Training 4      - 100    -          0.1956 -           0.9290    | 0.7511   - 0.4573\n",
      "Training 4      - 100    -          0.3107 -           0.8889    | 0.7538   - 0.0359\n",
      "Training 5      - 100    -          0.1325 -           0.9324    | 0.7567   - 0.4499\n",
      "Training 5      - 100    -          0.2920 -           0.8953    | 0.7594   - 0.0358\n",
      "Training 6      - 100    -          0.1448 -           0.9320    | 0.7622   - 0.4504\n",
      "Training 6      - 100    -          0.2913 -           0.8868    | 0.7650   - 0.0363\n",
      "Training 7      - 100    -          0.2074 -           0.9292    | 0.7678   - 0.4537\n",
      "Training 7      - 100    -          0.2867 -           0.8970    | 0.7702   - 0.0360\n",
      "Training 8      - 100    -          0.1970 -           0.9257    | 0.7725   - 0.4515\n",
      "Training 8      - 100    -          0.3039 -           0.8947    | 0.7747   - 0.0377\n",
      "Training 9      - 100    -          0.2339 -           0.9281    | 0.7772   - 0.4495\n",
      "Training 9      - 100    -          0.2953 -           0.8944    | 0.7795   - 0.0364\n",
      "Training 10     - 100    -          0.2755 -           0.9271    | 0.7821   - 0.4507\n",
      "Training 10     - 100    -          0.2928 -           0.8959    | 0.7844   - 0.0361\n",
      "Training 11     - 100    -          0.2250 -           0.9306    | 0.7868   - 0.4503\n",
      "Training 11     - 100    -          0.2964 -           0.8932    | 0.7889   - 0.0378\n",
      "Training 12     - 100    -          0.2172 -           0.9320    | 0.7913   - 0.4536\n",
      "Training 12     - 100    -          0.2757 -           0.8990    | 0.7938   - 0.0389\n",
      "Training 13     - 100    -          0.2381 -           0.9324    | 0.7964   - 0.4596\n",
      "Training 13     - 100    -          0.3147 -           0.8962    | 0.7988   - 0.0389\n",
      "Training 14     - 100    -          0.1151 -           0.9323    | 0.8011   - 0.4642\n",
      "Training 14     - 100    -          0.2953 -           0.8947    | 0.8033   - 0.0390\n",
      "Training 15     - 100    -          0.1648 -           0.9304    | 0.8057   - 0.4753\n",
      "Training 15     - 100    -          0.2961 -           0.8983    | 0.8078   - 0.0390\n",
      "Training 16     - 100    -          0.1258 -           0.9329    | 0.8102   - 0.4643\n",
      "Training 16     - 100    -          0.2805 -           0.8936    | 0.8123   - 0.0399\n",
      "Training 17     - 100    -          0.2295 -           0.9322    | 0.8145   - 0.4554\n",
      "Training 17     - 100    -          0.2874 -           0.8959    | 0.8166   - 0.0364\n",
      "Training 18     - 100    -          0.1843 -           0.9345    | 0.8188   - 0.4484\n",
      "Training 18     - 100    -          0.2825 -           0.8973    | 0.8210   - 0.0357\n",
      "Training 19     - 100    -          0.0920 -           0.9324    | 0.8231   - 0.4493\n",
      "Training 19     - 100    -          0.2856 -           0.9016    | 0.8254   - 0.0363\n",
      "Training 20     - 100    -          0.1750 -           0.9325    | 0.8275   - 0.4491\n",
      "Training 20     - 100    -          0.2765 -           0.8962    | 0.8291   - 0.0355\n",
      "Training 21     - 100    -          0.1674 -           0.9324    | 0.8312   - 0.4489\n",
      "Training 21     - 100    -          0.2914 -           0.8940    | 0.8332   - 0.0355\n",
      "Training 22     - 100    -          0.1369 -           0.9343    | 0.8353   - 0.4496\n",
      "Training 22     - 100    -          0.2786 -           0.8877    | 0.8370   - 0.0364\n",
      "Training 23     - 100    -          0.1636 -           0.9337    | 0.8388   - 0.4549\n",
      "Training 23     - 100    -          0.3012 -           0.9023    | 0.8407   - 0.0359\n",
      "Training 24     - 100    -          0.1577 -           0.9382    | 0.8431   - 0.4490\n",
      "Training 24     - 100    -          0.2935 -           0.8974    | 0.8452   - 0.0360\n",
      "Training 25     - 100    -          0.2184 -           0.9360    | 0.8472   - 0.4524\n",
      "Training 25     - 100    -          0.2994 -           0.8997    | 0.8493   - 0.0357\n",
      "Training 26     - 100    -          0.0944 -           0.9387    | 0.8516   - 0.4525\n",
      "Training 26     - 100    -          0.2803 -           0.8941    | 0.8537   - 0.0361\n",
      "Training 27     - 100    -          0.1200 -           0.9347    | 0.8555   - 0.4488\n",
      "Training 27     - 100    -          0.2931 -           0.8956    | 0.8572   - 0.0357\n",
      "Training 28     - 100    -          0.2376 -           0.9318    | 0.8591   - 0.4549\n",
      "Training 28     - 100    -          0.3035 -           0.8966    | 0.8606   - 0.0360\n",
      "Training 29     - 100    -          0.1303 -           0.9359    | 0.8625   - 0.4522\n",
      "Training 29     - 100    -          0.2895 -           0.8987    | 0.8643   - 0.0363\n",
      "Training 30     - 100    -          0.1942 -           0.9343    | 0.8661   - 0.4498\n",
      "Training 30     - 100    -          0.2954 -           0.8857    | 0.8677   - 0.0361\n",
      "Training 31     - 100    -          0.1130 -           0.9318    | 0.8691   - 0.4515\n",
      "Training 31     - 100    -          0.3304 -           0.9017    | 0.8707   - 0.0368\n",
      "Training 32     - 100    -          0.2026 -           0.9344    | 0.8722   - 0.4553\n",
      "Training 32     - 100    -          0.3266 -           0.8965    | 0.8738   - 0.0358\n",
      "Training 33     - 100    -          0.1841 -           0.9370    | 0.8755   - 0.4559\n",
      "Training 33     - 100    -          0.2862 -           0.8949    | 0.8771   - 0.0357\n",
      "Training 34     - 100    -          0.1287 -           0.9359    | 0.8788   - 0.4497\n",
      "Training 34     - 100    -          0.3091 -           0.9034    | 0.8805   - 0.0360\n",
      "Training 35     - 100    -          0.1943 -           0.9372    | 0.8825   - 0.4497\n",
      "Training 35     - 100    -          0.3098 -           0.8986    | 0.8841   - 0.0360\n",
      "Training 36     - 100    -          0.1608 -           0.9363    | 0.8857   - 0.4516\n",
      "Training 36     - 100    -          0.3348 -           0.8932    | 0.8871   - 0.0359\n",
      "Training 37     - 100    -          0.1527 -           0.9388    | 0.8889   - 0.4529\n",
      "Training 37     - 100    -          0.3009 -           0.8972    | 0.8905   - 0.0354\n",
      "Training 38     - 100    -          0.1221 -           0.9371    | 0.8920   - 0.4521\n",
      "Training 38     - 100    -          0.2973 -           0.8987    | 0.8935   - 0.0357\n",
      "Training 39     - 100    -          0.2057 -           0.9388    | 0.8953   - 0.4510\n",
      "Training 39     - 100    -          0.2903 -           0.8932    | 0.8967   - 0.0362\n",
      "Training 40     - 100    -          0.0830 -           0.9395    | 0.8984   - 0.4520\n",
      "Training 40     - 100    -          0.3028 -           0.9020    | 0.9002   - 0.0373\n",
      "Training 41     - 100    -          0.2900 -           0.9373    | 0.9019   - 0.4611\n",
      "Training 41     - 100    -          0.2933 -           0.8947    | 0.9033   - 0.0381\n",
      "Training 42     - 100    -          0.1966 -           0.9343    | 0.9046   - 0.4666\n",
      "Training 42     - 100    -          0.3043 -           0.8982    | 0.9059   - 0.0383\n",
      "Training 43     - 100    -          0.2056 -           0.9390    | 0.9075   - 0.4627\n",
      "Training 43     - 100    -          0.2904 -           0.8971    | 0.9090   - 0.0382\n",
      "Training 44     - 100    -          0.2533 -           0.9418    | 0.9106   - 0.4646\n",
      "Training 44     - 100    -          0.3119 -           0.8956    | 0.9120   - 0.0391\n",
      "Training 45     - 100    -          0.2646 -           0.9391    | 0.9134   - 0.4640\n",
      "Training 45     - 100    -          0.2909 -           0.8993    | 0.9149   - 0.0383\n",
      "Training 46     - 100    -          0.1187 -           0.9398    | 0.9163   - 0.4656\n",
      "Training 46     - 100    -          0.2908 -           0.8973    | 0.9178   - 0.0381\n",
      "Training 47     - 100    -          0.3034 -           0.9381    | 0.9191   - 0.4640\n",
      "Training 47     - 100    -          0.3205 -           0.8979    | 0.9204   - 0.0374\n",
      "Training 48     - 100    -          0.2091 -           0.9410    | 0.9219   - 0.4539\n",
      "Training 48     - 100    -          0.3148 -           0.8956    | 0.9232   - 0.0358\n",
      "Training 49     - 100    -          0.3029 -           0.9380    | 0.9245   - 0.4508\n",
      "Training 49     - 100    -          0.3036 -           0.8919    | 0.9256   - 0.0363\n",
      "Training 50     - 100    -          0.1656 -           0.9379    | 0.9270   - 0.4510\n",
      "Training 50     - 100    -          0.3205 -           0.8900    | 0.9281   - 0.0359\n",
      "Training 51     - 100    -          0.1952 -           0.9386    | 0.9293   - 0.4505\n",
      "Training 51     - 100    -          0.3002 -           0.8963    | 0.9305   - 0.0360\n",
      "Training 52     - 100    -          0.1131 -           0.9409    | 0.9319   - 0.4521\n",
      "Training 52     - 100    -          0.2926 -           0.8963    | 0.9332   - 0.0364\n",
      "Training 53     - 100    -          0.1340 -           0.9406    | 0.9344   - 0.4490\n",
      "Training 53     - 100    -          0.3093 -           0.9009    | 0.9357   - 0.0353\n",
      "Training 54     - 100    -          0.1014 -           0.9422    | 0.9371   - 0.4506\n",
      "Training 54     - 100    -          0.3047 -           0.8966    | 0.9384   - 0.0395\n",
      "Training 55     - 100    -          0.2223 -           0.9390    | 0.9397   - 0.4517\n",
      "Training 55     - 100    -          0.2926 -           0.9023    | 0.9409   - 0.0366\n",
      "Training 56     - 100    -          0.1782 -           0.9391    | 0.9422   - 0.4515\n",
      "Training 56     - 100    -          0.3124 -           0.8979    | 0.9433   - 0.0357\n",
      "Training 57     - 100    -          0.1706 -           0.9410    | 0.9446   - 0.4515\n",
      "Training 57     - 100    -          0.2827 -           0.8988    | 0.9458   - 0.0358\n",
      "Training 58     - 100    -          0.1242 -           0.9406    | 0.9471   - 0.4568\n",
      "Training 58     - 100    -          0.2875 -           0.8945    | 0.9482   - 0.0367\n",
      "Training 59     - 100    -          0.0976 -           0.9424    | 0.9494   - 0.4513\n",
      "Training 59     - 100    -          0.3055 -           0.8936    | 0.9506   - 0.0360\n",
      "Training 60     - 100    -          0.1373 -           0.9419    | 0.9519   - 0.4513\n",
      "Training 60     - 100    -          0.3149 -           0.8941    | 0.9531   - 0.0361\n",
      "Training 61     - 100    -          0.1998 -           0.9424    | 0.9542   - 0.4505\n",
      "Training 61     - 100    -          0.2894 -           0.8974    | 0.9554   - 0.0364\n",
      "Training 62     - 100    -          0.2486 -           0.9376    | 0.9564   - 0.4529\n",
      "Training 62     - 100    -          0.3192 -           0.8915    | 0.9574   - 0.0360\n",
      "Training 63     - 100    -          0.1082 -           0.9442    | 0.9586   - 0.4510\n",
      "Training 63     - 100    -          0.2955 -           0.8992    | 0.9598   - 0.0361\n",
      "Training 64     - 100    -          0.1142 -           0.9436    | 0.9611   - 0.4526\n",
      "Training 64     - 100    -          0.2886 -           0.8978    | 0.9623   - 0.0372\n",
      "Training 65     - 100    -          0.0704 -           0.9429    | 0.9634   - 0.4528\n",
      "Training 65     - 100    -          0.2999 -           0.8927    | 0.9646   - 0.0377\n",
      "Training 66     - 100    -          0.2090 -           0.9412    | 0.9657   - 0.4625\n",
      "Training 66     - 100    -          0.2903 -           0.8967    | 0.9667   - 0.0413\n",
      "Training 67     - 100    -          0.1599 -           0.9438    | 0.9679   - 0.4686\n",
      "Training 67     - 100    -          0.2863 -           0.9012    | 0.9689   - 0.0366\n",
      "Training 68     - 100    -          0.1169 -           0.9422    | 0.9701   - 0.4539\n",
      "Training 68     - 100    -          0.2910 -           0.8930    | 0.9711   - 0.0358\n",
      "Training 69     - 100    -          0.1589 -           0.9421    | 0.9722   - 0.4533\n",
      "Training 69     - 100    -          0.2922 -           0.8986    | 0.9732   - 0.0363\n",
      "Training 70     - 100    -          0.1709 -           0.9473    | 0.9744   - 0.4505\n",
      "Training 70     - 100    -          0.3133 -           0.8937    | 0.9755   - 0.0359\n",
      "Training 71     - 100    -          0.2341 -           0.9451    | 0.9766   - 0.4512\n",
      "Training 71     - 100    -          0.2958 -           0.8959    | 0.9776   - 0.0359\n",
      "Training 72     - 100    -          0.1422 -           0.9459    | 0.9789   - 0.4535\n",
      "Training 72     - 100    -          0.2817 -           0.8953    | 0.9799   - 0.0373\n",
      "Training 73     - 100    -          0.0728 -           0.9493    | 0.9812   - 0.4554\n",
      "Training 73     - 100    -          0.3030 -           0.8988    | 0.9824   - 0.0362\n",
      "Training 74     - 100    -          0.1516 -           0.9443    | 0.9836   - 0.4584\n",
      "Training 74     - 100    -          0.3049 -           0.8990    | 0.9846   - 0.0395\n",
      "Training 75     - 100    -          0.1499 -           0.9470    | 0.9858   - 0.4796\n",
      "Training 75     - 100    -          0.2903 -           0.9025    | 0.9870   - 0.0383\n",
      "Training 76     - 100    -          0.0995 -           0.9444    | 0.9882   - 0.4662\n",
      "Training 76     - 100    -          0.2805 -           0.9018    | 0.9891   - 0.0382\n",
      "Training 77     - 100    -          0.1603 -           0.9410    | 0.9900   - 0.4623\n",
      "Training 77     - 100    -          0.2930 -           0.8979    | 0.9908   - 0.0388\n",
      "Training 78     - 100    -          0.1286 -           0.9464    | 0.9919   - 0.4651\n",
      "Training 78     - 100    -          0.2815 -           0.8974    | 0.9930   - 0.0382\n",
      "Training 79     - 100    -          0.1538 -           0.9432    | 0.9940   - 0.4656\n",
      "Training 79     - 100    -          0.2881 -           0.8893    | 0.9949   - 0.0389\n",
      "Training 80     - 100    -          0.1740 -           0.9454    | 0.9959   - 0.4687\n",
      "Training 80     - 100    -          0.2962 -           0.9002    | 0.9968   - 0.0385\n",
      "Training 81     - 100    -          0.1868 -           0.9456    | 0.9980   - 0.4653\n",
      "Training 81     - 100    -          0.2778 -           0.9030    | 0.9990   - 0.0366\n",
      "Training 82     - 100    -          0.0805 -           0.9447    | 0.9999   - 0.4631\n",
      "Training 82     - 100    -          0.3105 -           0.8988    | 1.0010   - 0.0363\n",
      "Training 83     - 100    -          0.1899 -           0.9481    | 1.0020   - 0.4558\n",
      "Training 83     - 100    -          0.2662 -           0.8983    | 1.0029   - 0.0370\n",
      "Training 84     - 100    -          0.1152 -           0.9437    | 1.0040   - 0.4561\n",
      "Training 84     - 100    -          0.2967 -           0.8928    | 1.0048   - 0.0362\n",
      "Training 85     - 100    -          0.0785 -           0.9494    | 1.0059   - 0.4521\n",
      "Training 85     - 100    -          0.2846 -           0.8978    | 1.0070   - 0.0362\n",
      "Training 86     - 100    -          0.1326 -           0.9454    | 1.0079   - 0.4510\n",
      "Training 86     - 100    -          0.2868 -           0.8982    | 1.0089   - 0.0366\n",
      "Training 87     - 100    -          0.1712 -           0.9437    | 1.0098   - 0.4546\n",
      "Training 87     - 100    -          0.2893 -           0.9020    | 1.0108   - 0.0369\n",
      "Training 88     - 100    -          0.1694 -           0.9485    | 1.0119   - 0.4578\n",
      "Training 88     - 100    -          0.3023 -           0.9009    | 1.0128   - 0.0370\n",
      "Training 89     - 100    -          0.1649 -           0.9475    | 1.0139   - 0.4580\n",
      "Training 89     - 100    -          0.3005 -           0.8957    | 1.0148   - 0.0365\n",
      "Training 90     - 100    -          0.0933 -           0.9459    | 1.0157   - 0.4551\n",
      "Training 90     - 100    -          0.2917 -           0.9007    | 1.0166   - 0.0359\n",
      "Training 91     - 100    -          0.1047 -           0.9474    | 1.0176   - 0.4542\n",
      "Training 91     - 100    -          0.2995 -           0.8990    | 1.0185   - 0.0360\n",
      "Training 92     - 100    -          0.1312 -           0.9505    | 1.0195   - 0.4592\n",
      "Training 92     - 100    -          0.2845 -           0.8978    | 1.0205   - 0.0363\n",
      "Training 93     - 100    -          0.1787 -           0.9490    | 1.0214   - 0.4599\n",
      "Training 93     - 100    -          0.2924 -           0.8971    | 1.0224   - 0.0361\n",
      "Training 94     - 100    -          0.1086 -           0.9493    | 1.0234   - 0.4647\n",
      "Training 94     - 100    -          0.2916 -           0.8969    | 1.0244   - 0.0378\n",
      "Training 95     - 100    -          0.1259 -           0.9474    | 1.0253   - 0.4575\n",
      "Training 95     - 100    -          0.3048 -           0.8913    | 1.0261   - 0.0365\n",
      "Training 96     - 100    -          0.0485 -           0.9481    | 1.0270   - 0.4509\n",
      "Training 96     - 100    -          0.2956 -           0.8956    | 1.0278   - 0.0369\n",
      "Training 97     - 100    -          0.1304 -           0.9495    | 1.0286   - 0.4546\n",
      "Training 97     - 100    -          0.2997 -           0.8965    | 1.0297   - 0.0368\n",
      "Training 98     - 100    -          0.0447 -           0.9491    | 1.0306   - 0.4536\n",
      "Training 98     - 100    -          0.2886 -           0.8993    | 1.0317   - 0.0365\n",
      "Training 99     - 100    -          0.1116 -           0.9451    | 1.0325   - 0.4593\n",
      "Training 99     - 100    -          0.2793 -           0.8952    | 1.0332   - 0.0371\n",
      "Training 100    - 100    -          0.1346 -           0.9474    | 1.0340   - 0.4568\n",
      "Training 100    - 100    -          0.2909 -           0.9022    | 1.0349   - 0.0367\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
