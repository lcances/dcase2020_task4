{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:16<00:00, 471.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    validation_ratio=0.2,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53, # The model output localisation with a résolution of ~ 18ms --> 53 temporal bins\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak ans synthetic20 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "Loading dataset: train, subset: weak\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> Creating new train / validation split\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> validation ratio : 0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.split_train_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%  setup augmentation and create pytorch dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MultipleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(MultipleDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        for d in self.datasets:\n",
    "            assert not isinstance(d, IterableDataset), \"ConcatDataset does not support IterableDataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [d[sample_idx] for d in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1202, 263)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset\n",
    "\n",
    "- We want both the weak and strong ground truth --> the *weak* and *strong* parameters to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, weak=True, strong=False, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, weak=True, strong=False, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "This model is the same than the weak baseline but have an extra output. <br />\n",
    "the loc_output is compose of a single convolution layer with nb_filters == nb_class. <br />\n",
    "Since their is some pooling layer, the *loc_ouput* have a precision of 53 bins (~= 18 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): ReLU6()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache() \n",
    "\n",
    "model = WeakBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_features.Conv2d_3         [32, 32, 1, 1]    [1, 32, 1, 53]  1.056k   \n",
      "16_features.ReLU6_4                       -    [1, 32, 1, 53]       -   \n",
      "17_classifier.Flatten_0                   -         [1, 1696]       -   \n",
      "18_classifier.Dropout_1                   -         [1, 1696]       -   \n",
      "19_classifier.Linear_2           [1696, 10]           [1, 10]  16.97k   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_features.Conv2d_3           54.272k  \n",
      "16_features.ReLU6_4                  -  \n",
      "17_classifier.Flatten_0              -  \n",
      "18_classifier.Dropout_1              -  \n",
      "19_classifier.Linear_2          16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             37.034k\n",
      "Trainable params         37.034k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.663008M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom loss function\n",
    "\n",
    "Since not all file have strong truth, it is necessary to remove those files. <br />\n",
    "For that, the strong mask is computed. If the sum of the strong ground truth is equal to 0 then it is a fake one <br />\n",
    "This file strong loss must not be taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "weak_binacc_func = BinaryAccuracy()\n",
    "weak_f_func = FScore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_metrics():\n",
    "    metrics = [weak_binacc_func, weak_f_func]\n",
    "    \n",
    "    for m in metrics:\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak   - metrics:  Weak acc    | Weak F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6} - {:<9.9} {:<12.12}| {:<9.9}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f} - {:<9.9} {:<10.4f}| {:<9.4f}- {:<6.4f}\"\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"metrics: \", \"Weak acc \", \"Weak F1 \",\"Time\"\n",
    ")\n",
    "\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X, y) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X = X.cuda().float()\n",
    "        y_weak = y[0].cuda().float()\n",
    "        \n",
    "        weak_logits = model(X)\n",
    "        \n",
    "        # calc the loss\n",
    "        weak_loss = criterion(weak_logits, y_weak)\n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        weak_loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(),\n",
    "                \"\", weak_binacc, weak_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"train/weak_loss\", weak_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"train/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/weak_f1\", weak_fscore, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y_weak = y[0].cuda().float()\n",
    "\n",
    "            weak_logits = model(X)\n",
    "\n",
    "            # calc the loss\n",
    "            weak_loss = criterion(weak_logits, y_weak)\n",
    "\n",
    "             # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(),\n",
    "                \"\", weak_binacc, weak_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/weak_loss\", weak_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"val/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/weak_f1\", weak_fscore, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak   - metrics:  Weak acc    | Weak F1  - Time  \n",
      "\n",
      "Training 1      - 102    -          0.3927 -           0.8454    | 0.0631   - 15.2248\n",
      "\u001b[1;4mValidati 1      - 24     -          0.4046 -           0.8623    | 0.0274   - 3.2933\u001b[0m\n",
      "Training 2      - 102    -          0.4035 -           0.8562    | 0.1249   - 0.5406\n",
      "\u001b[1;4mValidati 2      - 24     -          0.4280 -           0.8644    | 0.1926   - 0.0408\u001b[0m\n",
      "Training 3      - 102    -          0.3545 -           0.8602    | 0.1878   - 0.4840\n",
      "\u001b[1;4mValidati 3      - 24     -          0.4555 -           0.8460    | 0.1802   - 0.0400\u001b[0m\n",
      "Training 4      - 102    -          0.3142 -           0.8655    | 0.2628   - 0.4848\n",
      "\u001b[1;4mValidati 4      - 24     -          0.4324 -           0.8734    | 0.2412   - 0.0401\u001b[0m\n",
      "Training 5      - 102    -          0.3565 -           0.8731    | 0.3323   - 0.5095\n",
      "\u001b[1;4mValidati 5      - 24     -          0.4045 -           0.8790    | 0.3341   - 0.0455\u001b[0m\n",
      "Training 6      - 102    -          0.3169 -           0.8726    | 0.3408   - 0.4848\n",
      "\u001b[1;4mValidati 6      - 24     -          0.3928 -           0.8783    | 0.3263   - 0.0400\u001b[0m\n",
      "Training 7      - 102    -          0.3072 -           0.8806    | 0.3975   - 0.4844\n",
      "\u001b[1;4mValidati 7      - 24     -          0.3984 -           0.8779    | 0.2838   - 0.0408\u001b[0m\n",
      "Training 8      - 102    -          0.3263 -           0.8785    | 0.3884   - 0.4898\n",
      "\u001b[1;4mValidati 8      - 24     -          0.3861 -           0.8814    | 0.3671   - 0.0401\u001b[0m\n",
      "Training 9      - 102    -          0.3230 -           0.8827    | 0.4302   - 0.4849\n",
      "\u001b[1;4mValidati 9      - 24     -          0.4044 -           0.8804    | 0.3682   - 0.0399\u001b[0m\n",
      "Training 10     - 102    -          0.3410 -           0.8890    | 0.4790   - 0.4845\n",
      "\u001b[1;4mValidati 10     - 24     -          0.3762 -           0.8821    | 0.3669   - 0.0402\u001b[0m\n",
      "Training 11     - 102    -          0.2828 -           0.8871    | 0.4581   - 0.4833\n",
      "\u001b[1;4mValidati 11     - 24     -          0.4418 -           0.8852    | 0.4176   - 0.0401\u001b[0m\n",
      "Training 12     - 102    -          0.2483 -           0.8929    | 0.4995   - 0.4883\n",
      "\u001b[1;4mValidati 12     - 24     -          0.3974 -           0.8883    | 0.4503   - 0.0416\u001b[0m\n",
      "Training 13     - 102    -          0.2370 -           0.8899    | 0.4932   - 0.4849\n",
      "\u001b[1;4mValidati 13     - 24     -          0.5402 -           0.8824    | 0.3571   - 0.0400\u001b[0m\n",
      "Training 14     - 102    -          0.3345 -           0.8940    | 0.5096   - 0.4845\n",
      "\u001b[1;4mValidati 14     - 24     -          0.4086 -           0.8859    | 0.4169   - 0.0425\u001b[0m\n",
      "Training 15     - 102    -          0.2626 -           0.8952    | 0.5216   - 0.4866\n",
      "\u001b[1;4mValidati 15     - 24     -          0.4029 -           0.8866    | 0.4049   - 0.0398\u001b[0m\n",
      "Training 16     - 102    -          0.2693 -           0.8950    | 0.5307   - 0.4845\n",
      "\u001b[1;4mValidati 16     - 24     -          0.4408 -           0.8922    | 0.4759   - 0.0401\u001b[0m\n",
      "Training 17     - 102    -          0.2411 -           0.8958    | 0.5263   - 0.4861\n",
      "\u001b[1;4mValidati 17     - 24     -          0.4625 -           0.8876    | 0.4100   - 0.0413\u001b[0m\n",
      "Training 18     - 102    -          0.3196 -           0.8993    | 0.5480   - 0.4863\n",
      "\u001b[1;4mValidati 18     - 24     -          0.3822 -           0.8922    | 0.4458   - 0.0397\u001b[0m\n",
      "Training 19     - 102    -          0.2606 -           0.9017    | 0.5657   - 0.4836\n",
      "\u001b[1;4mValidati 19     - 24     -          0.3953 -           0.8831    | 0.3889   - 0.0398\u001b[0m\n",
      "Training 20     - 102    -          0.2715 -           0.9005    | 0.5635   - 0.4874\n",
      "\u001b[1;4mValidati 20     - 24     -          0.3781 -           0.8942    | 0.4745   - 0.0402\u001b[0m\n",
      "Training 21     - 102    -          0.1958 -           0.9014    | 0.5588   - 0.4835\n",
      "\u001b[1;4mValidati 21     - 24     -          0.3901 -           0.8932    | 0.4687   - 0.0406\u001b[0m\n",
      "Training 22     - 102    -          0.2407 -           0.9010    | 0.5721   - 0.4840\n",
      "\u001b[1;4mValidati 22     - 24     -          0.4354 -           0.8942    | 0.4749   - 0.0397\u001b[0m\n",
      "Training 23     - 102    -          0.2696 -           0.9040    | 0.5811   - 0.4833\n",
      "\u001b[1;4mValidati 23     - 24     -          0.4033 -           0.8932    | 0.4840   - 0.0412\u001b[0m\n",
      "Training 24     - 102    -          0.1954 -           0.9042    | 0.5856   - 0.4842\n",
      "\u001b[1;4mValidati 24     - 24     -          0.4676 -           0.8849    | 0.5034   - 0.0402\u001b[0m\n",
      "Training 25     - 102    -          0.2765 -           0.9065    | 0.6007   - 0.4835\n",
      "\u001b[1;4mValidati 25     - 24     -          0.4189 -           0.8991    | 0.4993   - 0.0410\u001b[0m\n",
      "Training 26     - 102    -          0.2227 -           0.9050    | 0.5960   - 0.4843\n",
      "\u001b[1;4mValidati 26     - 24     -          0.4295 -           0.8956    | 0.4807   - 0.0401\u001b[0m\n",
      "Training 27     - 102    -          0.2379 -           0.9065    | 0.5957   - 0.4870\n",
      "\u001b[1;4mValidati 27     - 24     -          0.4316 -           0.8929    | 0.4605   - 0.0415\u001b[0m\n",
      "Training 28     - 102    -          0.2807 -           0.9105    | 0.6202   - 0.4874\n",
      "\u001b[1;4mValidati 28     - 24     -          0.4353 -           0.8946    | 0.4716   - 0.0406\u001b[0m\n",
      "Training 29     - 102    -          0.3353 -           0.9090    | 0.6153   - 0.4876\n",
      "\u001b[1;4mValidati 29     - 24     -          0.4328 -           0.8949    | 0.5030   - 0.0406\u001b[0m\n",
      "Training 30     - 102    -          0.2128 -           0.9117    | 0.6293   - 0.4853\n",
      "\u001b[1;4mValidati 30     - 24     -          0.4124 -           0.8977    | 0.4900   - 0.0408\u001b[0m\n",
      "Training 31     - 102    -          0.1850 -           0.9139    | 0.6369   - 0.4844\n",
      "\u001b[1;4mValidati 31     - 24     -          0.4528 -           0.8925    | 0.4579   - 0.0409\u001b[0m\n",
      "Training 32     - 102    -          0.2958 -           0.9125    | 0.6315   - 0.4861\n",
      "\u001b[1;4mValidati 32     - 24     -          0.4265 -           0.8953    | 0.4650   - 0.0406\u001b[0m\n",
      "Training 33     - 102    -          0.2175 -           0.9137    | 0.6373   - 0.4866\n",
      "\u001b[1;4mValidati 33     - 24     -          0.4309 -           0.8974    | 0.5041   - 0.0400\u001b[0m\n",
      "Training 34     - 102    -          0.2295 -           0.9133    | 0.6353   - 0.4867\n",
      "\u001b[1;4mValidati 34     - 24     -          0.4236 -           0.8967    | 0.4913   - 0.0435\u001b[0m\n",
      "Training 35     - 102    -          0.2157 -           0.9168    | 0.6538   - 0.4874\n",
      "\u001b[1;4mValidati 35     - 24     -          0.4152 -           0.8932    | 0.4618   - 0.0406\u001b[0m\n",
      "Training 36     - 102    -          0.1829 -           0.9178    | 0.6675   - 0.4921\n",
      "\u001b[1;4mValidati 36     - 24     -          0.4512 -           0.8977    | 0.5111   - 0.0411\u001b[0m\n",
      "Training 37     - 102    -          0.2188 -           0.9199    | 0.6735   - 0.4904\n",
      "\u001b[1;4mValidati 37     - 24     -          0.4474 -           0.8988    | 0.5064   - 0.0414\u001b[0m\n",
      "Training 38     - 102    -          0.1947 -           0.9202    | 0.6711   - 0.4871\n",
      "\u001b[1;4mValidati 38     - 24     -          0.4563 -           0.9005    | 0.5337   - 0.0398\u001b[0m\n",
      "Training 39     - 102    -          0.1811 -           0.9171    | 0.6575   - 0.4837\n",
      "\u001b[1;4mValidati 39     - 24     -          0.4630 -           0.8974    | 0.5000   - 0.0402\u001b[0m\n",
      "Training 40     - 102    -          0.2478 -           0.9193    | 0.6664   - 0.4855\n",
      "\u001b[1;4mValidati 40     - 24     -          0.4469 -           0.9022    | 0.5187   - 0.0401\u001b[0m\n",
      "Training 41     - 102    -          0.2432 -           0.9160    | 0.6579   - 0.4865\n",
      "\u001b[1;4mValidati 41     - 24     -          0.4626 -           0.8897    | 0.4442   - 0.0411\u001b[0m\n",
      "Training 42     - 102    -          0.1788 -           0.9168    | 0.6596   - 0.4849\n",
      "\u001b[1;4mValidati 42     - 24     -          0.4500 -           0.9005    | 0.5230   - 0.0399\u001b[0m\n",
      "Training 43     - 102    -          0.1879 -           0.9210    | 0.6790   - 0.4848\n",
      "\u001b[1;4mValidati 43     - 24     -          0.4555 -           0.8960    | 0.4929   - 0.0401\u001b[0m\n",
      "Training 44     - 102    -          0.1809 -           0.9213    | 0.6800   - 0.4860\n",
      "\u001b[1;4mValidati 44     - 24     -          0.4455 -           0.8981    | 0.5131   - 0.0403\u001b[0m\n",
      "Training 45     - 102    -          0.2554 -           0.9206    | 0.6762   - 0.4888\n",
      "\u001b[1;4mValidati 45     - 24     -          0.4575 -           0.9026    | 0.5256   - 0.0408\u001b[0m\n",
      "Training 46     - 102    -          0.2482 -           0.9214    | 0.6799   - 0.4905\n",
      "\u001b[1;4mValidati 46     - 24     -          0.4321 -           0.8932    | 0.4858   - 0.0413\u001b[0m\n",
      "Training 47     - 102    -          0.2262 -           0.9247    | 0.6970   - 0.4860\n",
      "\u001b[1;4mValidati 47     - 24     -          0.4351 -           0.9040    | 0.5344   - 0.0411\u001b[0m\n",
      "Training 48     - 102    -          0.2431 -           0.9239    | 0.6934   - 0.4841\n",
      "\u001b[1;4mValidati 48     - 24     -          0.4286 -           0.9012    | 0.5341   - 0.0400\u001b[0m\n",
      "Training 49     - 102    -          0.2783 -           0.9261    | 0.7039   - 0.4840\n",
      "\u001b[1;4mValidati 49     - 24     -          0.4472 -           0.8970    | 0.4925   - 0.0420\u001b[0m\n",
      "Training 50     - 102    -          0.1717 -           0.9241    | 0.6948   - 0.4859\n",
      "\u001b[1;4mValidati 50     - 24     -          0.4526 -           0.9001    | 0.5283   - 0.0401\u001b[0m\n",
      "Training 51     - 102    -          0.1702 -           0.9257    | 0.7003   - 0.4855\n",
      "\u001b[1;4mValidati 51     - 24     -          0.4383 -           0.8956    | 0.4902   - 0.0400\u001b[0m\n",
      "Training 52     - 102    -          0.2205 -           0.9205    | 0.6807   - 0.4853\n",
      "\u001b[1;4mValidati 52     - 24     -          0.4359 -           0.8967    | 0.5123   - 0.0405\u001b[0m\n",
      "Training 53     - 102    -          0.1995 -           0.9233    | 0.6886   - 0.4844\n",
      "\u001b[1;4mValidati 53     - 24     -          0.4466 -           0.8949    | 0.4735   - 0.0406\u001b[0m\n",
      "Training 54     - 102    -          0.2344 -           0.9249    | 0.7027   - 0.4868\n",
      "\u001b[1;4mValidati 54     - 24     -          0.4483 -           0.8953    | 0.4834   - 0.0400\u001b[0m\n",
      "Training 55     - 102    -          0.1285 -           0.9285    | 0.7172   - 0.4885\n",
      "\u001b[1;4mValidati 55     - 24     -          0.4586 -           0.8925    | 0.4876   - 0.0411\u001b[0m\n",
      "Training 56     - 102    -          0.1922 -           0.9296    | 0.7201   - 0.4925\n",
      "\u001b[1;4mValidati 56     - 24     -          0.4288 -           0.8956    | 0.5103   - 0.0415\u001b[0m\n",
      "Training 57     - 102    -          0.2294 -           0.9270    | 0.7046   - 0.4925\n",
      "\u001b[1;4mValidati 57     - 24     -          0.4310 -           0.8974    | 0.4978   - 0.0401\u001b[0m\n",
      "Training 58     - 102    -          0.2976 -           0.9303    | 0.7196   - 0.4856\n",
      "\u001b[1;4mValidati 58     - 24     -          0.4346 -           0.8942    | 0.5033   - 0.0401\u001b[0m\n",
      "Training 59     - 102    -          0.2254 -           0.9261    | 0.7032   - 0.4857\n",
      "\u001b[1;4mValidati 59     - 24     -          0.4679 -           0.8970    | 0.5079   - 0.0404\u001b[0m\n",
      "Training 60     - 102    -          0.2314 -           0.9281    | 0.7139   - 0.4886\n",
      "\u001b[1;4mValidati 60     - 24     -          0.4497 -           0.8967    | 0.4808   - 0.0417\u001b[0m\n",
      "Training 61     - 102    -          0.1380 -           0.9264    | 0.7042   - 0.4859\n",
      "\u001b[1;4mValidati 61     - 24     -          0.4371 -           0.8963    | 0.4910   - 0.0404\u001b[0m\n",
      "Training 62     - 102    -          0.1907 -           0.9289    | 0.7198   - 0.4854\n",
      "\u001b[1;4mValidati 62     - 24     -          0.4610 -           0.9015    | 0.5318   - 0.0408\u001b[0m\n",
      "Training 63     - 102    -          0.1623 -           0.9292    | 0.7187   - 0.4845\n",
      "\u001b[1;4mValidati 63     - 24     -          0.4437 -           0.8967    | 0.4976   - 0.0398\u001b[0m\n",
      "Training 64     - 102    -          0.2664 -           0.9283    | 0.7171   - 0.4853\n",
      "\u001b[1;4mValidati 64     - 24     -          0.4441 -           0.8956    | 0.5013   - 0.0410\u001b[0m\n",
      "Training 65     - 102    -          0.1626 -           0.9316    | 0.7291   - 0.4870\n",
      "\u001b[1;4mValidati 65     - 24     -          0.4432 -           0.8960    | 0.5011   - 0.0400\u001b[0m\n",
      "Training 66     - 102    -          0.1293 -           0.9275    | 0.7139   - 0.4845\n",
      "\u001b[1;4mValidati 66     - 24     -          0.4301 -           0.8967    | 0.5018   - 0.0409\u001b[0m\n",
      "Training 67     - 102    -          0.2207 -           0.9305    | 0.7223   - 0.4870\n",
      "\u001b[1;4mValidati 67     - 24     -          0.4412 -           0.9001    | 0.5245   - 0.0407\u001b[0m\n",
      "Training 68     - 102    -          0.1711 -           0.9298    | 0.7239   - 0.4867\n",
      "\u001b[1;4mValidati 68     - 24     -          0.4441 -           0.9008    | 0.5368   - 0.0411\u001b[0m\n",
      "Training 69     - 102    -          0.1213 -           0.9312    | 0.7308   - 0.4853\n",
      "\u001b[1;4mValidati 69     - 24     -          0.4281 -           0.8998    | 0.5153   - 0.0412\u001b[0m\n",
      "Training 70     - 102    -          0.2642 -           0.9327    | 0.7337   - 0.4868\n",
      "\u001b[1;4mValidati 70     - 24     -          0.4393 -           0.9033    | 0.5229   - 0.0414\u001b[0m\n",
      "Training 71     - 102    -          0.1434 -           0.9323    | 0.7344   - 0.4879\n",
      "\u001b[1;4mValidati 71     - 24     -          0.4292 -           0.8984    | 0.5152   - 0.0411\u001b[0m\n",
      "Training 72     - 102    -          0.1666 -           0.9307    | 0.7255   - 0.4911\n",
      "\u001b[1;4mValidati 72     - 24     -          0.4422 -           0.9005    | 0.5173   - 0.0400\u001b[0m\n",
      "Training 73     - 102    -          0.1663 -           0.9354    | 0.7486   - 0.4899\n",
      "\u001b[1;4mValidati 73     - 24     -          0.4481 -           0.8939    | 0.4906   - 0.0412\u001b[0m\n",
      "Training 74     - 102    -          0.2273 -           0.9342    | 0.7411   - 0.4945\n",
      "\u001b[1;4mValidati 74     - 24     -          0.4517 -           0.8981    | 0.5214   - 0.0411\u001b[0m\n",
      "Training 75     - 102    -          0.1110 -           0.9358    | 0.7513   - 0.4921\n",
      "\u001b[1;4mValidati 75     - 24     -          0.4390 -           0.8956    | 0.5080   - 0.0415\u001b[0m\n",
      "Training 76     - 102    -          0.1752 -           0.9375    | 0.7571   - 0.4894\n",
      "\u001b[1;4mValidati 76     - 24     -          0.4500 -           0.9029    | 0.5338   - 0.0403\u001b[0m\n",
      "Training 77     - 102    -          0.1874 -           0.9326    | 0.7370   - 0.4886\n",
      "\u001b[1;4mValidati 77     - 24     -          0.4585 -           0.8949    | 0.4941   - 0.0398\u001b[0m\n",
      "Training 78     - 102    -          0.1737 -           0.9309    | 0.7298   - 0.4874\n",
      "\u001b[1;4mValidati 78     - 24     -          0.4386 -           0.9001    | 0.5254   - 0.0403\u001b[0m\n",
      "Training 79     - 102    -          0.1386 -           0.9345    | 0.7458   - 0.4886\n",
      "\u001b[1;4mValidati 79     - 24     -          0.4440 -           0.8991    | 0.5204   - 0.0406\u001b[0m\n",
      "Training 80     - 102    -          0.1489 -           0.9353    | 0.7459   - 0.4873\n",
      "\u001b[1;4mValidati 80     - 24     -          0.4298 -           0.8984    | 0.5231   - 0.0404\u001b[0m\n",
      "Training 81     - 102    -          0.0997 -           0.9390    | 0.7640   - 0.4868\n",
      "\u001b[1;4mValidati 81     - 24     -          0.4331 -           0.8984    | 0.5147   - 0.0401\u001b[0m\n",
      "Training 82     - 102    -          0.2577 -           0.9369    | 0.7536   - 0.4899\n",
      "\u001b[1;4mValidati 82     - 24     -          0.4294 -           0.8998    | 0.5078   - 0.0411\u001b[0m\n",
      "Training 83     - 102    -          0.1152 -           0.9350    | 0.7505   - 0.4930\n",
      "\u001b[1;4mValidati 83     - 24     -          0.4412 -           0.9001    | 0.5271   - 0.0404\u001b[0m\n",
      "Training 84     - 102    -          0.2685 -           0.9321    | 0.7384   - 0.4911\n",
      "\u001b[1;4mValidati 84     - 24     -          0.4346 -           0.8946    | 0.4953   - 0.0401\u001b[0m\n",
      "Training 85     - 102    -          0.1838 -           0.9366    | 0.7499   - 0.4896\n",
      "\u001b[1;4mValidati 85     - 24     -          0.4397 -           0.9061    | 0.5553   - 0.0405\u001b[0m\n",
      "Training 86     - 102    -          0.2391 -           0.9386    | 0.7611   - 0.4865\n",
      "\u001b[1;4mValidati 86     - 24     -          0.4355 -           0.9022    | 0.5293   - 0.0400\u001b[0m\n",
      "Training 87     - 102    -          0.2572 -           0.9368    | 0.7547   - 0.4886\n",
      "\u001b[1;4mValidati 87     - 24     -          0.4491 -           0.8956    | 0.4942   - 0.0405\u001b[0m\n",
      "Training 88     - 102    -          0.1263 -           0.9377    | 0.7566   - 0.4879\n",
      "\u001b[1;4mValidati 88     - 24     -          0.4307 -           0.9033    | 0.5396   - 0.0401\u001b[0m\n",
      "Training 89     - 102    -          0.1172 -           0.9380    | 0.7591   - 0.4889\n",
      "\u001b[1;4mValidati 89     - 24     -          0.4360 -           0.8953    | 0.5109   - 0.0411\u001b[0m\n",
      "Training 90     - 102    -          0.1230 -           0.9374    | 0.7596   - 0.4894\n",
      "\u001b[1;4mValidati 90     - 24     -          0.4342 -           0.8995    | 0.5187   - 0.0404\u001b[0m\n",
      "Training 91     - 102    -          0.1249 -           0.9412    | 0.7725   - 0.4881\n",
      "\u001b[1;4mValidati 91     - 24     -          0.3802 -           0.9029    | 0.5396   - 0.0402\u001b[0m\n",
      "Training 92     - 102    -          0.1249 -           0.9384    | 0.7630   - 0.4896\n",
      "\u001b[1;4mValidati 92     - 24     -          0.4351 -           0.9005    | 0.5394   - 0.0408\u001b[0m\n",
      "Training 93     - 102    -          0.1241 -           0.9387    | 0.7624   - 0.4889\n",
      "\u001b[1;4mValidati 93     - 24     -          0.4512 -           0.9026    | 0.5289   - 0.0416\u001b[0m\n",
      "Training 94     - 102    -          0.1306 -           0.9396    | 0.7667   - 0.4921\n",
      "\u001b[1;4mValidati 94     - 24     -          0.4512 -           0.9005    | 0.5049   - 0.0413\u001b[0m\n",
      "Training 95     - 102    -          0.1868 -           0.9419    | 0.7768   - 0.4884\n",
      "\u001b[1;4mValidati 95     - 24     -          0.4469 -           0.9022    | 0.5405   - 0.0400\u001b[0m\n",
      "Training 96     - 102    -          0.0832 -           0.9361    | 0.7512   - 0.4892\n",
      "\u001b[1;4mValidati 96     - 24     -          0.4382 -           0.8991    | 0.5063   - 0.0404\u001b[0m\n",
      "Training 97     - 102    -          0.0978 -           0.9389    | 0.7673   - 0.4883\n",
      "\u001b[1;4mValidati 97     - 24     -          0.4293 -           0.9001    | 0.5229   - 0.0401\u001b[0m\n",
      "Training 98     - 102    -          0.1501 -           0.9404    | 0.7700   - 0.4888\n",
      "\u001b[1;4mValidati 98     - 24     -          0.4417 -           0.8988    | 0.5152   - 0.0406\u001b[0m\n",
      "Training 99     - 102    -          0.1805 -           0.9377    | 0.7592   - 0.4875\n",
      "\u001b[1;4mValidati 99     - 24     -          0.4399 -           0.9005    | 0.5281   - 0.0402\u001b[0m\n",
      "Training 100    - 102    -          0.1457 -           0.9433    | 0.7824   - 0.4888\n",
      "\u001b[1;4mValidati 100    - 24     -          0.4564 -           0.8998    | 0.5317   - 0.0402\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
