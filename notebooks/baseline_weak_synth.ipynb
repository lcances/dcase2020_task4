{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53, # The model output localisation with a résolution of ~ 18ms --> 53 temporal bins\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak ans synthetic20 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:14<00:00, 510.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: synthetic20\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4251/4251 [00:08<00:00, 479.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_val_subset >>> Loading dataset: validation\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/validation\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "manager.add_subset(\"synthetic20\")\n",
    "manager.add_subset(\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset\n",
    "\n",
    "- We want both the weak and strong ground truth --> the *weak* and *strong* parameters to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, weak=True, strong=True, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, weak=True, strong=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4049, 1058)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "This model is the same than the weak baseline but have an extra output. <br />\n",
    "the loc_output is compose of a single convolution layer with nb_filters == nb_class. <br />\n",
    "Since their is some pooling layer, the *loc_ouput* have a precision of 53 bins (~= 18 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakStrongBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (weak_output): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       "  (strong_output): Sequential(\n",
       "    (0): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakStrongBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_weak_output.Flatten_0                  -         [1, 1696]       -   \n",
      "16_weak_output.Dropout_1                  -         [1, 1696]       -   \n",
      "17_weak_output.Linear_2          [1696, 10]           [1, 10]  16.97k   \n",
      "18_strong_output.Conv2d_0    [32, 10, 1, 1]    [1, 10, 1, 53]   330.0   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_weak_output.Flatten_0             -  \n",
      "16_weak_output.Dropout_1             -  \n",
      "17_weak_output.Linear_2         16.96k  \n",
      "18_strong_output.Conv2d_0       16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             36.308k\n",
      "Trainable params         36.308k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.625696M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom loss function\n",
    "\n",
    "Since not all file have strong truth, it is necessary to remove those files. <br />\n",
    "For that, the strong mask is computed. If the sum of the strong ground truth is equal to 0 then it is a fake one <br />\n",
    "This file strong loss must not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_synth_loss(logits_weak, logits_strong, y_weak, y_strong, reduce: str = \"mean\"):\n",
    "    assert reduce in [\"mean\", \"sum\"], \"support only \\\"mean\\\" and \\\"sum\\\"\"\n",
    "    \n",
    "    #  Reduction function\n",
    "    if reduce == \"mean\":\n",
    "        reduce_fn = torch.mean\n",
    "    elif reduce == \"sum\":\n",
    "        reduce_fn = torch.sum\n",
    "    \n",
    "    # based on Binary Cross Entropy loss\n",
    "    weak_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    strong_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    # calc separate loss function\n",
    "    weak_bce = weak_criterion(logits_weak, y_weak)\n",
    "    strong_bce = strong_criterion(logits_strong, y_strong)\n",
    "    \n",
    "    weak_bce = reduce_fn(weak_bce, dim=1)\n",
    "    strong_bce = reduce_fn(strong_bce, dim=(1, 2))\n",
    "    \n",
    "    # calc strong mask\n",
    "    strong_mask = torch.clamp(torch.sum(y_strong, dim=(1, 2)), 0, 1) # vector of 0 or 1\n",
    "    strong_mask = strong_mask.detach() # declared not to need gradients\n",
    "    \n",
    "    # Output the different loss for logging purpose\n",
    "    weak_loss = reduce_fn(weak_bce)\n",
    "    strong_loss = reduce_fn(strong_mask * strong_bce)\n",
    "    total_loss = reduce_fn(weak_bce + strong_mask * strong_bce)\n",
    "    \n",
    "    return weak_loss, strong_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "weak_binacc_func = BinaryAccuracy()\n",
    "strong_binacc_func = BinaryAccuracy()\n",
    "weak_f_func = FScore()\n",
    "strong_f_func = FScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_metrics():\n",
    "    metrics = [weak_binacc_func, strong_binacc_func, weak_f_func, strong_f_func]\n",
    "    \n",
    "    for m in metrics:\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6}| {:<8.8}| {:<6.6} - {:<9.9} {:<10.10}| {:<12.12}| {:<9.9}| {:<11.11}- {:<6.6}\"\n",
    "\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f}| {:<8.4f}| {:<6.4f} - {:<9.9} {:<10.4f}| {:<12.4f}| {:<9.4f}| {:<11.4f}- {:<6.4f}\"\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"Strong \", \"Total \", \"metrics: \", \"Weak acc \", \"Strong acc \", \"Weak F1 \", \"Strong F1\", \"Time\"\n",
    ")\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X, y) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X = X.cuda().float()\n",
    "        y_weak = y[0].cuda().float()\n",
    "        y_strong = y[1].cuda().float()\n",
    "        \n",
    "        weak_logits, strong_logits = model(X)\n",
    "        \n",
    "        # calc the loss\n",
    "        weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "            weak_logits, strong_logits,\n",
    "            y_weak, y_strong,\n",
    "            reduce=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"train/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"train/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "        \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y_weak = y[0].cuda().float()\n",
    "            y_strong = y[1].cuda().float()\n",
    "\n",
    "            weak_logits, strong_logits = model(X)\n",
    "\n",
    "            # calc the loss\n",
    "            weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "                weak_logits, strong_logits,\n",
    "                y_weak, y_strong,\n",
    "                reduce=\"mean\"\n",
    "            )\n",
    "            \n",
    "             # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"val/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n",
      "\n",
      "Training 1      - 100    -          0.4003| 0.1050  | 0.5054 -           0.8207    | 0.9152      | 0.2971   | 0.0202     - 54.7130\n",
      "\u001b[1;4mValidati 1      - 26     -          0.2930| 0.2575  | 0.5505 -           0.8545    | 0.9185      | 0.2466   | 0.0000     - 13.5591\u001b[0m\n",
      "Training 2      - 100    -          0.4832| 0.1367  | 0.6199 -           0.8306    | 0.9655      | 0.3361   | 0.0023     - 2.0243\n",
      "\u001b[1;4mValidati 2      - 26     -          0.2459| 0.2523  | 0.4981 -           0.8708    | 0.9185      | 0.3607   | 0.0011     - 0.2242\u001b[0m\n",
      "Training 3      - 100    -          0.4249| 0.1602  | 0.5851 -           0.8335    | 0.9657      | 0.3345   | 0.0385     - 1.9953\n",
      "\u001b[1;4mValidati 3      - 26     -          0.2938| 0.2741  | 0.5679 -           0.8699    | 0.9207      | 0.3271   | 0.0541     - 0.2385\u001b[0m\n",
      "Training 4      - 100    -          0.3776| 0.1321  | 0.5096 -           0.8405    | 0.9661      | 0.3658   | 0.1086     - 1.9660\n",
      "\u001b[1;4mValidati 4      - 26     -          0.2954| 0.2579  | 0.5533 -           0.8776    | 0.9241      | 0.4412   | 0.1335     - 0.2373\u001b[0m\n",
      "Training 5      - 100    -          0.3441| 0.0996  | 0.4437 -           0.8451    | 0.9661      | 0.3769   | 0.1364     - 1.9467\n",
      "\u001b[1;4mValidati 5      - 26     -          0.3204| 0.2727  | 0.5931 -           0.8632    | 0.9275      | 0.4686   | 0.2204     - 0.2244\u001b[0m\n",
      "Training 6      - 100    -          0.3221| 0.1255  | 0.4475 -           0.8496    | 0.9662      | 0.3961   | 0.1613     - 1.9554\n",
      "\u001b[1;4mValidati 6      - 26     -          0.2965| 0.2553  | 0.5518 -           0.8835    | 0.9253      | 0.4254   | 0.1629     - 0.2278\u001b[0m\n",
      "Training 7      - 100    -          0.3553| 0.1067  | 0.4620 -           0.8562    | 0.9664      | 0.4243   | 0.1860     - 1.9410\n",
      "\u001b[1;4mValidati 7      - 26     -          0.3327| 0.2893  | 0.6219 -           0.8857    | 0.9281      | 0.4440   | 0.2251     - 0.2249\u001b[0m\n",
      "Training 8      - 100    -          0.3451| 0.0620  | 0.4071 -           0.8576    | 0.9661      | 0.4279   | 0.1882     - 1.9436\n",
      "\u001b[1;4mValidati 8      - 26     -          0.2929| 0.2512  | 0.5441 -           0.8874    | 0.9304      | 0.5091   | 0.2694     - 0.2269\u001b[0m\n",
      "Training 9      - 100    -          0.3660| 0.0971  | 0.4631 -           0.8603    | 0.9661      | 0.4438   | 0.2002     - 1.9463\n",
      "\u001b[1;4mValidati 9      - 26     -          0.3049| 0.2180  | 0.5229 -           0.8887    | 0.9287      | 0.4578   | 0.2260     - 0.2252\u001b[0m\n",
      "Training 10     - 100    -          0.3013| 0.0589  | 0.3603 -           0.8615    | 0.9664      | 0.4430   | 0.2175     - 1.9516\n",
      "\u001b[1;4mValidati 10     - 26     -          0.2983| 0.2292  | 0.5276 -           0.8826    | 0.9268      | 0.4368   | 0.2086     - 0.2280\u001b[0m\n",
      "Training 11     - 100    -          0.3903| 0.1122  | 0.5025 -           0.8633    | 0.9663      | 0.4585   | 0.2295     - 1.9623\n",
      "\u001b[1;4mValidati 11     - 26     -          0.3234| 0.2608  | 0.5842 -           0.8874    | 0.9297      | 0.4520   | 0.2548     - 0.2277\u001b[0m\n",
      "Training 12     - 100    -          0.3967| 0.0922  | 0.4889 -           0.8665    | 0.9666      | 0.4693   | 0.2432     - 1.9636\n",
      "\u001b[1;4mValidati 12     - 26     -          0.3420| 0.2293  | 0.5713 -           0.8845    | 0.9299      | 0.4951   | 0.2599     - 0.2304\u001b[0m\n",
      "Training 13     - 100    -          0.3256| 0.0964  | 0.4219 -           0.8674    | 0.9668      | 0.4761   | 0.2605     - 1.9576\n",
      "\u001b[1;4mValidati 13     - 26     -          0.3027| 0.2122  | 0.5148 -           0.8870    | 0.9283      | 0.4448   | 0.2310     - 0.2256\u001b[0m\n",
      "Training 14     - 100    -          0.4703| 0.1295  | 0.5998 -           0.8709    | 0.9667      | 0.4957   | 0.2760     - 1.9563\n",
      "\u001b[1;4mValidati 14     - 26     -          0.3278| 0.2188  | 0.5466 -           0.8916    | 0.9292      | 0.4863   | 0.2500     - 0.2263\u001b[0m\n",
      "Training 15     - 100    -          0.2530| 0.0853  | 0.3383 -           0.8696    | 0.9671      | 0.4930   | 0.2802     - 1.9600\n",
      "\u001b[1;4mValidati 15     - 26     -          0.3643| 0.2124  | 0.5766 -           0.8857    | 0.9273      | 0.4821   | 0.2166     - 0.2279\u001b[0m\n",
      "Training 16     - 100    -          0.3340| 0.0937  | 0.4277 -           0.8720    | 0.9669      | 0.5054   | 0.2895     - 1.9572\n",
      "\u001b[1;4mValidati 16     - 26     -          0.3513| 0.2107  | 0.5621 -           0.8931    | 0.9313      | 0.5108   | 0.2900     - 0.2282\u001b[0m\n",
      "Training 17     - 100    -          0.3372| 0.1054  | 0.4426 -           0.8727    | 0.9670      | 0.5099   | 0.2971     - 1.9537\n",
      "\u001b[1;4mValidati 17     - 26     -          0.3396| 0.2150  | 0.5546 -           0.8672    | 0.9240      | 0.4560   | 0.2701     - 0.2256\u001b[0m\n",
      "Training 18     - 100    -          0.3384| 0.1017  | 0.4401 -           0.8749    | 0.9668      | 0.5164   | 0.2918     - 1.9567\n",
      "\u001b[1;4mValidati 18     - 26     -          0.2924| 0.2104  | 0.5028 -           0.8937    | 0.9312      | 0.5113   | 0.2841     - 0.2252\u001b[0m\n",
      "Training 19     - 100    -          0.3382| 0.0716  | 0.4098 -           0.8750    | 0.9666      | 0.5212   | 0.3029     - 1.9679\n",
      "\u001b[1;4mValidati 19     - 26     -          0.3470| 0.2104  | 0.5574 -           0.8983    | 0.9326      | 0.5371   | 0.3105     - 0.2258\u001b[0m\n",
      "Training 20     - 100    -          0.3768| 0.0874  | 0.4641 -           0.8774    | 0.9670      | 0.5296   | 0.3083     - 1.9570\n",
      "\u001b[1;4mValidati 20     - 26     -          0.3499| 0.2121  | 0.5620 -           0.8926    | 0.9316      | 0.5275   | 0.2960     - 0.2212\u001b[0m\n",
      "Training 21     - 100    -          0.3620| 0.1059  | 0.4679 -           0.8784    | 0.9670      | 0.5373   | 0.3136     - 1.9884\n",
      "\u001b[1;4mValidati 21     - 26     -          0.3305| 0.2104  | 0.5410 -           0.8985    | 0.9329      | 0.5417   | 0.3193     - 0.2437\u001b[0m\n",
      "Training 22     - 100    -          0.3206| 0.0832  | 0.4038 -           0.8812    | 0.9671      | 0.5497   | 0.3314     - 2.1090\n",
      "\u001b[1;4mValidati 22     - 26     -          0.2953| 0.2064  | 0.5016 -           0.8950    | 0.9327      | 0.5430   | 0.3122     - 0.2262\u001b[0m\n",
      "Training 23     - 100    -          0.2965| 0.0649  | 0.3615 -           0.8837    | 0.9667      | 0.5613   | 0.3308     - 1.9782\n",
      "\u001b[1;4mValidati 23     - 26     -          0.3441| 0.2109  | 0.5551 -           0.8922    | 0.9310      | 0.5186   | 0.2826     - 0.2270\u001b[0m\n",
      "Training 24     - 100    -          0.2358| 0.0493  | 0.2851 -           0.8842    | 0.9671      | 0.5649   | 0.3377     - 2.0374\n",
      "\u001b[1;4mValidati 24     - 26     -          0.3443| 0.2104  | 0.5547 -           0.8948    | 0.9301      | 0.5027   | 0.2614     - 0.2487\u001b[0m\n",
      "Training 25     - 100    -          0.2726| 0.1166  | 0.3892 -           0.8846    | 0.9671      | 0.5682   | 0.3371     - 1.9730\n",
      "\u001b[1;4mValidati 25     - 26     -          0.3327| 0.2075  | 0.5402 -           0.8951    | 0.9302      | 0.5166   | 0.2662     - 0.2241\u001b[0m\n",
      "Training 26     - 100    -          0.3149| 0.0756  | 0.3906 -           0.8835    | 0.9671      | 0.5600   | 0.3386     - 1.9673\n",
      "\u001b[1;4mValidati 26     - 26     -          0.3326| 0.2093  | 0.5420 -           0.8969    | 0.9320      | 0.5446   | 0.3055     - 0.2298\u001b[0m\n",
      "Training 27     - 100    -          0.3192| 0.1210  | 0.4402 -           0.8867    | 0.9667      | 0.5727   | 0.3353     - 1.9759\n",
      "\u001b[1;4mValidati 27     - 26     -          0.3464| 0.2092  | 0.5556 -           0.8941    | 0.9311      | 0.5239   | 0.2856     - 0.2269\u001b[0m\n",
      "Training 28     - 100    -          0.3122| 0.1053  | 0.4176 -           0.8856    | 0.9675      | 0.5678   | 0.3537     - 1.9538\n",
      "\u001b[1;4mValidati 28     - 26     -          0.3271| 0.2134  | 0.5405 -           0.8976    | 0.9313      | 0.5411   | 0.3058     - 0.2255\u001b[0m\n",
      "Training 29     - 100    -          0.2641| 0.0724  | 0.3365 -           0.8867    | 0.9672      | 0.5769   | 0.3563     - 1.9646\n",
      "\u001b[1;4mValidati 29     - 26     -          0.3363| 0.2067  | 0.5430 -           0.8994    | 0.9326      | 0.5422   | 0.3275     - 0.2227\u001b[0m\n",
      "Training 30     - 100    -          0.2487| 0.0513  | 0.3001 -           0.8894    | 0.9671      | 0.5866   | 0.3571     - 1.9608\n",
      "\u001b[1;4mValidati 30     - 26     -          0.3176| 0.2116  | 0.5292 -           0.8895    | 0.9309      | 0.5073   | 0.3128     - 0.2200\u001b[0m\n",
      "Training 31     - 100    -          0.2161| 0.1148  | 0.3309 -           0.8882    | 0.9670      | 0.5828   | 0.3609     - 1.9666\n",
      "\u001b[1;4mValidati 31     - 26     -          0.3556| 0.2060  | 0.5615 -           0.8996    | 0.9320      | 0.5465   | 0.3050     - 0.2296\u001b[0m\n",
      "Training 32     - 100    -          0.2674| 0.0674  | 0.3347 -           0.8895    | 0.9667      | 0.5888   | 0.3554     - 1.9606\n",
      "\u001b[1;4mValidati 32     - 26     -          0.3325| 0.2149  | 0.5474 -           0.8994    | 0.9325      | 0.5452   | 0.3127     - 0.2252\u001b[0m\n",
      "Training 33     - 100    -          0.2401| 0.0655  | 0.3057 -           0.8898    | 0.9671      | 0.5922   | 0.3589     - 1.9890\n",
      "\u001b[1;4mValidati 33     - 26     -          0.3119| 0.2125  | 0.5244 -           0.8951    | 0.9317      | 0.5215   | 0.2989     - 0.2256\u001b[0m\n",
      "Training 34     - 100    -          0.2875| 0.0487  | 0.3361 -           0.8895    | 0.9677      | 0.5870   | 0.3713     - 2.0424\n",
      "\u001b[1;4mValidati 34     - 26     -          0.3537| 0.2109  | 0.5646 -           0.9012    | 0.9326      | 0.5517   | 0.3103     - 0.2442\u001b[0m\n",
      "Training 35     - 100    -          0.3510| 0.1025  | 0.4535 -           0.8911    | 0.9670      | 0.5972   | 0.3672     - 1.9719\n",
      "\u001b[1;4mValidati 35     - 26     -          0.3436| 0.2127  | 0.5563 -           0.8968    | 0.9326      | 0.5443   | 0.3170     - 0.2238\u001b[0m\n",
      "Training 36     - 100    -          0.2590| 0.0744  | 0.3334 -           0.8909    | 0.9669      | 0.5967   | 0.3646     - 1.9721\n",
      "\u001b[1;4mValidati 36     - 26     -          0.3305| 0.2032  | 0.5337 -           0.8902    | 0.9313      | 0.5235   | 0.2977     - 0.2269\u001b[0m\n",
      "Training 37     - 100    -          0.3147| 0.0805  | 0.3952 -           0.8918    | 0.9670      | 0.5988   | 0.3681     - 1.9780\n",
      "\u001b[1;4mValidati 37     - 26     -          0.3299| 0.2070  | 0.5369 -           0.8994    | 0.9316      | 0.5350   | 0.2983     - 0.2232\u001b[0m\n",
      "Training 38     - 100    -          0.2522| 0.0733  | 0.3255 -           0.8946    | 0.9672      | 0.6102   | 0.3701     - 1.9928\n",
      "\u001b[1;4mValidati 38     - 26     -          0.3279| 0.2046  | 0.5325 -           0.8997    | 0.9327      | 0.5477   | 0.3211     - 0.2258\u001b[0m\n",
      "Training 39     - 100    -          0.2905| 0.1046  | 0.3951 -           0.8934    | 0.9671      | 0.6056   | 0.3695     - 1.9549\n",
      "\u001b[1;4mValidati 39     - 26     -          0.3285| 0.2080  | 0.5365 -           0.9002    | 0.9333      | 0.5504   | 0.3311     - 0.2189\u001b[0m\n",
      "Training 40     - 100    -          0.2488| 0.0995  | 0.3483 -           0.8949    | 0.9668      | 0.6149   | 0.3757     - 1.9672\n",
      "\u001b[1;4mValidati 40     - 26     -          0.3236| 0.2071  | 0.5307 -           0.8977    | 0.9304      | 0.5288   | 0.2824     - 0.2268\u001b[0m\n",
      "Training 41     - 100    -          0.2401| 0.0943  | 0.3344 -           0.8946    | 0.9666      | 0.6115   | 0.3741     - 1.9627\n",
      "\u001b[1;4mValidati 41     - 26     -          0.3162| 0.2029  | 0.5191 -           0.8973    | 0.9324      | 0.5447   | 0.3390     - 0.2267\u001b[0m\n",
      "Training 42     - 100    -          0.2813| 0.0980  | 0.3793 -           0.8943    | 0.9664      | 0.6128   | 0.3725     - 1.9658\n",
      "\u001b[1;4mValidati 42     - 26     -          0.3204| 0.2024  | 0.5228 -           0.8998    | 0.9324      | 0.5488   | 0.3218     - 0.2233\u001b[0m\n",
      "Training 43     - 100    -          0.3171| 0.0630  | 0.3802 -           0.8956    | 0.9664      | 0.6177   | 0.3695     - 2.0095\n",
      "\u001b[1;4mValidati 43     - 26     -          0.3281| 0.2183  | 0.5464 -           0.9032    | 0.9339      | 0.5706   | 0.3437     - 0.2272\u001b[0m\n",
      "Training 44     - 100    -          0.3239| 0.0768  | 0.4007 -           0.8959    | 0.9670      | 0.6161   | 0.3790     - 1.9864\n",
      "\u001b[1;4mValidati 44     - 26     -          0.3147| 0.2046  | 0.5193 -           0.8977    | 0.9329      | 0.5461   | 0.3273     - 0.2456\u001b[0m\n",
      "Training 45     - 100    -          0.3751| 0.0673  | 0.4424 -           0.8945    | 0.9667      | 0.6123   | 0.3758     - 1.9734\n",
      "\u001b[1;4mValidati 45     - 26     -          0.3280| 0.2031  | 0.5311 -           0.8972    | 0.9307      | 0.5155   | 0.2827     - 0.2248\u001b[0m\n",
      "Training 46     - 8      -          0.2781| 0.0671  | 0.3452 -           0.8955    | 0.9664      | 0.6194   | 0.3680     - 0.1678\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
