{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53, # The model output localisation with a résolution of ~ 18ms --> 53 temporal bins\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak ans synthetic20 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:14<00:00, 532.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: synthetic20\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4251/4251 [00:08<00:00, 507.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_val_subset >>> Loading dataset: validation\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/validation\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "manager.add_subset(\"synthetic20\")\n",
    "manager.add_subset(\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset\n",
    "\n",
    "- We want both the weak and strong ground truth --> the *weak* and *strong* parameters to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, weak=True, strong=True, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, weak=True, strong=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "This model is the same than the weak baseline but have an extra output. <br />\n",
    "the loc_output is compose of a single convolution layer with nb_filters == nb_class. <br />\n",
    "Since their is some pooling layer, the *loc_ouput* have a precision of 53 bins (~= 18 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakStrongBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (weak_output): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       "  (strong_output): Sequential(\n",
       "    (0): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakStrongBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_weak_output.Flatten_0                  -         [1, 1696]       -   \n",
      "16_weak_output.Dropout_1                  -         [1, 1696]       -   \n",
      "17_weak_output.Linear_2          [1696, 10]           [1, 10]  16.97k   \n",
      "18_strong_output.Conv2d_0    [32, 10, 1, 1]    [1, 10, 1, 53]   330.0   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_weak_output.Flatten_0             -  \n",
      "16_weak_output.Dropout_1             -  \n",
      "17_weak_output.Linear_2         16.96k  \n",
      "18_strong_output.Conv2d_0       16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             36.308k\n",
      "Trainable params         36.308k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.625696M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom loss function\n",
    "\n",
    "Since not all file have strong truth, it is necessary to remove those files. <br />\n",
    "For that, the strong mask is computed. If the sum of the strong ground truth is equal to 0 then it is a fake one <br />\n",
    "This file strong loss must not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_synth_loss(logits_weak, logits_strong, y_weak, y_strong, reduce: str = \"mean\"):\n",
    "    assert reduce in [\"mean\", \"sum\"], \"support only \\\"mean\\\" and \\\"sum\\\"\"\n",
    "    \n",
    "    #  Reduction function\n",
    "    if reduce == \"mean\":\n",
    "        reduce_fn = torch.mean\n",
    "    elif reduce == \"sum\":\n",
    "        reduce_fn = torch.sum\n",
    "    \n",
    "    # based on Binary Cross Entropy loss\n",
    "    weak_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    strong_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    # calc separate loss function\n",
    "    weak_bce = weak_criterion(logits_weak, y_weak)\n",
    "    strong_bce = strong_criterion(logits_strong, y_strong)\n",
    "    \n",
    "    weak_bce = reduce_fn(weak_bce, dim=1)\n",
    "    strong_bce = reduce_fn(strong_bce, dim=(1, 2))\n",
    "    \n",
    "    # calc strong mask\n",
    "    strong_mask = torch.clamp(torch.sum(y_strong, dim=(1, 2)), 0, 1) # vector of 0 or 1\n",
    "    strong_mask = strong_mask.detach() # declared not to need gradients\n",
    "    \n",
    "    # Output the different loss for logging purpose\n",
    "    weak_loss = reduce_fn(weak_bce)\n",
    "    strong_loss = reduce_fn(strong_mask * strong_bce)\n",
    "    total_loss = reduce_fn(weak_bce + strong_mask * strong_bce)\n",
    "    \n",
    "    return weak_loss, strong_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "weak_binacc_func = BinaryAccuracy()\n",
    "strong_binacc_func = BinaryAccuracy()\n",
    "weak_f_func = FScore()\n",
    "strong_f_func = FScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_metrics():\n",
    "    metrics = [weak_binacc_func, strong_binacc_func, weak_f_func, strong_f_func]\n",
    "    \n",
    "    for m in metrics:\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6}| {:<8.8}| {:<6.6} - {:<9.9} {:<10.10}| {:<12.12}| {:<9.9}| {:<11.11}- {:<6.6}\"\n",
    "\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f}| {:<8.4f}| {:<6.4f} - {:<9.9} {:<10.4f}| {:<12.4f}| {:<9.4f}| {:<11.4f}- {:<6.4f}\"\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"Strong \", \"Total \", \"metrics: \", \"Weak acc \", \"Strong acc \", \"Weak F1 \", \"Strong F1\", \"Time\"\n",
    ")\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X, y) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X = X.cuda().float()\n",
    "        y_weak = y[0].cuda().float()\n",
    "        y_strong = y[1].cuda().float()\n",
    "        \n",
    "        weak_logits, strong_logits = model(X)\n",
    "        \n",
    "        # calc the loss\n",
    "        weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "            weak_logits, strong_logits,\n",
    "            y_weak, y_strong,\n",
    "            reduce=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"train/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"train/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "        \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y_weak = y[0].cuda().float()\n",
    "            y_strong = y[1].cuda().float()\n",
    "\n",
    "            weak_logits, strong_logits = model(X)\n",
    "\n",
    "            # calc the loss\n",
    "            weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "                weak_logits, strong_logits,\n",
    "                y_weak, y_strong,\n",
    "                reduce=\"mean\"\n",
    "            )\n",
    "            \n",
    "             # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"val/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n",
      "\n",
      "Training 1      - 100    -          0.4163| 0.1561  | 0.5724 -           0.8208    | 0.9025      | 0.2979   | 0.0268     - 42.7076\n",
      "\u001b[1;4mValidati 1      - 22     -          0.4995| 0.2660  | 0.7656 -           0.8406    | 0.9640      | 0.3367   | 0.0000     - 9.0982\u001b[0m\n",
      "Training 2      - 100    -          0.4049| 0.1519  | 0.5568 -           0.8289    | 0.9658      | 0.3311   | 0.0061     - 1.5662\n",
      "\u001b[1;4mValidati 2      - 22     -          0.4473| 0.2501  | 0.6974 -           0.8468    | 0.9640      | 0.3485   | 0.0078     - 0.1403\u001b[0m\n",
      "Training 3      - 100    -          0.4195| 0.1358  | 0.5553 -           0.8337    | 0.9659      | 0.3482   | 0.0544     - 1.5193\n",
      "\u001b[1;4mValidati 3      - 22     -          0.5474| 0.2127  | 0.7600 -           0.8510    | 0.9654      | 0.3666   | 0.0798     - 0.1412\u001b[0m\n",
      "Training 4      - 100    -          0.4503| 0.0993  | 0.5496 -           0.8392    | 0.9660      | 0.3701   | 0.1213     - 1.5262\n",
      "\u001b[1;4mValidati 4      - 22     -          0.4491| 0.2587  | 0.7078 -           0.8557    | 0.9649      | 0.3607   | 0.1475     - 0.1410\u001b[0m\n",
      "Training 5      - 100    -          0.3698| 0.1253  | 0.4951 -           0.8437    | 0.9662      | 0.3848   | 0.1443     - 1.5249\n",
      "\u001b[1;4mValidati 5      - 22     -          0.5051| 0.2229  | 0.7279 -           0.8594    | 0.9656      | 0.4054   | 0.1415     - 0.1417\u001b[0m\n",
      "Training 6      - 100    -          0.3957| 0.1045  | 0.5003 -           0.8469    | 0.9661      | 0.4057   | 0.1587     - 1.5169\n",
      "\u001b[1;4mValidati 6      - 22     -          0.4438| 0.2601  | 0.7039 -           0.8675    | 0.9654      | 0.3893   | 0.1511     - 0.1407\u001b[0m\n",
      "Training 7      - 100    -          0.3834| 0.1187  | 0.5021 -           0.8507    | 0.9658      | 0.4216   | 0.1776     - 1.5287\n",
      "\u001b[1;4mValidati 7      - 22     -          0.4654| 0.2393  | 0.7047 -           0.8654    | 0.9659      | 0.3684   | 0.1341     - 0.1393\u001b[0m\n",
      "Training 8      - 100    -          0.3468| 0.1490  | 0.4958 -           0.8549    | 0.9656      | 0.4324   | 0.1799     - 1.5196\n",
      "\u001b[1;4mValidati 8      - 22     -          0.4529| 0.2410  | 0.6939 -           0.8681    | 0.9659      | 0.4039   | 0.1376     - 0.1386\u001b[0m\n",
      "Training 9      - 100    -          0.3595| 0.0999  | 0.4593 -           0.8573    | 0.9661      | 0.4437   | 0.1928     - 1.5261\n",
      "\u001b[1;4mValidati 9      - 22     -          0.4272| 0.2576  | 0.6849 -           0.8704    | 0.9659      | 0.4455   | 0.1496     - 0.1390\u001b[0m\n",
      "Training 10     - 100    -          0.4341| 0.1102  | 0.5443 -           0.8584    | 0.9659      | 0.4533   | 0.2105     - 1.5221\n",
      "\u001b[1;4mValidati 10     - 22     -          0.4275| 0.2093  | 0.6369 -           0.8747    | 0.9665      | 0.4351   | 0.1749     - 0.1395\u001b[0m\n",
      "Training 11     - 100    -          0.3634| 0.0982  | 0.4616 -           0.8585    | 0.9660      | 0.4532   | 0.2273     - 1.5099\n",
      "\u001b[1;4mValidati 11     - 22     -          0.4214| 0.2262  | 0.6477 -           0.8751    | 0.9670      | 0.4407   | 0.2035     - 0.1389\u001b[0m\n",
      "Training 12     - 100    -          0.2770| 0.0771  | 0.3541 -           0.8627    | 0.9664      | 0.4674   | 0.2605     - 1.5191\n",
      "\u001b[1;4mValidati 12     - 22     -          0.4165| 0.2069  | 0.6234 -           0.8816    | 0.9668      | 0.4878   | 0.2103     - 0.1411\u001b[0m\n",
      "Training 13     - 100    -          0.3382| 0.0935  | 0.4317 -           0.8650    | 0.9660      | 0.4784   | 0.2498     - 1.5266\n",
      "\u001b[1;4mValidati 13     - 22     -          0.4128| 0.2129  | 0.6257 -           0.8789    | 0.9678      | 0.4702   | 0.2191     - 0.1409\u001b[0m\n",
      "Training 14     - 100    -          0.3337| 0.0976  | 0.4313 -           0.8655    | 0.9665      | 0.4846   | 0.2786     - 1.5166\n",
      "\u001b[1;4mValidati 14     - 22     -          0.4093| 0.2144  | 0.6237 -           0.8805    | 0.9669      | 0.4927   | 0.2257     - 0.1395\u001b[0m\n",
      "Training 15     - 100    -          0.3193| 0.1041  | 0.4234 -           0.8693    | 0.9661      | 0.5008   | 0.2837     - 1.5316\n",
      "\u001b[1;4mValidati 15     - 22     -          0.4108| 0.2115  | 0.6223 -           0.8787    | 0.9671      | 0.4877   | 0.2519     - 0.1436\u001b[0m\n",
      "Training 16     - 100    -          0.3568| 0.1528  | 0.5096 -           0.8707    | 0.9664      | 0.5062   | 0.2951     - 1.5182\n",
      "\u001b[1;4mValidati 16     - 22     -          0.4024| 0.2086  | 0.6110 -           0.8720    | 0.9678      | 0.4629   | 0.2603     - 0.1432\u001b[0m\n",
      "Training 17     - 100    -          0.3304| 0.0759  | 0.4063 -           0.8712    | 0.9662      | 0.5113   | 0.2958     - 1.5263\n",
      "\u001b[1;4mValidati 17     - 22     -          0.4218| 0.2068  | 0.6286 -           0.8793    | 0.9673      | 0.4812   | 0.2415     - 0.1404\u001b[0m\n",
      "Training 18     - 100    -          0.3073| 0.0996  | 0.4069 -           0.8733    | 0.9661      | 0.5219   | 0.3027     - 1.5262\n",
      "\u001b[1;4mValidati 18     - 22     -          0.3777| 0.1936  | 0.5712 -           0.8792    | 0.9671      | 0.4674   | 0.2631     - 0.1400\u001b[0m\n",
      "Training 19     - 100    -          0.3013| 0.0841  | 0.3853 -           0.8736    | 0.9663      | 0.5259   | 0.3088     - 1.5292\n",
      "\u001b[1;4mValidati 19     - 22     -          0.3898| 0.1998  | 0.5896 -           0.8851    | 0.9678      | 0.5004   | 0.2512     - 0.1411\u001b[0m\n",
      "Training 20     - 100    -          0.3404| 0.1202  | 0.4606 -           0.8753    | 0.9664      | 0.5291   | 0.3122     - 1.5236\n",
      "\u001b[1;4mValidati 20     - 22     -          0.3910| 0.1965  | 0.5875 -           0.8897    | 0.9690      | 0.5436   | 0.2889     - 0.1396\u001b[0m\n",
      "Training 21     - 100    -          0.3796| 0.0936  | 0.4732 -           0.8747    | 0.9666      | 0.5241   | 0.3218     - 1.5248\n",
      "\u001b[1;4mValidati 21     - 22     -          0.3874| 0.1970  | 0.5843 -           0.8852    | 0.9682      | 0.5382   | 0.2683     - 0.1401\u001b[0m\n",
      "Training 22     - 100    -          0.2683| 0.1007  | 0.3690 -           0.8783    | 0.9662      | 0.5430   | 0.3177     - 1.5344\n",
      "\u001b[1;4mValidati 22     - 22     -          0.4127| 0.2071  | 0.6198 -           0.8824    | 0.9683      | 0.5213   | 0.2511     - 0.1403\u001b[0m\n",
      "Training 23     - 100    -          0.3476| 0.1002  | 0.4478 -           0.8784    | 0.9664      | 0.5459   | 0.3258     - 1.5275\n",
      "\u001b[1;4mValidati 23     - 22     -          0.3932| 0.1961  | 0.5893 -           0.8869    | 0.9686      | 0.5086   | 0.2802     - 0.1398\u001b[0m\n",
      "Training 24     - 100    -          0.2854| 0.0993  | 0.3848 -           0.8788    | 0.9665      | 0.5471   | 0.3201     - 1.5288\n",
      "\u001b[1;4mValidati 24     - 22     -          0.3761| 0.1893  | 0.5654 -           0.8893    | 0.9691      | 0.5465   | 0.2955     - 0.1396\u001b[0m\n",
      "Training 25     - 100    -          0.3167| 0.0957  | 0.4124 -           0.8804    | 0.9667      | 0.5538   | 0.3300     - 1.5231\n",
      "\u001b[1;4mValidati 25     - 22     -          0.3962| 0.2012  | 0.5974 -           0.8910    | 0.9685      | 0.5425   | 0.2880     - 0.1406\u001b[0m\n",
      "Training 26     - 100    -          0.2607| 0.0629  | 0.3237 -           0.8806    | 0.9667      | 0.5533   | 0.3373     - 1.5521\n",
      "\u001b[1;4mValidati 26     - 22     -          0.3886| 0.1951  | 0.5836 -           0.8903    | 0.9687      | 0.5377   | 0.2770     - 0.1410\u001b[0m\n",
      "Training 27     - 100    -          0.3526| 0.0990  | 0.4515 -           0.8812    | 0.9665      | 0.5557   | 0.3367     - 1.5381\n",
      "\u001b[1;4mValidati 27     - 22     -          0.3916| 0.1869  | 0.5786 -           0.8913    | 0.9685      | 0.5488   | 0.2794     - 0.1403\u001b[0m\n",
      "Training 28     - 100    -          0.3315| 0.0616  | 0.3931 -           0.8820    | 0.9664      | 0.5626   | 0.3334     - 1.5391\n",
      "\u001b[1;4mValidati 28     - 22     -          0.3976| 0.1864  | 0.5840 -           0.8871    | 0.9679      | 0.5285   | 0.2412     - 0.1394\u001b[0m\n",
      "Training 29     - 100    -          0.2698| 0.0885  | 0.3583 -           0.8841    | 0.9666      | 0.5716   | 0.3370     - 1.5353\n",
      "\u001b[1;4mValidati 29     - 22     -          0.4018| 0.1963  | 0.5981 -           0.8894    | 0.9682      | 0.5378   | 0.2534     - 0.1423\u001b[0m\n",
      "Training 30     - 100    -          0.2987| 0.0877  | 0.3863 -           0.8844    | 0.9662      | 0.5698   | 0.3378     - 1.5361\n",
      "\u001b[1;4mValidati 30     - 22     -          0.3983| 0.1955  | 0.5937 -           0.8924    | 0.9681      | 0.5539   | 0.2386     - 0.1425\u001b[0m\n",
      "Training 31     - 100    -          0.2502| 0.1067  | 0.3569 -           0.8861    | 0.9665      | 0.5796   | 0.3441     - 1.5428\n",
      "\u001b[1;4mValidati 31     - 22     -          0.3943| 0.1973  | 0.5916 -           0.8942    | 0.9690      | 0.5496   | 0.2779     - 0.1411\u001b[0m\n",
      "Training 32     - 100    -          0.3331| 0.0790  | 0.4121 -           0.8870    | 0.9663      | 0.5887   | 0.3490     - 1.5625\n",
      "\u001b[1;4mValidati 32     - 22     -          0.3766| 0.1865  | 0.5630 -           0.8924    | 0.9683      | 0.5470   | 0.2817     - 0.1419\u001b[0m\n",
      "Training 33     - 100    -          0.2870| 0.0877  | 0.3747 -           0.8868    | 0.9662      | 0.5821   | 0.3422     - 1.5356\n",
      "\u001b[1;4mValidati 33     - 22     -          0.3734| 0.1798  | 0.5532 -           0.8938    | 0.9681      | 0.5613   | 0.3157     - 0.1404\u001b[0m\n",
      "Training 34     - 100    -          0.2866| 0.0715  | 0.3581 -           0.8875    | 0.9666      | 0.5865   | 0.3462     - 1.5914\n",
      "\u001b[1;4mValidati 34     - 22     -          0.3662| 0.1812  | 0.5474 -           0.8934    | 0.9688      | 0.5480   | 0.2969     - 0.1417\u001b[0m\n",
      "Training 35     - 100    -          0.2707| 0.0842  | 0.3548 -           0.8897    | 0.9663      | 0.5951   | 0.3490     - 1.5384\n",
      "\u001b[1;4mValidati 35     - 22     -          0.3859| 0.1883  | 0.5742 -           0.8944    | 0.9688      | 0.5550   | 0.2738     - 0.1403\u001b[0m\n",
      "Training 36     - 100    -          0.2652| 0.0860  | 0.3511 -           0.8893    | 0.9671      | 0.5962   | 0.3651     - 1.5369\n",
      "\u001b[1;4mValidati 36     - 22     -          0.3896| 0.1863  | 0.5760 -           0.8962    | 0.9682      | 0.5671   | 0.2825     - 0.1404\u001b[0m\n",
      "Training 37     - 100    -          0.2134| 0.0807  | 0.2942 -           0.8886    | 0.9660      | 0.5912   | 0.3565     - 1.5411\n",
      "\u001b[1;4mValidati 37     - 22     -          0.4078| 0.1902  | 0.5981 -           0.8969    | 0.9690      | 0.5674   | 0.2910     - 0.1399\u001b[0m\n",
      "Training 38     - 100    -          0.2962| 0.1113  | 0.4075 -           0.8886    | 0.9666      | 0.5932   | 0.3623     - 1.5436\n",
      "\u001b[1;4mValidati 38     - 22     -          0.3777| 0.1853  | 0.5630 -           0.8960    | 0.9693      | 0.5791   | 0.2863     - 0.1402\u001b[0m\n",
      "Training 39     - 100    -          0.3049| 0.0920  | 0.3969 -           0.8914    | 0.9668      | 0.6046   | 0.3610     - 1.5305\n",
      "\u001b[1;4mValidati 39     - 22     -          0.3830| 0.1847  | 0.5677 -           0.8961    | 0.9692      | 0.5675   | 0.2933     - 0.1408\u001b[0m\n",
      "Training 40     - 100    -          0.2351| 0.0831  | 0.3182 -           0.8929    | 0.9663      | 0.6097   | 0.3604     - 1.5338\n",
      "\u001b[1;4mValidati 40     - 22     -          0.3836| 0.1858  | 0.5694 -           0.8961    | 0.9688      | 0.5742   | 0.3104     - 0.1427\u001b[0m\n",
      "Training 41     - 100    -          0.2679| 0.0541  | 0.3219 -           0.8925    | 0.9661      | 0.6075   | 0.3527     - 1.5302\n",
      "\u001b[1;4mValidati 41     - 22     -          0.3855| 0.1865  | 0.5719 -           0.8906    | 0.9683      | 0.5469   | 0.2474     - 0.1400\u001b[0m\n",
      "Training 42     - 100    -          0.3473| 0.1012  | 0.4486 -           0.8917    | 0.9663      | 0.6068   | 0.3609     - 1.5240\n",
      "\u001b[1;4mValidati 42     - 22     -          0.3776| 0.1879  | 0.5655 -           0.8981    | 0.9695      | 0.5743   | 0.2942     - 0.1398\u001b[0m\n",
      "Training 43     - 100    -          0.3131| 0.1255  | 0.4387 -           0.8928    | 0.9664      | 0.6115   | 0.3669     - 1.5359\n",
      "\u001b[1;4mValidati 43     - 22     -          0.3768| 0.1890  | 0.5658 -           0.8984    | 0.9687      | 0.5872   | 0.2939     - 0.1409\u001b[0m\n",
      "Training 44     - 100    -          0.2660| 0.0811  | 0.3471 -           0.8921    | 0.9664      | 0.6106   | 0.3657     - 1.5334\n",
      "\u001b[1;4mValidati 44     - 22     -          0.3681| 0.1840  | 0.5520 -           0.9042    | 0.9690      | 0.6282   | 0.3066     - 0.1394\u001b[0m\n",
      "Training 45     - 100    -          0.2610| 0.0960  | 0.3570 -           0.8962    | 0.9662      | 0.6251   | 0.3664     - 1.5324\n",
      "\u001b[1;4mValidati 45     - 22     -          0.3786| 0.1849  | 0.5636 -           0.9029    | 0.9690      | 0.6090   | 0.3168     - 0.1395\u001b[0m\n",
      "Training 46     - 100    -          0.2173| 0.0943  | 0.3116 -           0.8933    | 0.9664      | 0.6174   | 0.3648     - 1.7595\n",
      "\u001b[1;4mValidati 46     - 22     -          0.3813| 0.1880  | 0.5692 -           0.9014    | 0.9698      | 0.5943   | 0.3021     - 0.1405\u001b[0m\n",
      "Training 47     - 100    -          0.2823| 0.0950  | 0.3773 -           0.8943    | 0.9663      | 0.6164   | 0.3683     - 1.5320\n",
      "\u001b[1;4mValidati 47     - 22     -          0.3759| 0.1865  | 0.5624 -           0.8983    | 0.9693      | 0.5862   | 0.3152     - 0.1399\u001b[0m\n",
      "Training 48     - 100    -          0.2427| 0.0575  | 0.3002 -           0.8957    | 0.9661      | 0.6254   | 0.3642     - 1.5475\n",
      "\u001b[1;4mValidati 48     - 22     -          0.3708| 0.1822  | 0.5530 -           0.8973    | 0.9685      | 0.5722   | 0.3084     - 0.1375\u001b[0m\n",
      "Training 49     - 100    -          0.2640| 0.0667  | 0.3307 -           0.8938    | 0.9664      | 0.6182   | 0.3716     - 1.5385\n",
      "\u001b[1;4mValidati 49     - 22     -          0.3640| 0.1801  | 0.5441 -           0.9046    | 0.9694      | 0.6220   | 0.2904     - 0.1405\u001b[0m\n",
      "Training 50     - 100    -          0.3094| 0.0633  | 0.3727 -           0.8958    | 0.9664      | 0.6273   | 0.3730     - 1.5416\n",
      "\u001b[1;4mValidati 50     - 22     -          0.3784| 0.1959  | 0.5742 -           0.9050    | 0.9693      | 0.6179   | 0.3138     - 0.1420\u001b[0m\n",
      "Training 51     - 100    -          0.2947| 0.0592  | 0.3538 -           0.8985    | 0.9662      | 0.6323   | 0.3687     - 1.5753\n",
      "\u001b[1;4mValidati 51     - 22     -          0.3763| 0.1921  | 0.5684 -           0.9046    | 0.9696      | 0.6149   | 0.3134     - 0.1406\u001b[0m\n",
      "Training 52     - 100    -          0.2799| 0.1199  | 0.3997 -           0.8979    | 0.9662      | 0.6358   | 0.3730     - 1.5797\n",
      "\u001b[1;4mValidati 52     - 22     -          0.3714| 0.1813  | 0.5526 -           0.9037    | 0.9692      | 0.6137   | 0.3004     - 0.1534\u001b[0m\n",
      "Training 53     - 100    -          0.2848| 0.1025  | 0.3873 -           0.8958    | 0.9665      | 0.6252   | 0.3673     - 1.5409\n",
      "\u001b[1;4mValidati 53     - 22     -          0.3640| 0.1805  | 0.5445 -           0.9019    | 0.9691      | 0.6118   | 0.3113     - 0.1429\u001b[0m\n",
      "Training 54     - 100    -          0.2368| 0.0917  | 0.3284 -           0.8978    | 0.9662      | 0.6363   | 0.3725     - 1.5359\n",
      "\u001b[1;4mValidati 54     - 22     -          0.3657| 0.1803  | 0.5460 -           0.9026    | 0.9696      | 0.6024   | 0.3150     - 0.1402\u001b[0m\n",
      "Training 55     - 100    -          0.2727| 0.1106  | 0.3833 -           0.8987    | 0.9662      | 0.6365   | 0.3714     - 1.5445\n",
      "\u001b[1;4mValidati 55     - 22     -          0.3803| 0.1866  | 0.5669 -           0.9002    | 0.9686      | 0.5841   | 0.2707     - 0.1390\u001b[0m\n",
      "Training 56     - 100    -          0.2640| 0.0768  | 0.3409 -           0.9003    | 0.9663      | 0.6397   | 0.3796     - 1.5217\n",
      "\u001b[1;4mValidati 56     - 22     -          0.3724| 0.1864  | 0.5588 -           0.9028    | 0.9690      | 0.6152   | 0.3402     - 0.1400\u001b[0m\n",
      "Training 57     - 100    -          0.2267| 0.0914  | 0.3180 -           0.8990    | 0.9663      | 0.6364   | 0.3792     - 1.5266\n",
      "\u001b[1;4mValidati 57     - 22     -          0.3671| 0.1833  | 0.5503 -           0.9042    | 0.9692      | 0.6043   | 0.3079     - 0.1552\u001b[0m\n",
      "Training 58     - 100    -          0.2209| 0.0683  | 0.2892 -           0.8987    | 0.9659      | 0.6379   | 0.3790     - 1.5350\n",
      "\u001b[1;4mValidati 58     - 22     -          0.3685| 0.1864  | 0.5549 -           0.9044    | 0.9692      | 0.6114   | 0.2934     - 0.1412\u001b[0m\n",
      "Training 59     - 100    -          0.2845| 0.0507  | 0.3352 -           0.8964    | 0.9665      | 0.6284   | 0.3799     - 1.5619\n",
      "\u001b[1;4mValidati 59     - 22     -          0.3745| 0.1920  | 0.5665 -           0.9078    | 0.9695      | 0.6337   | 0.3069     - 0.1400\u001b[0m\n",
      "Training 60     - 100    -          0.3055| 0.0939  | 0.3994 -           0.9013    | 0.9660      | 0.6487   | 0.3764     - 1.5498\n",
      "\u001b[1;4mValidati 60     - 22     -          0.3790| 0.1836  | 0.5625 -           0.9033    | 0.9694      | 0.6093   | 0.3307     - 0.1384\u001b[0m\n",
      "Training 61     - 100    -          0.2537| 0.0709  | 0.3247 -           0.8985    | 0.9665      | 0.6355   | 0.3727     - 1.5562\n",
      "\u001b[1;4mValidati 61     - 22     -          0.3767| 0.1897  | 0.5664 -           0.9068    | 0.9697      | 0.6190   | 0.3162     - 0.1411\u001b[0m\n",
      "Training 62     - 100    -          0.2259| 0.0730  | 0.2988 -           0.9027    | 0.9661      | 0.6554   | 0.3819     - 1.5311\n",
      "\u001b[1;4mValidati 62     - 22     -          0.3760| 0.1905  | 0.5665 -           0.9083    | 0.9697      | 0.6257   | 0.3138     - 0.1418\u001b[0m\n",
      "Training 63     - 100    -          0.2501| 0.0665  | 0.3166 -           0.9010    | 0.9663      | 0.6463   | 0.3822     - 1.5320\n",
      "\u001b[1;4mValidati 63     - 22     -          0.3693| 0.1855  | 0.5548 -           0.9087    | 0.9696      | 0.6342   | 0.3214     - 0.1400\u001b[0m\n",
      "Training 64     - 100    -          0.2916| 0.0841  | 0.3757 -           0.9012    | 0.9660      | 0.6508   | 0.3807     - 1.5581\n",
      "\u001b[1;4mValidati 64     - 22     -          0.3626| 0.1825  | 0.5451 -           0.9046    | 0.9691      | 0.6080   | 0.2839     - 0.1402\u001b[0m\n",
      "Training 65     - 100    -          0.2102| 0.0663  | 0.2765 -           0.9018    | 0.9658      | 0.6489   | 0.3793     - 1.5594\n",
      "\u001b[1;4mValidati 65     - 22     -          0.3813| 0.1889  | 0.5702 -           0.9034    | 0.9683      | 0.5990   | 0.2686     - 0.1402\u001b[0m\n",
      "Training 66     - 100    -          0.2587| 0.0534  | 0.3121 -           0.9032    | 0.9661      | 0.6550   | 0.3862     - 1.5608\n",
      "\u001b[1;4mValidati 66     - 22     -          0.3741| 0.1907  | 0.5648 -           0.8967    | 0.9685      | 0.5834   | 0.3300     - 0.1554\u001b[0m\n",
      "Training 67     - 100    -          0.2166| 0.0796  | 0.2962 -           0.9040    | 0.9661      | 0.6611   | 0.3817     - 1.5801\n",
      "\u001b[1;4mValidati 67     - 22     -          0.3702| 0.1870  | 0.5572 -           0.9091    | 0.9696      | 0.6371   | 0.3204     - 0.1415\u001b[0m\n",
      "Training 68     - 100    -          0.2843| 0.0863  | 0.3706 -           0.9028    | 0.9660      | 0.6528   | 0.3802     - 1.5228\n",
      "\u001b[1;4mValidati 68     - 22     -          0.3650| 0.1870  | 0.5521 -           0.9110    | 0.9698      | 0.6429   | 0.3287     - 0.1565\u001b[0m\n",
      "Training 69     - 100    -          0.2353| 0.0855  | 0.3208 -           0.9048    | 0.9659      | 0.6643   | 0.3853     - 1.5326\n",
      "\u001b[1;4mValidati 69     - 22     -          0.3657| 0.1872  | 0.5529 -           0.9017    | 0.9684      | 0.6028   | 0.3329     - 0.1414\u001b[0m\n",
      "Training 70     - 100    -          0.2719| 0.0635  | 0.3354 -           0.9029    | 0.9658      | 0.6551   | 0.3847     - 1.5530\n",
      "\u001b[1;4mValidati 70     - 22     -          0.3739| 0.1885  | 0.5623 -           0.9073    | 0.9691      | 0.6319   | 0.3363     - 0.1374\u001b[0m\n",
      "Training 71     - 100    -          0.2384| 0.0501  | 0.2885 -           0.9014    | 0.9658      | 0.6512   | 0.3835     - 1.5511\n",
      "\u001b[1;4mValidati 71     - 22     -          0.3658| 0.1815  | 0.5473 -           0.9090    | 0.9692      | 0.6541   | 0.3308     - 0.1431\u001b[0m\n",
      "Training 72     - 100    -          0.2792| 0.0761  | 0.3553 -           0.9039    | 0.9663      | 0.6602   | 0.3876     - 1.5344\n",
      "\u001b[1;4mValidati 72     - 22     -          0.3652| 0.1842  | 0.5495 -           0.9104    | 0.9700      | 0.6357   | 0.3186     - 0.1418\u001b[0m\n",
      "Training 73     - 100    -          0.2497| 0.0955  | 0.3452 -           0.9061    | 0.9657      | 0.6686   | 0.3891     - 1.5473\n",
      "\u001b[1;4mValidati 73     - 22     -          0.3793| 0.1888  | 0.5681 -           0.8985    | 0.9677      | 0.5770   | 0.2519     - 0.1398\u001b[0m\n",
      "Training 74     - 100    -          0.2501| 0.1213  | 0.3714 -           0.9047    | 0.9657      | 0.6634   | 0.3794     - 1.5446\n",
      "\u001b[1;4mValidati 74     - 22     -          0.3696| 0.1847  | 0.5543 -           0.9053    | 0.9684      | 0.6141   | 0.2801     - 0.1438\u001b[0m\n",
      "Training 75     - 100    -          0.3094| 0.0973  | 0.4067 -           0.9049    | 0.9654      | 0.6656   | 0.3794     - 1.5811\n",
      "\u001b[1;4mValidati 75     - 22     -          0.3732| 0.1912  | 0.5644 -           0.9096    | 0.9695      | 0.6330   | 0.3266     - 0.1427\u001b[0m\n",
      "Training 76     - 100    -          0.2024| 0.0461  | 0.2484 -           0.9053    | 0.9662      | 0.6641   | 0.3881     - 1.5907\n",
      "\u001b[1;4mValidati 76     - 22     -          0.3801| 0.1931  | 0.5731 -           0.9078    | 0.9693      | 0.6229   | 0.3102     - 0.1534\u001b[0m\n",
      "Training 77     - 100    -          0.1978| 0.0667  | 0.2644 -           0.9067    | 0.9661      | 0.6724   | 0.3900     - 1.5542\n",
      "\u001b[1;4mValidati 77     - 22     -          0.3712| 0.1894  | 0.5607 -           0.9075    | 0.9690      | 0.6269   | 0.3269     - 0.1393\u001b[0m\n",
      "Training 78     - 100    -          0.1957| 0.1107  | 0.3064 -           0.9063    | 0.9659      | 0.6717   | 0.3936     - 1.5420\n",
      "\u001b[1;4mValidati 78     - 22     -          0.3780| 0.1928  | 0.5708 -           0.9087    | 0.9692      | 0.6277   | 0.2907     - 0.1395\u001b[0m\n",
      "Training 79     - 100    -          0.2829| 0.1195  | 0.4024 -           0.9062    | 0.9663      | 0.6684   | 0.3935     - 1.5457\n",
      "\u001b[1;4mValidati 79     - 22     -          0.3658| 0.1859  | 0.5517 -           0.9099    | 0.9692      | 0.6366   | 0.3123     - 0.1434\u001b[0m\n",
      "Training 80     - 100    -          0.3326| 0.0847  | 0.4173 -           0.9071    | 0.9658      | 0.6727   | 0.3919     - 1.5603\n",
      "\u001b[1;4mValidati 80     - 22     -          0.3750| 0.1925  | 0.5674 -           0.9128    | 0.9695      | 0.6547   | 0.3320     - 0.1447\u001b[0m\n",
      "Training 81     - 100    -          0.2261| 0.0854  | 0.3115 -           0.9051    | 0.9657      | 0.6675   | 0.3788     - 1.5536\n",
      "\u001b[1;4mValidati 81     - 22     -          0.3812| 0.1937  | 0.5749 -           0.9081    | 0.9693      | 0.6298   | 0.3101     - 0.1408\u001b[0m\n",
      "Training 82     - 100    -          0.1965| 0.0812  | 0.2777 -           0.9081    | 0.9661      | 0.6781   | 0.3931     - 1.5500\n",
      "\u001b[1;4mValidati 82     - 22     -          0.3689| 0.1866  | 0.5555 -           0.9121    | 0.9690      | 0.6550   | 0.3189     - 0.1438\u001b[0m\n",
      "Training 83     - 100    -          0.2610| 0.0929  | 0.3539 -           0.9058    | 0.9661      | 0.6688   | 0.3878     - 1.5479\n",
      "\u001b[1;4mValidati 83     - 22     -          0.3753| 0.1879  | 0.5631 -           0.9020    | 0.9687      | 0.5955   | 0.2733     - 0.1395\u001b[0m\n",
      "Training 84     - 100    -          0.2536| 0.0876  | 0.3411 -           0.9058    | 0.9661      | 0.6675   | 0.3852     - 1.5397\n",
      "\u001b[1;4mValidati 84     - 22     -          0.3759| 0.1944  | 0.5703 -           0.9069    | 0.9693      | 0.6213   | 0.3108     - 0.1410\u001b[0m\n",
      "Training 85     - 100    -          0.2158| 0.0514  | 0.2672 -           0.9072    | 0.9663      | 0.6771   | 0.3930     - 1.5659\n",
      "\u001b[1;4mValidati 85     - 22     -          0.3722| 0.1920  | 0.5642 -           0.9097    | 0.9700      | 0.6404   | 0.3265     - 0.1405\u001b[0m\n",
      "Training 86     - 100    -          0.2902| 0.0874  | 0.3776 -           0.9088    | 0.9659      | 0.6812   | 0.3905     - 1.5651\n",
      "\u001b[1;4mValidati 86     - 22     -          0.3669| 0.1898  | 0.5567 -           0.9131    | 0.9692      | 0.6531   | 0.3246     - 0.1491\u001b[0m\n",
      "Training 87     - 100    -          0.2346| 0.0703  | 0.3049 -           0.9076    | 0.9657      | 0.6752   | 0.3813     - 1.5736\n",
      "\u001b[1;4mValidati 87     - 22     -          0.3719| 0.1921  | 0.5640 -           0.9155    | 0.9695      | 0.6689   | 0.3360     - 0.1496\u001b[0m\n",
      "Training 88     - 100    -          0.1836| 0.0735  | 0.2571 -           0.9088    | 0.9657      | 0.6808   | 0.3866     - 1.5735\n",
      "\u001b[1;4mValidati 88     - 22     -          0.3730| 0.1896  | 0.5626 -           0.9119    | 0.9697      | 0.6490   | 0.3141     - 0.1449\u001b[0m\n",
      "Training 89     - 100    -          0.2355| 0.1044  | 0.3398 -           0.9084    | 0.9657      | 0.6817   | 0.3976     - 1.5601\n",
      "\u001b[1;4mValidati 89     - 22     -          0.3781| 0.1929  | 0.5710 -           0.9081    | 0.9688      | 0.6319   | 0.3021     - 0.1405\u001b[0m\n",
      "Training 90     - 100    -          0.2119| 0.0639  | 0.2758 -           0.9086    | 0.9659      | 0.6807   | 0.3899     - 1.5346\n",
      "\u001b[1;4mValidati 90     - 22     -          0.3751| 0.1919  | 0.5670 -           0.9110    | 0.9685      | 0.6459   | 0.3326     - 0.1564\u001b[0m\n",
      "Training 91     - 100    -          0.2125| 0.1045  | 0.3171 -           0.9088    | 0.9660      | 0.6804   | 0.3909     - 1.5381\n",
      "\u001b[1;4mValidati 91     - 22     -          0.3762| 0.1929  | 0.5691 -           0.9002    | 0.9681      | 0.5870   | 0.2734     - 0.1410\u001b[0m\n",
      "Training 92     - 100    -          0.2157| 0.0618  | 0.2775 -           0.9091    | 0.9658      | 0.6848   | 0.3907     - 1.5401\n",
      "\u001b[1;4mValidati 92     - 22     -          0.3688| 0.1905  | 0.5593 -           0.9141    | 0.9694      | 0.6560   | 0.3092     - 0.1401\u001b[0m\n",
      "Training 93     - 100    -          0.2117| 0.0770  | 0.2887 -           0.9100    | 0.9660      | 0.6843   | 0.3892     - 1.5353\n",
      "\u001b[1;4mValidati 93     - 22     -          0.3678| 0.1886  | 0.5564 -           0.9095    | 0.9691      | 0.6359   | 0.3093     - 0.1391\u001b[0m\n",
      "Training 94     - 100    -          0.1828| 0.0651  | 0.2479 -           0.9065    | 0.9665      | 0.6738   | 0.3965     - 1.5844\n",
      "\u001b[1;4mValidati 94     - 22     -          0.3734| 0.1921  | 0.5655 -           0.9064    | 0.9693      | 0.6103   | 0.2811     - 0.1418\u001b[0m\n",
      "Training 95     - 100    -          0.2182| 0.0844  | 0.3026 -           0.9100    | 0.9665      | 0.6873   | 0.4039     - 1.5323\n",
      "\u001b[1;4mValidati 95     - 22     -          0.3624| 0.1905  | 0.5529 -           0.9116    | 0.9697      | 0.6493   | 0.3247     - 0.1421\u001b[0m\n",
      "Training 96     - 100    -          0.2781| 0.1067  | 0.3848 -           0.9088    | 0.9662      | 0.6824   | 0.3923     - 1.5428\n",
      "\u001b[1;4mValidati 96     - 22     -          0.3699| 0.1884  | 0.5583 -           0.9148    | 0.9702      | 0.6628   | 0.3371     - 0.1437\u001b[0m\n",
      "Training 97     - 100    -          0.2700| 0.0524  | 0.3224 -           0.9079    | 0.9664      | 0.6787   | 0.3924     - 1.5381\n",
      "\u001b[1;4mValidati 97     - 22     -          0.3689| 0.1918  | 0.5606 -           0.9078    | 0.9688      | 0.6244   | 0.2764     - 0.1399\u001b[0m\n",
      "Training 98     - 100    -          0.2523| 0.0805  | 0.3328 -           0.9089    | 0.9662      | 0.6801   | 0.3911     - 1.5712\n",
      "\u001b[1;4mValidati 98     - 22     -          0.3696| 0.1897  | 0.5594 -           0.9077    | 0.9690      | 0.6222   | 0.2883     - 0.1439\u001b[0m\n",
      "Training 99     - 100    -          0.3008| 0.0899  | 0.3907 -           0.9083    | 0.9663      | 0.6795   | 0.3944     - 1.6086\n",
      "\u001b[1;4mValidati 99     - 22     -          0.3658| 0.1913  | 0.5571 -           0.9151    | 0.9700      | 0.6589   | 0.3317     - 0.1567\u001b[0m\n",
      "Training 100    - 100    -          0.2498| 0.1108  | 0.3606 -           0.9088    | 0.9660      | 0.6819   | 0.3901     - 1.5741\n",
      "\u001b[1;4mValidati 100    - 22     -          0.3694| 0.1914  | 0.5608 -           0.9109    | 0.9693      | 0.6315   | 0.2922     - 0.1408\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
