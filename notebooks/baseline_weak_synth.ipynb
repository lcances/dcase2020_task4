{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:15<00:00, 479.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    validation_ratio=0.2,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53, # The model output localisation with a résolution of ~ 18ms --> 53 temporal bins\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak ans synthetic20 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "Loading dataset: train, subset: weak\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: synthetic20\u001b[0m\n",
      "Loading dataset: train, subset: synthetic20\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/synthetic20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "manager.add_subset(\"synthetic20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> Creating new train / validation split\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> validation ratio : 0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.split_train_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%  setup augmentation and create pytorch dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MultipleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(MultipleDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        for d in self.datasets:\n",
    "            assert not isinstance(d, IterableDataset), \"ConcatDataset does not support IterableDataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [d[sample_idx] for d in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218, 706)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset\n",
    "\n",
    "- We want both the weak and strong ground truth --> the *weak* and *strong* parameters to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, weak=True, strong=True, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, weak=True, strong=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "This model is the same than the weak baseline but have an extra output. <br />\n",
    "the loc_output is compose of a single convolution layer with nb_filters == nb_class. <br />\n",
    "Since their is some pooling layer, the *loc_ouput* have a precision of 53 bins (~= 18 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakStrongBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (weak_output): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       "  (strong_output): Sequential(\n",
       "    (0): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakStrongBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_weak_output.Flatten_0                  -         [1, 1696]       -   \n",
      "16_weak_output.Dropout_1                  -         [1, 1696]       -   \n",
      "17_weak_output.Linear_2          [1696, 10]           [1, 10]  16.97k   \n",
      "18_strong_output.Conv2d_0    [32, 10, 1, 1]    [1, 10, 1, 53]   330.0   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_weak_output.Flatten_0             -  \n",
      "16_weak_output.Dropout_1             -  \n",
      "17_weak_output.Linear_2         16.96k  \n",
      "18_strong_output.Conv2d_0       16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             36.308k\n",
      "Trainable params         36.308k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.625696M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom loss function\n",
    "\n",
    "Since not all file have strong truth, it is necessary to remove those files. <br />\n",
    "For that, the strong mask is computed. If the sum of the strong ground truth is equal to 0 then it is a fake one <br />\n",
    "This file strong loss must not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_synth_loss(logits_weak, logits_strong, y_weak, y_strong, reduce: str = \"mean\"):\n",
    "    assert reduce in [\"mean\", \"sum\"], \"support only \\\"mean\\\" and \\\"sum\\\"\"\n",
    "    \n",
    "    #  Reduction function\n",
    "    if reduce == \"mean\":\n",
    "        reduce_fn = torch.mean\n",
    "    elif reduce == \"sum\":\n",
    "        reduce_fn = torch.sum\n",
    "    \n",
    "    # based on Binary Cross Entropy loss\n",
    "    weak_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    strong_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    # calc separate loss function\n",
    "    weak_bce = weak_criterion(logits_weak, y_weak)\n",
    "    strong_bce = strong_criterion(logits_strong, y_strong)\n",
    "    \n",
    "    weak_bce = reduce_fn(weak_bce, dim=1)\n",
    "    strong_bce = reduce_fn(strong_bce, dim=(1, 2))\n",
    "    \n",
    "    # calc strong mask\n",
    "    strong_mask = torch.clamp(torch.sum(y_strong, dim=(1, 2)), 0, 1) # vector of 0 or 1\n",
    "    strong_mask = strong_mask.detach() # declared not to need gradients\n",
    "    \n",
    "    # Output the different loss for logging purpose\n",
    "    weak_loss = reduce_fn(weak_bce)\n",
    "    strong_loss = reduce_fn(strong_mask * strong_bce)\n",
    "    total_loss = reduce_fn(weak_bce + strong_mask * strong_bce)\n",
    "    \n",
    "    return weak_loss, strong_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "weak_binacc_func = BinaryAccuracy()\n",
    "strong_binacc_func = BinaryAccuracy()\n",
    "weak_f_func = FScore()\n",
    "strong_f_func = FScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_metrics():\n",
    "    metrics = [weak_binacc_func, strong_binacc_func, weak_f_func, strong_f_func]\n",
    "    \n",
    "    for m in metrics:\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6}| {:<8.8}| {:<6.6} - {:<9.9} {:<10.10}| {:<12.12}| {:<9.9}| {:<11.11}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f}| {:<8.4f}| {:<6.4f} - {:<9.9} {:<10.4f}| {:<12.4f}| {:<9.4f}| {:<11.4f}- {:<6.4f}\"\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"Strong \", \"Total \", \"metrics: \", \"Weak acc \", \"Strong acc \", \"Weak F1 \", \"Strong F1\", \"Time\"\n",
    ")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X, y) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X = X.cuda().float()\n",
    "        y_weak = y[0].cuda().float()\n",
    "        y_strong = y[1].cuda().float()\n",
    "        \n",
    "        weak_logits, strong_logits = model(X)\n",
    "        \n",
    "        # calc the loss\n",
    "        weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "            weak_logits, strong_logits,\n",
    "            y_weak, y_strong,\n",
    "            reduce=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # calc metrics\n",
    "        weak_pred = torch.sigmoid(weak_logits)\n",
    "        strong_pred = torch.sigmoid(strong_logits)\n",
    "        \n",
    "        # tagging\n",
    "        weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "        weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "        \n",
    "        # loc\n",
    "        strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "        strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "        \n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        # logs\n",
    "        print(value_form.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            int(100 * (i + 1) / nb_batch),\n",
    "            \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "            \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")\n",
    "        \n",
    "    # tensorboard logs\n",
    "    tensorboard.add_scalar(\"train/weak_loss\", weak_loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/strong_loss\", strong_loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/total_loss\", total_loss.item(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/weak_acc\", weak_binacc, epoch)\n",
    "    tensorboard.add_scalar(\"train/strong_acc\", strong_binacc, epoch)\n",
    "    tensorboard.add_scalar(\"train/weak_f1\", weak_fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "        \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y_weak = y[0].cuda().float()\n",
    "            y_strong = y[1].cuda().float()\n",
    "\n",
    "            weak_logits, strong_logits = model(X)\n",
    "\n",
    "            # calc the loss\n",
    "            weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "                weak_logits, strong_logits,\n",
    "                y_weak, y_strong,\n",
    "                reduce=\"mean\"\n",
    "            )\n",
    "            \n",
    "             # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "            # logs\n",
    "            print(value_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"val/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n",
      "\n",
      "Training 1      - 101    -          0.4253| 0.1034  | 0.5286 -           0.8198    | 48.5914     | 0.3104   | 0.0179     - 1.5711\n",
      "Validati 1      - 23     -          0.6506| 0.3005  | 0.9511 -           0.8344    | 51.0548     | 0.1775   | 0.0000     - 0.1524\n",
      "Training 2      - 101    -          0.3958| 0.0914  | 0.4872 -           0.8288    | 51.1122     | 0.3400   | 0.0266     - 1.4717\n",
      "Validati 2      - 23     -          0.4732| 0.2638  | 0.7369 -           0.8493    | 51.0829     | 0.4116   | 0.0694     - 0.1392\n",
      "Training 3      - 101    -          0.4594| 0.1602  | 0.6196 -           0.8373    | 51.1275     | 0.3655   | 0.0926     - 1.4886\n",
      "Validati 3      - 23     -          0.6200| 0.2496  | 0.8696 -           0.8537    | 51.1293     | 0.3758   | 0.1026     - 0.1398\n",
      "Training 4      - 101    -          0.4179| 0.1314  | 0.5493 -           0.8418    | 51.1494     | 0.3836   | 0.1350     - 1.4881\n",
      "Validati 4      - 23     -          0.5027| 0.3310  | 0.8337 -           0.8542    | 51.1516     | 0.3015   | 0.1096     - 0.1374\n",
      "Training 5      - 101    -          0.3633| 0.0518  | 0.4150 -           0.8466    | 51.1564     | 0.4030   | 0.1675     - 1.5661\n",
      "Validati 5      - 23     -          0.5324| 0.2402  | 0.7726 -           0.8603    | 51.1235     | 0.3587   | 0.1192     - 0.1738\n",
      "Training 6      - 101    -          0.3549| 0.0725  | 0.4274 -           0.8528    | 51.1559     | 0.4210   | 0.1780     - 1.5669\n",
      "Validati 6      - 23     -          0.6299| 0.2569  | 0.8868 -           0.8563    | 51.0597     | 0.4345   | 0.1794     - 0.1586\n",
      "Training 7      - 101    -          0.3643| 0.0805  | 0.4448 -           0.8533    | 51.1433     | 0.4213   | 0.1902     - 1.5283\n",
      "Validati 7      - 23     -          0.5137| 0.2455  | 0.7592 -           0.8667    | 51.1182     | 0.4179   | 0.1546     - 0.1405\n",
      "Training 8      - 101    -          0.3325| 0.1157  | 0.4482 -           0.8583    | 51.1553     | 0.4468   | 0.2153     - 1.4884\n",
      "Validati 8      - 23     -          0.4183| 0.2373  | 0.6557 -           0.8712    | 51.1288     | 0.4777   | 0.1699     - 0.1367\n",
      "Training 9      - 101    -          0.3064| 0.0916  | 0.3981 -           0.8591    | 51.1813     | 0.4552   | 0.2385     - 1.4864\n",
      "Validati 9      - 23     -          0.4108| 0.2240  | 0.6348 -           0.8732    | 51.1671     | 0.4795   | 0.2027     - 0.1414\n",
      "Training 10     - 101    -          0.2887| 0.0927  | 0.3814 -           0.8614    | 51.1815     | 0.4634   | 0.2559     - 1.4877\n",
      "Validati 10     - 23     -          0.5009| 0.2752  | 0.7761 -           0.8614    | 51.1174     | 0.4580   | 0.2155     - 0.1393\n",
      "Training 11     - 101    -          0.3177| 0.1018  | 0.4195 -           0.8625    | 51.1749     | 0.4642   | 0.2605     - 1.4716\n",
      "Validati 11     - 23     -          0.4971| 0.2494  | 0.7465 -           0.8721    | 51.2345     | 0.5011   | 0.2541     - 0.1402\n",
      "Training 12     - 101    -          0.3257| 0.1000  | 0.4257 -           0.8649    | 51.1620     | 0.4811   | 0.2754     - 1.4707\n",
      "Validati 12     - 23     -          0.4073| 0.2424  | 0.6497 -           0.8596    | 50.9856     | 0.5091   | 0.2452     - 0.1408\n",
      "Training 13     - 101    -          0.3717| 0.0788  | 0.4505 -           0.8690    | 51.1692     | 0.4956   | 0.2867     - 1.5086\n",
      "Validati 13     - 23     -          0.3959| 0.2432  | 0.6392 -           0.8772    | 51.2160     | 0.5074   | 0.2562     - 0.1415\n",
      "Training 14     - 101    -          0.2592| 0.1097  | 0.3689 -           0.8693    | 51.1643     | 0.5020   | 0.2963     - 1.5600\n",
      "Validati 14     - 23     -          0.4552| 0.2386  | 0.6938 -           0.8724    | 51.2442     | 0.4406   | 0.2118     - 0.1522\n",
      "Training 15     - 101    -          0.2675| 0.0831  | 0.3506 -           0.8714    | 51.1481     | 0.5058   | 0.2831     - 1.4856\n",
      "Validati 15     - 23     -          0.4185| 0.2295  | 0.6480 -           0.8776    | 51.2476     | 0.5067   | 0.2363     - 0.1411\n",
      "Training 16     - 101    -          0.3347| 0.0971  | 0.4318 -           0.8721    | 51.1569     | 0.5189   | 0.2986     - 1.4884\n",
      "Validati 16     - 23     -          0.4001| 0.2332  | 0.6333 -           0.8800    | 51.2457     | 0.5051   | 0.2505     - 0.1404\n",
      "Training 17     - 101    -          0.3109| 0.1115  | 0.4224 -           0.8745    | 51.1831     | 0.5228   | 0.3064     - 1.4896\n",
      "Validati 17     - 23     -          0.3867| 0.2254  | 0.6122 -           0.8736    | 51.1618     | 0.5203   | 0.2619     - 0.1386\n",
      "Training 18     - 101    -          0.2869| 0.0551  | 0.3420 -           0.8707    | 51.1846     | 0.5067   | 0.3026     - 1.4970\n",
      "Validati 18     - 23     -          0.4899| 0.2382  | 0.7280 -           0.8825    | 51.2838     | 0.5253   | 0.2716     - 0.1394\n",
      "Training 19     - 101    -          0.2972| 0.0981  | 0.3953 -           0.8766    | 51.1875     | 0.5324   | 0.3213     - 1.4761\n",
      "Validati 19     - 23     -          0.4619| 0.2241  | 0.6860 -           0.8761    | 51.2270     | 0.5056   | 0.2552     - 0.1395\n",
      "Training 20     - 101    -          0.3000| 0.0719  | 0.3719 -           0.8774    | 51.1940     | 0.5398   | 0.3259     - 1.4831\n",
      "Validati 20     - 23     -          0.4148| 0.2260  | 0.6408 -           0.8772    | 51.1921     | 0.5166   | 0.2696     - 0.1385\n",
      "Training 21     - 101    -          0.3513| 0.0979  | 0.4491 -           0.8794    | 51.1821     | 0.5452   | 0.3222     - 1.4834\n",
      "Validati 21     - 23     -          0.3900| 0.2151  | 0.6051 -           0.8815    | 51.2462     | 0.5257   | 0.2232     - 0.1382\n",
      "Training 22     - 101    -          0.2852| 0.1179  | 0.4030 -           0.8787    | 51.1659     | 0.5472   | 0.3188     - 1.4775\n",
      "Validati 22     - 23     -          0.4177| 0.2166  | 0.6343 -           0.8889    | 51.2375     | 0.5780   | 0.2941     - 0.1379\n",
      "Training 23     - 101    -          0.3180| 0.0953  | 0.4134 -           0.8792    | 51.2040     | 0.5482   | 0.3373     - 1.4780\n",
      "Validati 23     - 23     -          0.4531| 0.2182  | 0.6713 -           0.8833    | 51.2705     | 0.5251   | 0.2678     - 0.1435\n",
      "Training 24     - 101    -          0.3404| 0.0920  | 0.4324 -           0.8803    | 51.2021     | 0.5513   | 0.3304     - 1.5049\n",
      "Validati 24     - 23     -          0.4426| 0.2249  | 0.6675 -           0.8842    | 51.2539     | 0.5404   | 0.2743     - 0.1405\n",
      "Training 25     - 101    -          0.3136| 0.0884  | 0.4020 -           0.8808    | 51.1999     | 0.5514   | 0.3337     - 1.5728\n",
      "Validati 25     - 23     -          0.4134| 0.2169  | 0.6303 -           0.8916    | 51.2736     | 0.5805   | 0.2814     - 0.1586\n",
      "Training 26     - 101    -          0.3022| 0.1266  | 0.4288 -           0.8826    | 51.1965     | 0.5605   | 0.3420     - 1.5715\n",
      "Validati 26     - 23     -          0.4315| 0.2175  | 0.6490 -           0.8841    | 51.2697     | 0.5235   | 0.2677     - 0.1410\n",
      "Training 27     - 101    -          0.3082| 0.0967  | 0.4049 -           0.8847    | 51.2009     | 0.5675   | 0.3415     - 1.5323\n",
      "Validati 27     - 23     -          0.4409| 0.2220  | 0.6629 -           0.8793    | 51.2153     | 0.5282   | 0.2901     - 0.1407\n",
      "Training 28     - 101    -          0.3223| 0.1190  | 0.4413 -           0.8835    | 51.1958     | 0.5683   | 0.3462     - 1.5229\n",
      "Validati 28     - 23     -          0.4030| 0.2118  | 0.6148 -           0.8837    | 51.3255     | 0.5229   | 0.2753     - 0.1541\n",
      "Training 29     - 101    -          0.2785| 0.1213  | 0.3997 -           0.8869    | 51.2284     | 0.5822   | 0.3534     - 1.4924\n",
      "Validati 29     - 23     -          0.4107| 0.2103  | 0.6209 -           0.8863    | 51.2849     | 0.5461   | 0.2660     - 0.1403\n",
      "Training 30     - 101    -          0.3671| 0.0801  | 0.4471 -           0.8846    | 51.1999     | 0.5722   | 0.3452     - 1.4839\n",
      "Validati 30     - 23     -          0.4251| 0.2158  | 0.6409 -           0.8880    | 51.2785     | 0.5374   | 0.2713     - 0.1393\n",
      "Training 31     - 101    -          0.2639| 0.1273  | 0.3911 -           0.8878    | 51.1914     | 0.5831   | 0.3413     - 1.5057\n",
      "Validati 31     - 23     -          0.4064| 0.2080  | 0.6144 -           0.8757    | 51.1683     | 0.5398   | 0.2489     - 0.1394\n",
      "Training 32     - 101    -          0.2730| 0.0841  | 0.3571 -           0.8865    | 51.2078     | 0.5820   | 0.3477     - 1.4866\n",
      "Validati 32     - 23     -          0.4295| 0.2081  | 0.6376 -           0.8950    | 51.2810     | 0.6014   | 0.2884     - 0.1394\n",
      "Training 33     - 101    -          0.3291| 0.0731  | 0.4022 -           0.8875    | 51.1948     | 0.5828   | 0.3437     - 1.5864\n",
      "Validati 33     - 23     -          0.3990| 0.2107  | 0.6097 -           0.8946    | 51.2492     | 0.6038   | 0.2934     - 0.1609\n",
      "Training 34     - 101    -          0.2436| 0.1238  | 0.3674 -           0.8879    | 51.1819     | 0.5873   | 0.3475     - 1.5998\n",
      "Validati 34     - 23     -          0.4162| 0.2101  | 0.6264 -           0.8853    | 51.3295     | 0.5380   | 0.2772     - 0.1588\n",
      "Training 35     - 101    -          0.2624| 0.0706  | 0.3330 -           0.8910    | 51.1946     | 0.5995   | 0.3494     - 1.5633\n",
      "Validati 35     - 23     -          0.3764| 0.2039  | 0.5803 -           0.8943    | 51.3129     | 0.5949   | 0.2853     - 0.1401\n",
      "Training 36     - 101    -          0.3194| 0.0748  | 0.3942 -           0.8897    | 51.2134     | 0.5970   | 0.3594     - 1.4869\n",
      "Validati 36     - 23     -          0.4036| 0.2058  | 0.6094 -           0.8952    | 51.3140     | 0.6013   | 0.2861     - 0.1421\n",
      "Training 37     - 101    -          0.3146| 0.0895  | 0.4041 -           0.8896    | 51.2003     | 0.5911   | 0.3514     - 1.4933\n",
      "Validati 37     - 23     -          0.4185| 0.2074  | 0.6259 -           0.8912    | 51.2997     | 0.5655   | 0.2729     - 0.1408\n",
      "Training 38     - 101    -          0.2536| 0.0602  | 0.3138 -           0.8905    | 51.2224     | 0.6019   | 0.3657     - 1.4898\n",
      "Validati 38     - 23     -          0.4445| 0.2071  | 0.6516 -           0.8910    | 51.2874     | 0.5584   | 0.2862     - 0.1399\n",
      "Training 39     - 101    -          0.2918| 0.1290  | 0.4208 -           0.8906    | 51.2215     | 0.6002   | 0.3685     - 1.4941\n",
      "Validati 39     - 23     -          0.4322| 0.2104  | 0.6426 -           0.8952    | 51.2958     | 0.5917   | 0.3020     - 0.1416\n",
      "Training 40     - 101    -          0.3007| 0.0822  | 0.3829 -           0.8905    | 51.1872     | 0.5996   | 0.3459     - 1.4932\n",
      "Validati 40     - 23     -          0.4298| 0.2107  | 0.6405 -           0.8880    | 51.2802     | 0.5539   | 0.2820     - 0.1424\n",
      "Training 41     - 101    -          0.3374| 0.0856  | 0.4231 -           0.8897    | 51.1858     | 0.5961   | 0.3482     - 1.5031\n",
      "Validati 41     - 23     -          0.4118| 0.2106  | 0.6223 -           0.8893    | 51.2876     | 0.5574   | 0.2358     - 0.1395\n",
      "Training 42     - 101    -          0.3070| 0.0813  | 0.3882 -           0.8927    | 51.2008     | 0.6121   | 0.3571     - 1.5175\n",
      "Validati 42     - 23     -          0.4328| 0.2097  | 0.6425 -           0.8878    | 51.1920     | 0.5511   | 0.2939     - 0.1531\n",
      "Training 43     - 101    -          0.3188| 0.1053  | 0.4240 -           0.8917    | 51.1934     | 0.6091   | 0.3650     - 1.5080\n",
      "Validati 43     - 23     -          0.4362| 0.2116  | 0.6478 -           0.8951    | 51.2923     | 0.5804   | 0.2896     - 0.1404\n",
      "Training 44     - 101    -          0.3367| 0.1295  | 0.4662 -           0.8946    | 51.2062     | 0.6173   | 0.3671     - 1.4878\n",
      "Validati 44     - 23     -          0.4534| 0.2086  | 0.6621 -           0.8928    | 51.3164     | 0.5823   | 0.2852     - 0.1396\n",
      "Training 45     - 101    -          0.3207| 0.0625  | 0.3831 -           0.8932    | 51.2253     | 0.6121   | 0.3670     - 1.4900\n",
      "Validati 45     - 23     -          0.4514| 0.2071  | 0.6585 -           0.8867    | 51.3269     | 0.5522   | 0.2731     - 0.1567\n",
      "Training 46     - 101    -          0.3516| 0.0642  | 0.4159 -           0.8937    | 51.2021     | 0.6127   | 0.3638     - 1.4941\n",
      "Validati 46     - 23     -          0.4000| 0.2061  | 0.6061 -           0.8910    | 51.2740     | 0.5763   | 0.3093     - 0.1409\n",
      "Training 47     - 101    -          0.2662| 0.0908  | 0.3570 -           0.8968    | 51.2094     | 0.6277   | 0.3684     - 1.5153\n",
      "Validati 47     - 23     -          0.4281| 0.2071  | 0.6352 -           0.8913    | 51.2924     | 0.5735   | 0.2670     - 0.1409\n",
      "Training 48     - 101    -          0.2611| 0.1320  | 0.3931 -           0.8946    | 51.1762     | 0.6196   | 0.3598     - 1.5055\n",
      "Validati 48     - 23     -          0.3965| 0.2140  | 0.6104 -           0.8959    | 51.3337     | 0.6167   | 0.3016     - 0.1404\n",
      "Training 49     - 101    -          0.2746| 0.0691  | 0.3437 -           0.8946    | 51.2022     | 0.6192   | 0.3687     - 1.5011\n",
      "Validati 49     - 23     -          0.4297| 0.2071  | 0.6368 -           0.8928    | 51.3098     | 0.5821   | 0.2764     - 0.1399\n",
      "Training 50     - 101    -          0.2497| 0.0652  | 0.3149 -           0.8939    | 51.1949     | 0.6147   | 0.3668     - 1.4940\n",
      "Validati 50     - 23     -          0.4152| 0.2058  | 0.6211 -           0.8902    | 51.3306     | 0.5546   | 0.2893     - 0.1414\n",
      "Training 51     - 101    -          0.3046| 0.0715  | 0.3761 -           0.8948    | 51.2113     | 0.6176   | 0.3701     - 1.5736\n",
      "Validati 51     - 23     -          0.4209| 0.2142  | 0.6352 -           0.8996    | 51.3221     | 0.6205   | 0.2881     - 0.1568\n",
      "Training 52     - 101    -          0.3169| 0.0920  | 0.4089 -           0.8921    | 51.2020     | 0.6099   | 0.3639     - 1.5401\n",
      "Validati 52     - 23     -          0.4314| 0.2059  | 0.6373 -           0.8933    | 51.3322     | 0.5828   | 0.2871     - 0.1395\n",
      "Training 53     - 101    -          0.2592| 0.0878  | 0.3470 -           0.8968    | 51.2086     | 0.6302   | 0.3719     - 1.5084\n",
      "Validati 53     - 23     -          0.4550| 0.2066  | 0.6616 -           0.8929    | 51.3115     | 0.5816   | 0.2912     - 0.1450\n",
      "Training 54     - 101    -          0.2904| 0.0892  | 0.3796 -           0.8947    | 51.1873     | 0.6203   | 0.3657     - 1.4973\n",
      "Validati 54     - 23     -          0.4392| 0.2106  | 0.6498 -           0.8958    | 51.3291     | 0.5842   | 0.3054     - 0.1421\n",
      "Training 55     - 101    -          0.2000| 0.1159  | 0.3159 -           0.8976    | 51.1971     | 0.6331   | 0.3724     - 1.5233\n",
      "Validati 55     - 23     -          0.4488| 0.2085  | 0.6573 -           0.8981    | 51.3238     | 0.6009   | 0.3092     - 0.1419\n",
      "Training 56     - 101    -          0.2797| 0.0943  | 0.3740 -           0.8988    | 51.1816     | 0.6380   | 0.3697     - 1.5236\n",
      "Validati 56     - 23     -          0.4372| 0.2062  | 0.6434 -           0.8980    | 51.2914     | 0.6020   | 0.3150     - 0.1536\n",
      "Training 57     - 101    -          0.2400| 0.0979  | 0.3379 -           0.8975    | 51.1852     | 0.6351   | 0.3755     - 1.5151\n",
      "Validati 57     - 23     -          0.4426| 0.2056  | 0.6482 -           0.8954    | 51.3268     | 0.5706   | 0.2600     - 0.1420\n",
      "Training 58     - 101    -          0.2675| 0.0523  | 0.3198 -           0.8967    | 51.2124     | 0.6291   | 0.3781     - 1.5028\n",
      "Validati 58     - 23     -          0.4461| 0.2053  | 0.6514 -           0.8969    | 51.3039     | 0.5996   | 0.2990     - 0.1417\n",
      "Training 59     - 101    -          0.2389| 0.0557  | 0.2946 -           0.8992    | 51.1716     | 0.6395   | 0.3703     - 1.5048\n",
      "Validati 59     - 23     -          0.4447| 0.2074  | 0.6521 -           0.8882    | 51.2950     | 0.5672   | 0.2745     - 0.1432\n",
      "Training 60     - 101    -          0.2498| 0.0596  | 0.3094 -           0.8984    | 51.1892     | 0.6357   | 0.3777     - 1.4940\n",
      "Validati 60     - 23     -          0.4198| 0.2083  | 0.6281 -           0.8952    | 51.3208     | 0.5972   | 0.2758     - 0.1415\n",
      "Training 61     - 101    -          0.2034| 0.1143  | 0.3177 -           0.8985    | 51.2052     | 0.6376   | 0.3740     - 1.5649\n",
      "Validati 61     - 23     -          0.4580| 0.2051  | 0.6631 -           0.8974    | 51.2899     | 0.5953   | 0.3055     - 0.1414\n",
      "Training 62     - 101    -          0.2539| 0.0688  | 0.3226 -           0.8967    | 51.2055     | 0.6298   | 0.3714     - 1.5311\n",
      "Validati 62     - 23     -          0.4448| 0.2052  | 0.6500 -           0.9023    | 51.3279     | 0.6157   | 0.2999     - 0.1433\n",
      "Training 63     - 101    -          0.3233| 0.0529  | 0.3763 -           0.8981    | 51.1874     | 0.6387   | 0.3755     - 1.4944\n",
      "Validati 63     - 23     -          0.4398| 0.2056  | 0.6454 -           0.8776    | 51.0459     | 0.5225   | 0.2734     - 0.1462\n",
      "Training 64     - 101    -          0.2526| 0.1123  | 0.3649 -           0.8990    | 51.1920     | 0.6395   | 0.3770     - 1.5543\n",
      "Validati 64     - 23     -          0.4561| 0.2096  | 0.6658 -           0.9014    | 51.3141     | 0.6137   | 0.3039     - 0.1414\n",
      "Training 65     - 101    -          0.2769| 0.0680  | 0.3448 -           0.8999    | 51.1881     | 0.6396   | 0.3803     - 1.4998\n",
      "Validati 65     - 23     -          0.4427| 0.2055  | 0.6482 -           0.8965    | 51.2917     | 0.5966   | 0.3188     - 0.1392\n",
      "Training 66     - 101    -          0.2146| 0.1080  | 0.3226 -           0.8974    | 51.1786     | 0.6325   | 0.3719     - 1.4951\n",
      "Validati 66     - 23     -          0.4551| 0.2046  | 0.6597 -           0.8992    | 51.3636     | 0.6068   | 0.3156     - 0.1410\n",
      "Training 67     - 101    -          0.2117| 0.1487  | 0.3603 -           0.9010    | 51.1830     | 0.6499   | 0.3771     - 1.5707\n",
      "Validati 67     - 23     -          0.4275| 0.2061  | 0.6336 -           0.9004    | 51.3429     | 0.6044   | 0.3084     - 0.1573\n",
      "Training 68     - 101    -          0.2641| 0.0656  | 0.3298 -           0.9012    | 51.1994     | 0.6508   | 0.3905     - 1.5808\n",
      "Validati 68     - 23     -          0.4364| 0.2080  | 0.6444 -           0.9010    | 51.3476     | 0.6156   | 0.3126     - 0.1413\n",
      "Training 69     - 101    -          0.3185| 0.0578  | 0.3763 -           0.8984    | 51.1777     | 0.6359   | 0.3635     - 1.5139\n",
      "Validati 69     - 23     -          0.4537| 0.2075  | 0.6612 -           0.9010    | 51.3332     | 0.6157   | 0.3019     - 0.1417\n",
      "Training 70     - 101    -          0.2721| 0.1021  | 0.3742 -           0.9021    | 51.2079     | 0.6535   | 0.3858     - 1.5305\n",
      "Validati 70     - 23     -          0.4514| 0.2061  | 0.6576 -           0.9018    | 51.3489     | 0.6129   | 0.2900     - 0.1550\n",
      "Training 71     - 101    -          0.2909| 0.0715  | 0.3624 -           0.9026    | 51.2061     | 0.6554   | 0.3861     - 1.5185\n",
      "Validati 71     - 23     -          0.4559| 0.2064  | 0.6623 -           0.8971    | 51.3183     | 0.5910   | 0.3119     - 0.1415\n",
      "Training 72     - 101    -          0.3306| 0.1317  | 0.4622 -           0.9019    | 51.1930     | 0.6526   | 0.3813     - 1.4954\n",
      "Validati 72     - 23     -          0.4437| 0.2082  | 0.6519 -           0.9007    | 51.3163     | 0.6125   | 0.3114     - 0.1425\n",
      "Training 73     - 101    -          0.2115| 0.0663  | 0.2778 -           0.9004    | 51.1881     | 0.6457   | 0.3762     - 1.5132\n",
      "Validati 73     - 23     -          0.4595| 0.2059  | 0.6654 -           0.9001    | 51.3253     | 0.6048   | 0.3000     - 0.1438\n",
      "Training 74     - 101    -          0.2022| 0.0843  | 0.2865 -           0.9025    | 51.1863     | 0.6572   | 0.3860     - 1.5896\n",
      "Validati 74     - 23     -          0.4456| 0.2067  | 0.6523 -           0.9016    | 51.3143     | 0.6149   | 0.3065     - 0.1597\n",
      "Training 75     - 101    -          0.2037| 0.0528  | 0.2566 -           0.9015    | 51.2077     | 0.6508   | 0.3870     - 1.5894\n",
      "Validati 75     - 23     -          0.4438| 0.2074  | 0.6512 -           0.8971    | 51.3029     | 0.5938   | 0.3064     - 0.1607\n",
      "Training 76     - 101    -          0.2681| 0.0697  | 0.3378 -           0.9023    | 51.2025     | 0.6539   | 0.3821     - 1.6134\n",
      "Validati 76     - 23     -          0.4484| 0.2077  | 0.6561 -           0.9007    | 51.3548     | 0.6103   | 0.3086     - 0.1573\n",
      "Training 77     - 101    -          0.2793| 0.0985  | 0.3778 -           0.9040    | 51.1800     | 0.6658   | 0.3875     - 1.5987\n",
      "Validati 77     - 23     -          0.4521| 0.2073  | 0.6594 -           0.9018    | 51.3277     | 0.6133   | 0.3064     - 0.1585\n",
      "Training 78     - 101    -          0.2581| 0.0660  | 0.3240 -           0.9020    | 51.2152     | 0.6551   | 0.3848     - 1.5281\n",
      "Validati 78     - 23     -          0.4551| 0.2057  | 0.6608 -           0.9026    | 51.3632     | 0.6203   | 0.3108     - 0.1412\n",
      "Training 79     - 101    -          0.2877| 0.1028  | 0.3905 -           0.9013    | 51.2129     | 0.6517   | 0.3857     - 1.5735\n",
      "Validati 79     - 23     -          0.4413| 0.2151  | 0.6564 -           0.8802    | 51.1853     | 0.5221   | 0.2690     - 0.1578\n",
      "Training 80     - 101    -          0.2410| 0.0552  | 0.2962 -           0.9041    | 51.2176     | 0.6606   | 0.3865     - 1.5956\n",
      "Validati 80     - 23     -          0.4446| 0.2076  | 0.6522 -           0.9038    | 51.3417     | 0.6353   | 0.3216     - 0.1576\n",
      "Training 81     - 101    -          0.3232| 0.0782  | 0.4014 -           0.9022    | 51.1943     | 0.6553   | 0.3790     - 1.5273\n",
      "Validati 81     - 23     -          0.4628| 0.2096  | 0.6724 -           0.9043    | 51.3561     | 0.6318   | 0.2995     - 0.1427\n",
      "Training 82     - 101    -          0.2974| 0.1162  | 0.4137 -           0.9033    | 51.1726     | 0.6621   | 0.3833     - 1.4962\n",
      "Validati 82     - 23     -          0.4556| 0.2059  | 0.6615 -           0.9046    | 51.3311     | 0.6318   | 0.3055     - 0.1455\n",
      "Training 83     - 101    -          0.2128| 0.1057  | 0.3185 -           0.9034    | 51.2263     | 0.6596   | 0.3987     - 1.5337\n",
      "Validati 83     - 23     -          0.4465| 0.2068  | 0.6533 -           0.8955    | 51.3315     | 0.5951   | 0.2976     - 0.1427\n",
      "Training 84     - 101    -          0.1649| 0.1055  | 0.2704 -           0.9053    | 51.1926     | 0.6664   | 0.3851     - 1.5287\n",
      "Validati 84     - 23     -          0.4560| 0.2073  | 0.6634 -           0.9015    | 51.3717     | 0.6190   | 0.3047     - 0.1549\n",
      "Training 85     - 101    -          0.2929| 0.1287  | 0.4216 -           0.9038    | 51.2126     | 0.6640   | 0.3960     - 1.4945\n",
      "Validati 85     - 23     -          0.4457| 0.2089  | 0.6546 -           0.9033    | 51.3409     | 0.6279   | 0.3116     - 0.1406\n",
      "Training 86     - 101    -          0.1984| 0.0869  | 0.2852 -           0.9041    | 51.1642     | 0.6635   | 0.3825     - 1.5001\n",
      "Validati 86     - 23     -          0.4516| 0.2060  | 0.6576 -           0.9050    | 51.3402     | 0.6334   | 0.3198     - 0.1407\n",
      "Training 87     - 101    -          0.2161| 0.0994  | 0.3155 -           0.9033    | 51.2116     | 0.6619   | 0.3945     - 1.5109\n",
      "Validati 87     - 23     -          0.4517| 0.2068  | 0.6585 -           0.9023    | 51.3280     | 0.6154   | 0.3007     - 0.1437\n",
      "Training 88     - 101    -          0.2043| 0.1147  | 0.3190 -           0.9061    | 51.2027     | 0.6727   | 0.4022     - 1.5174\n",
      "Validati 88     - 23     -          0.4411| 0.2107  | 0.6518 -           0.9067    | 51.3738     | 0.6408   | 0.3192     - 0.1602\n",
      "Training 89     - 101    -          0.2517| 0.0846  | 0.3363 -           0.9059    | 51.2108     | 0.6706   | 0.3964     - 1.5867\n",
      "Validati 89     - 23     -          0.4374| 0.2076  | 0.6451 -           0.9061    | 51.3626     | 0.6394   | 0.3091     - 0.1585\n",
      "Training 90     - 101    -          0.1612| 0.0654  | 0.2265 -           0.9067    | 51.2056     | 0.6739   | 0.3900     - 1.6050\n",
      "Validati 90     - 23     -          0.4303| 0.2100  | 0.6403 -           0.9012    | 51.3530     | 0.6194   | 0.3219     - 0.1578\n",
      "Training 91     - 101    -          0.2632| 0.1098  | 0.3731 -           0.9054    | 51.1916     | 0.6672   | 0.3875     - 1.5915\n",
      "Validati 91     - 23     -          0.4326| 0.2091  | 0.6417 -           0.9030    | 51.3336     | 0.6313   | 0.3295     - 0.1614\n",
      "Training 92     - 101    -          0.2292| 0.0909  | 0.3200 -           0.9079    | 51.1935     | 0.6822   | 0.3864     - 1.5924\n",
      "Validati 92     - 23     -          0.4375| 0.2074  | 0.6449 -           0.9052    | 51.3827     | 0.6266   | 0.3125     - 0.1558\n",
      "Training 93     - 101    -          0.2371| 0.1113  | 0.3485 -           0.9052    | 51.2091     | 0.6649   | 0.3888     - 1.5910\n",
      "Validati 93     - 23     -          0.4653| 0.2091  | 0.6744 -           0.9087    | 51.3447     | 0.6467   | 0.3058     - 0.1587\n",
      "Training 94     - 101    -          0.2576| 0.0651  | 0.3226 -           0.9064    | 51.2175     | 0.6711   | 0.3942     - 1.5017\n",
      "Validati 94     - 23     -          0.4573| 0.2071  | 0.6643 -           0.9052    | 51.3531     | 0.6310   | 0.2986     - 0.1398\n",
      "Training 95     - 101    -          0.2620| 0.1172  | 0.3792 -           0.9082    | 51.1774     | 0.6795   | 0.3914     - 1.5014\n",
      "Validati 95     - 23     -          0.4519| 0.2075  | 0.6594 -           0.9046    | 51.3577     | 0.6331   | 0.3167     - 0.1388\n",
      "Training 96     - 101    -          0.2504| 0.1055  | 0.3559 -           0.9086    | 51.1983     | 0.6829   | 0.3935     - 1.4999\n",
      "Validati 96     - 23     -          0.4492| 0.2074  | 0.6566 -           0.9054    | 51.3247     | 0.6335   | 0.2895     - 0.1401\n",
      "Training 97     - 101    -          0.2608| 0.0809  | 0.3417 -           0.9067    | 51.1748     | 0.6759   | 0.3824     - 1.5219\n",
      "Validati 97     - 23     -          0.4504| 0.2064  | 0.6568 -           0.9050    | 51.3735     | 0.6324   | 0.3125     - 0.1535\n",
      "Training 98     - 101    -          0.1723| 0.0709  | 0.2432 -           0.9078    | 51.1937     | 0.6784   | 0.3972     - 1.5197\n",
      "Validati 98     - 23     -          0.4530| 0.2081  | 0.6610 -           0.9082    | 51.3660     | 0.6480   | 0.3156     - 0.1415\n",
      "Training 99     - 101    -          0.2811| 0.0614  | 0.3425 -           0.9096    | 51.1795     | 0.6873   | 0.3982     - 1.5822\n",
      "Validati 99     - 23     -          0.4484| 0.2079  | 0.6563 -           0.9091    | 51.3743     | 0.6493   | 0.3190     - 0.1587\n",
      "Training 100    - 101    -          0.2066| 0.0757  | 0.2823 -           0.9069    | 51.2171     | 0.6756   | 0.4018     - 1.6347\n",
      "Validati 100    - 23     -          0.4413| 0.2077  | 0.6490 -           0.9037    | 51.3738     | 0.6264   | 0.3012     - 0.1608\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
