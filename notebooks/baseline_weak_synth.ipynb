{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:15<00:00, 490.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    validation_ratio=0.2,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53, # The model output localisation with a résolution of ~ 18ms --> 53 temporal bins\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak ans synthetic20 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "Loading dataset: train, subset: weak\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: synthetic20\u001b[0m\n",
      "Loading dataset: train, subset: synthetic20\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/synthetic20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "manager.add_subset(\"synthetic20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> Creating new train / validation split\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> validation ratio : 0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.split_train_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%  setup augmentation and create pytorch dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MultipleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(MultipleDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        for d in self.datasets:\n",
    "            assert not isinstance(d, IterableDataset), \"ConcatDataset does not support IterableDataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [d[sample_idx] for d in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218, 706)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset\n",
    "\n",
    "- We want both the weak and strong ground truth --> the *weak* and *strong* parameters to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, weak=True, strong=True, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, weak=True, strong=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "This model is the same than the weak baseline but have an extra output. <br />\n",
    "the loc_output is compose of a single convolution layer with nb_filters == nb_class. <br />\n",
    "Since their is some pooling layer, the *loc_ouput* have a precision of 53 bins (~= 18 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakStrongBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (weak_output): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       "  (strong_output): Sequential(\n",
       "    (0): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakStrongBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_weak_output.Flatten_0                  -         [1, 1696]       -   \n",
      "16_weak_output.Dropout_1                  -         [1, 1696]       -   \n",
      "17_weak_output.Linear_2          [1696, 10]           [1, 10]  16.97k   \n",
      "18_strong_output.Conv2d_0    [32, 10, 1, 1]    [1, 10, 1, 53]   330.0   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_weak_output.Flatten_0             -  \n",
      "16_weak_output.Dropout_1             -  \n",
      "17_weak_output.Linear_2         16.96k  \n",
      "18_strong_output.Conv2d_0       16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             36.308k\n",
      "Trainable params         36.308k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.625696M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom loss function\n",
    "\n",
    "Since not all file have strong truth, it is necessary to remove those files. <br />\n",
    "For that, the strong mask is computed. If the sum of the strong ground truth is equal to 0 then it is a fake one <br />\n",
    "This file strong loss must not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_synth_loss(logits_weak, logits_strong, y_weak, y_strong, reduce: str = \"mean\"):\n",
    "    assert reduce in [\"mean\", \"sum\"], \"support only \\\"mean\\\" and \\\"sum\\\"\"\n",
    "    \n",
    "    #  Reduction function\n",
    "    if reduce == \"mean\":\n",
    "        reduce_fn = torch.mean\n",
    "    elif reduce == \"sum\":\n",
    "        reduce_fn = torch.sum\n",
    "    \n",
    "    # based on Binary Cross Entropy loss\n",
    "    weak_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    strong_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    # calc separate loss function\n",
    "    weak_bce = weak_criterion(logits_weak, y_weak)\n",
    "    strong_bce = strong_criterion(logits_strong, y_strong)\n",
    "    \n",
    "    weak_bce = reduce_fn(weak_bce, dim=1)\n",
    "    strong_bce = reduce_fn(strong_bce, dim=(1, 2))\n",
    "    \n",
    "    # calc strong mask\n",
    "    strong_mask = torch.clamp(torch.sum(y_strong, dim=(1, 2)), 0, 1) # vector of 0 or 1\n",
    "    strong_mask = strong_mask.detach() # declared not to need gradients\n",
    "    \n",
    "    # Output the different loss for logging purpose\n",
    "    weak_loss = reduce_fn(weak_bce)\n",
    "    strong_loss = reduce_fn(strong_mask * strong_bce)\n",
    "    total_loss = reduce_fn(weak_bce + strong_mask * strong_bce)\n",
    "    \n",
    "    return weak_loss, strong_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "weak_binacc_func = BinaryAccuracy()\n",
    "strong_binacc_func = BinaryAccuracy()\n",
    "weak_f_func = FScore()\n",
    "strong_f_func = FScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_metrics():\n",
    "    metrics = [weak_binacc_func, strong_binacc_func, weak_f_func, strong_f_func]\n",
    "    \n",
    "    for m in metrics:\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6}| {:<8.8}| {:<6.6} - {:<9.9} {:<10.10}| {:<12.12}| {:<9.9}| {:<11.11}- {:<6.6}\"\n",
    "\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f}| {:<8.4f}| {:<6.4f} - {:<9.9} {:<10.4f}| {:<12.4f}| {:<9.4f}| {:<11.4f}- {:<6.4f}\"\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"Strong \", \"Total \", \"metrics: \", \"Weak acc \", \"Strong acc \", \"Weak F1 \", \"Strong F1\", \"Time\"\n",
    ")\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X, y) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X = X.cuda().float()\n",
    "        y_weak = y[0].cuda().float()\n",
    "        y_strong = y[1].cuda().float()\n",
    "        \n",
    "        weak_logits, strong_logits = model(X)\n",
    "        \n",
    "        # calc the loss\n",
    "        weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "            weak_logits, strong_logits,\n",
    "            y_weak, y_strong,\n",
    "            reduce=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"train/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"train/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "        \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y_weak = y[0].cuda().float()\n",
    "            y_strong = y[1].cuda().float()\n",
    "\n",
    "            weak_logits, strong_logits = model(X)\n",
    "\n",
    "            # calc the loss\n",
    "            weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "                weak_logits, strong_logits,\n",
    "                y_weak, y_strong,\n",
    "                reduce=\"mean\"\n",
    "            )\n",
    "            \n",
    "             # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"val/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n",
      "\n",
      "Training 1      - 101    -          0.4659| 0.1687  | 0.6346 -           0.8207    | 0.9038      | 0.3028   | 0.0226     - 41.2679\n",
      "\u001b[1;4mValidati 1      - 23     -          0.6096| 0.2816  | 0.8913 -           0.8450    | 0.9633      | 0.2840   | 0.0022     - 8.7866\u001b[0m\n",
      "Training 2      - 101    -          0.4808| 0.1322  | 0.6130 -           0.8287    | 0.9647      | 0.3418   | 0.0606     - 1.5485\n",
      "\u001b[1;4mValidati 2      - 23     -          0.5951| 0.2853  | 0.8804 -           0.8413    | 0.9582      | 0.3862   | 0.1686     - 0.1422\u001b[0m\n",
      "Training 3      - 101    -          0.3791| 0.1086  | 0.4876 -           0.8372    | 0.9645      | 0.3656   | 0.1317     - 1.4992\n",
      "\u001b[1;4mValidati 3      - 23     -          0.5822| 0.2323  | 0.8145 -           0.8518    | 0.9644      | 0.2981   | 0.1240     - 0.1404\u001b[0m\n",
      "Training 4      - 101    -          0.3453| 0.1242  | 0.4695 -           0.8419    | 0.9649      | 0.3850   | 0.1601     - 1.5030\n",
      "\u001b[1;4mValidati 4      - 23     -          0.5460| 0.2323  | 0.7783 -           0.8533    | 0.9644      | 0.4428   | 0.1500     - 0.1446\u001b[0m\n",
      "Training 5      - 101    -          0.3654| 0.1333  | 0.4987 -           0.8471    | 0.9652      | 0.4079   | 0.1736     - 1.5096\n",
      "\u001b[1;4mValidati 5      - 23     -          0.4396| 0.2417  | 0.6813 -           0.8515    | 0.9630      | 0.4262   | 0.1653     - 0.1395\u001b[0m\n",
      "Training 6      - 101    -          0.3882| 0.0821  | 0.4703 -           0.8510    | 0.9650      | 0.4205   | 0.1825     - 1.5124\n",
      "\u001b[1;4mValidati 6      - 23     -          0.4506| 0.2682  | 0.7188 -           0.8614    | 0.9649      | 0.3797   | 0.1379     - 0.1416\u001b[0m\n",
      "Training 7      - 101    -          0.3840| 0.1313  | 0.5153 -           0.8537    | 0.9649      | 0.4378   | 0.1878     - 1.5030\n",
      "\u001b[1;4mValidati 7      - 23     -          0.3566| 0.2398  | 0.5964 -           0.8694    | 0.9648      | 0.4398   | 0.1505     - 0.1404\u001b[0m\n",
      "Training 8      - 101    -          0.3857| 0.1196  | 0.5052 -           0.8556    | 0.9650      | 0.4441   | 0.2024     - 1.5140\n",
      "\u001b[1;4mValidati 8      - 23     -          0.4040| 0.2629  | 0.6668 -           0.8656    | 0.9634      | 0.4804   | 0.1752     - 0.1408\u001b[0m\n",
      "Training 9      - 101    -          0.3605| 0.0806  | 0.4411 -           0.8593    | 0.9647      | 0.4581   | 0.2121     - 1.5169\n",
      "\u001b[1;4mValidati 9      - 23     -          0.3747| 0.2571  | 0.6318 -           0.8640    | 0.9646      | 0.3993   | 0.1512     - 0.1430\u001b[0m\n",
      "Training 10     - 101    -          0.3071| 0.0878  | 0.3949 -           0.8627    | 0.9648      | 0.4701   | 0.2138     - 1.5079\n",
      "\u001b[1;4mValidati 10     - 23     -          0.4059| 0.2504  | 0.6563 -           0.8776    | 0.9646      | 0.5153   | 0.1605     - 0.1408\u001b[0m\n",
      "Training 11     - 101    -          0.3843| 0.1141  | 0.4984 -           0.8603    | 0.9647      | 0.4600   | 0.2187     - 1.5122\n",
      "\u001b[1;4mValidati 11     - 23     -          0.4237| 0.2510  | 0.6747 -           0.8701    | 0.9651      | 0.4560   | 0.1711     - 0.1410\u001b[0m\n",
      "Training 12     - 101    -          0.3176| 0.0773  | 0.3949 -           0.8648    | 0.9649      | 0.4891   | 0.2412     - 1.5095\n",
      "\u001b[1;4mValidati 12     - 23     -          0.4870| 0.2548  | 0.7417 -           0.8561    | 0.9659      | 0.3843   | 0.1818     - 0.1414\u001b[0m\n",
      "Training 13     - 101    -          0.3225| 0.1166  | 0.4391 -           0.8651    | 0.9651      | 0.4802   | 0.2495     - 1.5108\n",
      "\u001b[1;4mValidati 13     - 23     -          0.4448| 0.2523  | 0.6971 -           0.8702    | 0.9665      | 0.4590   | 0.2031     - 0.1410\u001b[0m\n",
      "Training 14     - 101    -          0.3095| 0.1174  | 0.4269 -           0.8700    | 0.9654      | 0.5050   | 0.2727     - 1.5165\n",
      "\u001b[1;4mValidati 14     - 23     -          0.4036| 0.2481  | 0.6517 -           0.8745    | 0.9657      | 0.4879   | 0.2211     - 0.1404\u001b[0m\n",
      "Training 15     - 101    -          0.3733| 0.1356  | 0.5088 -           0.8703    | 0.9650      | 0.5074   | 0.2672     - 1.5214\n",
      "\u001b[1;4mValidati 15     - 23     -          0.4421| 0.2508  | 0.6929 -           0.8747    | 0.9660      | 0.4640   | 0.1907     - 0.1413\u001b[0m\n",
      "Training 16     - 101    -          0.2827| 0.0927  | 0.3755 -           0.8737    | 0.9654      | 0.5212   | 0.2954     - 1.5195\n",
      "\u001b[1;4mValidati 16     - 23     -          0.4480| 0.2490  | 0.6970 -           0.8791    | 0.9666      | 0.5211   | 0.2322     - 0.1393\u001b[0m\n",
      "Training 17     - 101    -          0.3269| 0.0741  | 0.4010 -           0.8705    | 0.9651      | 0.5152   | 0.2856     - 1.5143\n",
      "\u001b[1;4mValidati 17     - 23     -          0.4615| 0.2458  | 0.7073 -           0.8821    | 0.9672      | 0.5157   | 0.2438     - 0.1409\u001b[0m\n",
      "Training 18     - 101    -          0.3402| 0.0991  | 0.4393 -           0.8726    | 0.9653      | 0.5204   | 0.2893     - 1.5436\n",
      "\u001b[1;4mValidati 18     - 23     -          0.4258| 0.2407  | 0.6666 -           0.8811    | 0.9666      | 0.5294   | 0.2393     - 0.1406\u001b[0m\n",
      "Training 19     - 101    -          0.2401| 0.1052  | 0.3453 -           0.8751    | 0.9651      | 0.5314   | 0.2874     - 1.5136\n",
      "\u001b[1;4mValidati 19     - 23     -          0.4554| 0.2551  | 0.7105 -           0.8746    | 0.9661      | 0.5571   | 0.2392     - 0.1407\u001b[0m\n",
      "Training 20     - 101    -          0.3502| 0.0958  | 0.4460 -           0.8765    | 0.9657      | 0.5416   | 0.3113     - 1.5123\n",
      "\u001b[1;4mValidati 20     - 23     -          0.4340| 0.2481  | 0.6821 -           0.8769    | 0.9649      | 0.5789   | 0.2516     - 0.1410\u001b[0m\n",
      "Training 21     - 101    -          0.3615| 0.0972  | 0.4587 -           0.8772    | 0.9650      | 0.5422   | 0.2953     - 1.5211\n",
      "\u001b[1;4mValidati 21     - 23     -          0.4266| 0.2349  | 0.6615 -           0.8825    | 0.9663      | 0.5513   | 0.2462     - 0.1411\u001b[0m\n",
      "Training 22     - 101    -          0.2968| 0.0876  | 0.3844 -           0.8778    | 0.9656      | 0.5463   | 0.3147     - 1.5167\n",
      "\u001b[1;4mValidati 22     - 23     -          0.4484| 0.2366  | 0.6850 -           0.8769    | 0.9644      | 0.5314   | 0.2519     - 0.1422\u001b[0m\n",
      "Training 23     - 101    -          0.3035| 0.1062  | 0.4096 -           0.8799    | 0.9656      | 0.5557   | 0.3146     - 1.5175\n",
      "\u001b[1;4mValidati 23     - 23     -          0.4768| 0.2278  | 0.7046 -           0.8810    | 0.9666      | 0.5256   | 0.2527     - 0.1409\u001b[0m\n",
      "Training 24     - 101    -          0.3216| 0.1047  | 0.4263 -           0.8830    | 0.9659      | 0.5661   | 0.3272     - 1.5138\n",
      "\u001b[1;4mValidati 24     - 23     -          0.4152| 0.2234  | 0.6386 -           0.8846    | 0.9667      | 0.5337   | 0.2655     - 0.1412\u001b[0m\n",
      "Training 25     - 101    -          0.2118| 0.0577  | 0.2695 -           0.8823    | 0.9660      | 0.5652   | 0.3245     - 1.5145\n",
      "\u001b[1;4mValidati 25     - 23     -          0.4529| 0.2273  | 0.6801 -           0.8834    | 0.9671      | 0.5215   | 0.2445     - 0.1404\u001b[0m\n",
      "Training 26     - 101    -          0.3079| 0.0891  | 0.3970 -           0.8826    | 0.9658      | 0.5680   | 0.3294     - 1.5184\n",
      "\u001b[1;4mValidati 26     - 23     -          0.4411| 0.2256  | 0.6667 -           0.8818    | 0.9669      | 0.5348   | 0.2484     - 0.1394\u001b[0m\n",
      "Training 27     - 101    -          0.3103| 0.0926  | 0.4029 -           0.8819    | 0.9660      | 0.5628   | 0.3344     - 1.5326\n",
      "\u001b[1;4mValidati 27     - 23     -          0.4240| 0.2215  | 0.6455 -           0.8886    | 0.9672      | 0.5499   | 0.2743     - 0.1427\u001b[0m\n",
      "Training 28     - 101    -          0.3779| 0.0890  | 0.4669 -           0.8819    | 0.9661      | 0.5645   | 0.3316     - 1.5344\n",
      "\u001b[1;4mValidati 28     - 23     -          0.4257| 0.2204  | 0.6461 -           0.8882    | 0.9672      | 0.5470   | 0.2405     - 0.1400\u001b[0m\n",
      "Training 29     - 101    -          0.3143| 0.0667  | 0.3810 -           0.8867    | 0.9656      | 0.5854   | 0.3381     - 1.5234\n",
      "\u001b[1;4mValidati 29     - 23     -          0.4382| 0.2249  | 0.6631 -           0.8927    | 0.9676      | 0.5925   | 0.2690     - 0.1384\u001b[0m\n",
      "Training 30     - 101    -          0.3100| 0.1012  | 0.4111 -           0.8838    | 0.9655      | 0.5732   | 0.3269     - 1.5162\n",
      "\u001b[1;4mValidati 30     - 23     -          0.4059| 0.2200  | 0.6259 -           0.8918    | 0.9675      | 0.6016   | 0.2404     - 0.1433\u001b[0m\n",
      "Training 31     - 101    -          0.2756| 0.0720  | 0.3475 -           0.8854    | 0.9660      | 0.5806   | 0.3443     - 1.5161\n",
      "\u001b[1;4mValidati 31     - 23     -          0.4456| 0.2173  | 0.6629 -           0.8901    | 0.9672      | 0.6060   | 0.2728     - 0.1435\u001b[0m\n",
      "Training 32     - 101    -          0.2461| 0.1052  | 0.3513 -           0.8854    | 0.9660      | 0.5793   | 0.3434     - 1.5243\n",
      "\u001b[1;4mValidati 32     - 23     -          0.4597| 0.2255  | 0.6852 -           0.8874    | 0.9671      | 0.5430   | 0.2646     - 0.1400\u001b[0m\n",
      "Training 33     - 101    -          0.2869| 0.0556  | 0.3425 -           0.8863    | 0.9658      | 0.5876   | 0.3445     - 1.5200\n",
      "\u001b[1;4mValidati 33     - 23     -          0.4303| 0.2226  | 0.6530 -           0.8893    | 0.9674      | 0.5597   | 0.2836     - 0.1404\u001b[0m\n",
      "Training 34     - 101    -          0.3562| 0.1229  | 0.4791 -           0.8866    | 0.9655      | 0.5873   | 0.3475     - 1.5171\n",
      "\u001b[1;4mValidati 34     - 23     -          0.4456| 0.2174  | 0.6630 -           0.8925    | 0.9667      | 0.5710   | 0.2894     - 0.1414\u001b[0m\n",
      "Training 35     - 101    -          0.2551| 0.0780  | 0.3331 -           0.8886    | 0.9656      | 0.5949   | 0.3430     - 1.5141\n",
      "\u001b[1;4mValidati 35     - 23     -          0.4303| 0.2198  | 0.6501 -           0.8906    | 0.9671      | 0.5846   | 0.2413     - 0.1411\u001b[0m\n",
      "Training 36     - 101    -          0.2766| 0.0868  | 0.3634 -           0.8901    | 0.9653      | 0.6046   | 0.3405     - 1.5215\n",
      "\u001b[1;4mValidati 36     - 23     -          0.4060| 0.2167  | 0.6227 -           0.8978    | 0.9678      | 0.6058   | 0.2706     - 0.1414\u001b[0m\n",
      "Training 37     - 101    -          0.2442| 0.0996  | 0.3438 -           0.8892    | 0.9660      | 0.5948   | 0.3505     - 1.5247\n",
      "\u001b[1;4mValidati 37     - 23     -          0.4249| 0.2215  | 0.6464 -           0.8901    | 0.9673      | 0.5765   | 0.2572     - 0.1398\u001b[0m\n",
      "Training 38     - 101    -          0.3164| 0.0953  | 0.4117 -           0.8909    | 0.9659      | 0.6051   | 0.3542     - 1.5184\n",
      "\u001b[1;4mValidati 38     - 23     -          0.4175| 0.2190  | 0.6365 -           0.8969    | 0.9675      | 0.6241   | 0.2987     - 0.1401\u001b[0m\n",
      "Training 39     - 101    -          0.3071| 0.1058  | 0.4129 -           0.8897    | 0.9655      | 0.6036   | 0.3448     - 1.5281\n",
      "\u001b[1;4mValidati 39     - 23     -          0.3750| 0.2174  | 0.5923 -           0.8959    | 0.9675      | 0.6110   | 0.2955     - 0.1417\u001b[0m\n",
      "Training 40     - 101    -          0.2744| 0.0625  | 0.3369 -           0.8896    | 0.9654      | 0.6021   | 0.3422     - 1.5188\n",
      "\u001b[1;4mValidati 40     - 23     -          0.4395| 0.2156  | 0.6551 -           0.8904    | 0.9677      | 0.5600   | 0.2938     - 0.1406\u001b[0m\n",
      "Training 41     - 101    -          0.3486| 0.1194  | 0.4681 -           0.8911    | 0.9658      | 0.6062   | 0.3551     - 1.5192\n",
      "\u001b[1;4mValidati 41     - 23     -          0.4481| 0.2144  | 0.6625 -           0.8931    | 0.9680      | 0.5915   | 0.3051     - 0.1402\u001b[0m\n",
      "Training 42     - 101    -          0.2262| 0.0256  | 0.2518 -           0.8897    | 0.9657      | 0.5983   | 0.3495     - 1.5244\n",
      "\u001b[1;4mValidati 42     - 23     -          0.4296| 0.2209  | 0.6505 -           0.8868    | 0.9661      | 0.5587   | 0.2933     - 0.1413\u001b[0m\n",
      "Training 43     - 101    -          0.2336| 0.1045  | 0.3381 -           0.8905    | 0.9652      | 0.6057   | 0.3418     - 1.5162\n",
      "\u001b[1;4mValidati 43     - 23     -          0.4390| 0.2177  | 0.6568 -           0.8971    | 0.9675      | 0.6117   | 0.2890     - 0.1388\u001b[0m\n",
      "Training 44     - 101    -          0.2736| 0.0882  | 0.3618 -           0.8911    | 0.9659      | 0.6060   | 0.3592     - 1.5243\n",
      "\u001b[1;4mValidati 44     - 23     -          0.4295| 0.2139  | 0.6434 -           0.8961    | 0.9676      | 0.6075   | 0.2605     - 0.1426\u001b[0m\n",
      "Training 45     - 101    -          0.3425| 0.0709  | 0.4134 -           0.8931    | 0.9657      | 0.6153   | 0.3635     - 1.5206\n",
      "\u001b[1;4mValidati 45     - 23     -          0.4477| 0.2138  | 0.6615 -           0.8974    | 0.9677      | 0.6254   | 0.2683     - 0.1431\u001b[0m\n",
      "Training 46     - 101    -          0.2889| 0.1183  | 0.4072 -           0.8925    | 0.9651      | 0.6142   | 0.3509     - 1.5177\n",
      "\u001b[1;4mValidati 46     - 23     -          0.4533| 0.2135  | 0.6669 -           0.8967    | 0.9678      | 0.6021   | 0.2845     - 0.1405\u001b[0m\n",
      "Training 47     - 101    -          0.2074| 0.0715  | 0.2789 -           0.8942    | 0.9655      | 0.6223   | 0.3606     - 1.5258\n",
      "\u001b[1;4mValidati 47     - 23     -          0.4386| 0.2132  | 0.6518 -           0.8955    | 0.9680      | 0.5804   | 0.2783     - 0.1396\u001b[0m\n",
      "Training 48     - 101    -          0.1829| 0.0595  | 0.2424 -           0.8940    | 0.9653      | 0.6199   | 0.3612     - 1.5154\n",
      "\u001b[1;4mValidati 48     - 23     -          0.4492| 0.2196  | 0.6688 -           0.8925    | 0.9668      | 0.5712   | 0.2910     - 0.1404\u001b[0m\n",
      "Training 49     - 101    -          0.2551| 0.0672  | 0.3223 -           0.8950    | 0.9652      | 0.6276   | 0.3577     - 1.5195\n",
      "\u001b[1;4mValidati 49     - 23     -          0.4521| 0.2190  | 0.6711 -           0.8981    | 0.9676      | 0.6063   | 0.2865     - 0.1404\u001b[0m\n",
      "Training 50     - 101    -          0.3006| 0.0969  | 0.3975 -           0.8932    | 0.9651      | 0.6214   | 0.3611     - 1.5179\n",
      "\u001b[1;4mValidati 50     - 23     -          0.4423| 0.2150  | 0.6573 -           0.8937    | 0.9682      | 0.5878   | 0.2867     - 0.1392\u001b[0m\n",
      "Training 51     - 101    -          0.2461| 0.0654  | 0.3115 -           0.8959    | 0.9656      | 0.6302   | 0.3644     - 1.5230\n",
      "\u001b[1;4mValidati 51     - 23     -          0.4394| 0.2115  | 0.6509 -           0.8942    | 0.9680      | 0.5933   | 0.2839     - 0.1416\u001b[0m\n",
      "Training 52     - 101    -          0.2373| 0.0630  | 0.3003 -           0.8948    | 0.9657      | 0.6279   | 0.3726     - 1.5162\n",
      "\u001b[1;4mValidati 52     - 23     -          0.4201| 0.2128  | 0.6328 -           0.8973    | 0.9660      | 0.6179   | 0.3188     - 0.1440\u001b[0m\n",
      "Training 53     - 101    -          0.2619| 0.1018  | 0.3637 -           0.8961    | 0.9654      | 0.6275   | 0.3633     - 1.5254\n",
      "\u001b[1;4mValidati 53     - 23     -          0.4436| 0.2125  | 0.6561 -           0.8997    | 0.9683      | 0.6121   | 0.2888     - 0.1396\u001b[0m\n",
      "Training 54     - 101    -          0.2689| 0.1290  | 0.3980 -           0.8964    | 0.9657      | 0.6298   | 0.3681     - 1.5318\n",
      "\u001b[1;4mValidati 54     - 23     -          0.4298| 0.2124  | 0.6422 -           0.8973    | 0.9681      | 0.5956   | 0.3025     - 0.1442\u001b[0m\n",
      "Training 55     - 101    -          0.2390| 0.0737  | 0.3126 -           0.8967    | 0.9656      | 0.6332   | 0.3671     - 1.5225\n",
      "\u001b[1;4mValidati 55     - 23     -          0.4166| 0.2124  | 0.6290 -           0.8947    | 0.9676      | 0.5961   | 0.2825     - 0.1442\u001b[0m\n",
      "Training 56     - 101    -          0.2467| 0.0753  | 0.3220 -           0.8962    | 0.9659      | 0.6286   | 0.3689     - 1.5223\n",
      "\u001b[1;4mValidati 56     - 23     -          0.4437| 0.2152  | 0.6589 -           0.8912    | 0.9675      | 0.5577   | 0.2507     - 0.1407\u001b[0m\n",
      "Training 57     - 101    -          0.3263| 0.0873  | 0.4136 -           0.8969    | 0.9658      | 0.6353   | 0.3742     - 1.5405\n",
      "\u001b[1;4mValidati 57     - 23     -          0.4424| 0.2136  | 0.6560 -           0.9001    | 0.9684      | 0.6109   | 0.2992     - 0.1423\u001b[0m\n",
      "Training 58     - 101    -          0.2574| 0.1148  | 0.3723 -           0.8973    | 0.9660      | 0.6363   | 0.3719     - 1.5337\n",
      "\u001b[1;4mValidati 58     - 23     -          0.4401| 0.2168  | 0.6569 -           0.8970    | 0.9680      | 0.6004   | 0.3164     - 0.1405\u001b[0m\n",
      "Training 59     - 101    -          0.1850| 0.0700  | 0.2550 -           0.8977    | 0.9659      | 0.6397   | 0.3705     - 1.5179\n",
      "\u001b[1;4mValidati 59     - 23     -          0.4479| 0.2106  | 0.6585 -           0.9004    | 0.9683      | 0.6077   | 0.2984     - 0.1429\u001b[0m\n",
      "Training 60     - 101    -          0.1928| 0.0759  | 0.2686 -           0.8996    | 0.9659      | 0.6462   | 0.3792     - 1.5187\n",
      "\u001b[1;4mValidati 60     - 23     -          0.4302| 0.2116  | 0.6418 -           0.8977    | 0.9678      | 0.6111   | 0.2783     - 0.1440\u001b[0m\n",
      "Training 61     - 101    -          0.2677| 0.0839  | 0.3516 -           0.9015    | 0.9653      | 0.6510   | 0.3651     - 1.5305\n",
      "\u001b[1;4mValidati 61     - 23     -          0.4086| 0.2159  | 0.6245 -           0.8917    | 0.9671      | 0.5649   | 0.2567     - 0.1434\u001b[0m\n",
      "Training 62     - 101    -          0.3330| 0.1179  | 0.4509 -           0.9007    | 0.9659      | 0.6485   | 0.3786     - 1.5220\n",
      "\u001b[1;4mValidati 62     - 23     -          0.4423| 0.2106  | 0.6529 -           0.8997    | 0.9687      | 0.6081   | 0.2999     - 0.1391\u001b[0m\n",
      "Training 63     - 101    -          0.2630| 0.0796  | 0.3426 -           0.8987    | 0.9657      | 0.6399   | 0.3653     - 1.5245\n",
      "\u001b[1;4mValidati 63     - 23     -          0.4305| 0.2193  | 0.6498 -           0.8929    | 0.9663      | 0.5821   | 0.3126     - 0.1406\u001b[0m\n",
      "Training 64     - 101    -          0.3638| 0.1199  | 0.4837 -           0.8984    | 0.9655      | 0.6399   | 0.3647     - 1.5250\n",
      "\u001b[1;4mValidati 64     - 23     -          0.4324| 0.2107  | 0.6431 -           0.9014    | 0.9679      | 0.6183   | 0.3235     - 0.1395\u001b[0m\n",
      "Training 65     - 101    -          0.2631| 0.0696  | 0.3326 -           0.9023    | 0.9655      | 0.6539   | 0.3822     - 1.5204\n",
      "\u001b[1;4mValidati 65     - 23     -          0.4226| 0.2114  | 0.6340 -           0.8988    | 0.9679      | 0.6139   | 0.3174     - 0.1429\u001b[0m\n",
      "Training 66     - 101    -          0.1741| 0.0640  | 0.2380 -           0.9009    | 0.9658      | 0.6548   | 0.3799     - 1.5277\n",
      "\u001b[1;4mValidati 66     - 23     -          0.4311| 0.2149  | 0.6460 -           0.8959    | 0.9673      | 0.5958   | 0.3166     - 0.1415\u001b[0m\n",
      "Training 67     - 101    -          0.2666| 0.0838  | 0.3504 -           0.9005    | 0.9657      | 0.6495   | 0.3766     - 1.5355\n",
      "\u001b[1;4mValidati 67     - 23     -          0.4156| 0.2124  | 0.6280 -           0.9008    | 0.9687      | 0.6094   | 0.3005     - 0.1399\u001b[0m\n",
      "Training 68     - 101    -          0.2241| 0.0888  | 0.3128 -           0.8994    | 0.9656      | 0.6466   | 0.3761     - 1.5173\n",
      "\u001b[1;4mValidati 68     - 23     -          0.4318| 0.2149  | 0.6466 -           0.8904    | 0.9669      | 0.5821   | 0.2676     - 0.1407\u001b[0m\n",
      "Training 69     - 101    -          0.3032| 0.0878  | 0.3910 -           0.9033    | 0.9651      | 0.6630   | 0.3787     - 1.5191\n",
      "\u001b[1;4mValidati 69     - 23     -          0.4295| 0.2164  | 0.6459 -           0.9010    | 0.9674      | 0.6180   | 0.3164     - 0.1408\u001b[0m\n",
      "Training 70     - 101    -          0.2318| 0.0789  | 0.3107 -           0.8986    | 0.9658      | 0.6427   | 0.3713     - 1.5170\n",
      "\u001b[1;4mValidati 70     - 23     -          0.4389| 0.2161  | 0.6549 -           0.8992    | 0.9682      | 0.6089   | 0.3048     - 0.1411\u001b[0m\n",
      "Training 71     - 101    -          0.2595| 0.0944  | 0.3539 -           0.9005    | 0.9659      | 0.6499   | 0.3751     - 1.5249\n",
      "\u001b[1;4mValidati 71     - 23     -          0.4286| 0.2162  | 0.6448 -           0.8982    | 0.9680      | 0.6073   | 0.2761     - 0.1401\u001b[0m\n",
      "Training 72     - 101    -          0.3277| 0.1153  | 0.4430 -           0.9013    | 0.9658      | 0.6541   | 0.3758     - 1.5266\n",
      "\u001b[1;4mValidati 72     - 23     -          0.4348| 0.2180  | 0.6528 -           0.9016    | 0.9679      | 0.6194   | 0.3257     - 0.1411\u001b[0m\n",
      "Training 73     - 101    -          0.2711| 0.0897  | 0.3608 -           0.9024    | 0.9662      | 0.6578   | 0.3826     - 1.5167\n",
      "\u001b[1;4mValidati 73     - 23     -          0.4226| 0.2168  | 0.6394 -           0.9019    | 0.9679      | 0.6191   | 0.2762     - 0.1460\u001b[0m\n",
      "Training 74     - 101    -          0.2656| 0.1029  | 0.3685 -           0.9040    | 0.9658      | 0.6627   | 0.3827     - 1.5284\n",
      "\u001b[1;4mValidati 74     - 23     -          0.4446| 0.2135  | 0.6582 -           0.8970    | 0.9679      | 0.6011   | 0.2850     - 0.1388\u001b[0m\n",
      "Training 75     - 101    -          0.1818| 0.1282  | 0.3100 -           0.9034    | 0.9659      | 0.6609   | 0.3862     - 1.5331\n",
      "\u001b[1;4mValidati 75     - 23     -          0.4489| 0.2116  | 0.6605 -           0.9015    | 0.9683      | 0.6129   | 0.2977     - 0.1413\u001b[0m\n",
      "Training 76     - 101    -          0.1891| 0.0817  | 0.2708 -           0.9032    | 0.9660      | 0.6599   | 0.3821     - 1.5464\n",
      "\u001b[1;4mValidati 76     - 23     -          0.4381| 0.2137  | 0.6518 -           0.9037    | 0.9689      | 0.6249   | 0.2959     - 0.1444\u001b[0m\n",
      "Training 77     - 101    -          0.2574| 0.0724  | 0.3298 -           0.9043    | 0.9660      | 0.6673   | 0.3795     - 1.5671\n",
      "\u001b[1;4mValidati 77     - 23     -          0.4560| 0.2180  | 0.6740 -           0.9056    | 0.9683      | 0.6435   | 0.3147     - 0.1449\u001b[0m\n",
      "Training 78     - 101    -          0.1863| 0.0842  | 0.2705 -           0.9041    | 0.9663      | 0.6643   | 0.3879     - 1.5662\n",
      "\u001b[1;4mValidati 78     - 23     -          0.4452| 0.2188  | 0.6640 -           0.8981    | 0.9684      | 0.5944   | 0.2902     - 0.1578\u001b[0m\n",
      "Training 79     - 101    -          0.2653| 0.1124  | 0.3777 -           0.9048    | 0.9661      | 0.6677   | 0.3878     - 1.5622\n",
      "\u001b[1;4mValidati 79     - 23     -          0.4550| 0.2186  | 0.6737 -           0.9015    | 0.9682      | 0.6129   | 0.2793     - 0.1433\u001b[0m\n",
      "Training 80     - 101    -          0.2410| 0.1208  | 0.3618 -           0.9010    | 0.9660      | 0.6499   | 0.3790     - 1.5307\n",
      "\u001b[1;4mValidati 80     - 23     -          0.4477| 0.2148  | 0.6625 -           0.9035    | 0.9683      | 0.6185   | 0.2792     - 0.1413\u001b[0m\n",
      "Training 81     - 101    -          0.2246| 0.1047  | 0.3294 -           0.9071    | 0.9658      | 0.6778   | 0.3895     - 1.5396\n",
      "\u001b[1;4mValidati 81     - 23     -          0.4437| 0.2177  | 0.6615 -           0.9034    | 0.9682      | 0.6350   | 0.3158     - 0.1408\u001b[0m\n",
      "Training 82     - 101    -          0.2264| 0.0618  | 0.2882 -           0.9028    | 0.9658      | 0.6604   | 0.3823     - 1.5251\n",
      "\u001b[1;4mValidati 82     - 23     -          0.4509| 0.2153  | 0.6662 -           0.9043    | 0.9683      | 0.6361   | 0.3139     - 0.1408\u001b[0m\n",
      "Training 83     - 101    -          0.2518| 0.0913  | 0.3430 -           0.9041    | 0.9661      | 0.6638   | 0.3847     - 1.5503\n",
      "\u001b[1;4mValidati 83     - 23     -          0.4366| 0.2159  | 0.6524 -           0.9058    | 0.9685      | 0.6371   | 0.3029     - 0.1420\u001b[0m\n",
      "Training 84     - 101    -          0.2558| 0.1157  | 0.3716 -           0.9037    | 0.9660      | 0.6643   | 0.3882     - 1.5359\n",
      "\u001b[1;4mValidati 84     - 23     -          0.4416| 0.2135  | 0.6551 -           0.9041    | 0.9687      | 0.6371   | 0.3120     - 0.1401\u001b[0m\n",
      "Training 85     - 101    -          0.2905| 0.1215  | 0.4120 -           0.9044    | 0.9660      | 0.6675   | 0.3868     - 1.5325\n",
      "\u001b[1;4mValidati 85     - 23     -          0.4371| 0.2151  | 0.6522 -           0.8997    | 0.9680      | 0.6027   | 0.3086     - 0.1386\u001b[0m\n",
      "Training 86     - 101    -          0.2834| 0.1034  | 0.3868 -           0.9041    | 0.9658      | 0.6672   | 0.3870     - 1.5533\n",
      "\u001b[1;4mValidati 86     - 23     -          0.4536| 0.2122  | 0.6659 -           0.9058    | 0.9685      | 0.6418   | 0.3055     - 0.1419\u001b[0m\n",
      "Training 87     - 101    -          0.2251| 0.0655  | 0.2906 -           0.9058    | 0.9660      | 0.6722   | 0.3852     - 1.5305\n",
      "\u001b[1;4mValidati 87     - 23     -          0.4377| 0.2128  | 0.6505 -           0.9031    | 0.9681      | 0.6305   | 0.3268     - 0.1446\u001b[0m\n",
      "Training 88     - 101    -          0.3923| 0.1008  | 0.4932 -           0.9061    | 0.9659      | 0.6737   | 0.3914     - 1.5306\n",
      "\u001b[1;4mValidati 88     - 23     -          0.4204| 0.2110  | 0.6314 -           0.9102    | 0.9688      | 0.6777   | 0.3196     - 0.1415\u001b[0m\n",
      "Training 89     - 101    -          0.2408| 0.1099  | 0.3507 -           0.9046    | 0.9661      | 0.6692   | 0.3900     - 1.5327\n",
      "\u001b[1;4mValidati 89     - 23     -          0.4323| 0.2105  | 0.6427 -           0.9016    | 0.9681      | 0.6165   | 0.3073     - 0.1450\u001b[0m\n",
      "Training 90     - 101    -          0.3464| 0.0821  | 0.4285 -           0.9054    | 0.9660      | 0.6713   | 0.3929     - 1.5555\n",
      "\u001b[1;4mValidati 90     - 23     -          0.4351| 0.2109  | 0.6460 -           0.9003    | 0.9680      | 0.6083   | 0.2755     - 0.1421\u001b[0m\n",
      "Training 91     - 101    -          0.1960| 0.0445  | 0.2405 -           0.9067    | 0.9663      | 0.6763   | 0.3971     - 1.5660\n",
      "\u001b[1;4mValidati 91     - 23     -          0.4398| 0.2109  | 0.6507 -           0.9080    | 0.9684      | 0.6485   | 0.3056     - 0.1538\u001b[0m\n",
      "Training 92     - 101    -          0.1942| 0.0491  | 0.2433 -           0.9052    | 0.9661      | 0.6688   | 0.3818     - 1.5680\n",
      "\u001b[1;4mValidati 92     - 23     -          0.4311| 0.2138  | 0.6449 -           0.9092    | 0.9683      | 0.6630   | 0.3290     - 0.1454\u001b[0m\n",
      "Training 93     - 101    -          0.2108| 0.0337  | 0.2446 -           0.9055    | 0.9658      | 0.6730   | 0.3908     - 1.5393\n",
      "\u001b[1;4mValidati 93     - 23     -          0.4396| 0.2169  | 0.6564 -           0.9056    | 0.9683      | 0.6349   | 0.2925     - 0.1418\u001b[0m\n",
      "Training 94     - 101    -          0.2538| 0.0922  | 0.3460 -           0.9053    | 0.9659      | 0.6688   | 0.3892     - 1.5468\n",
      "\u001b[1;4mValidati 94     - 23     -          0.4221| 0.2107  | 0.6328 -           0.9086    | 0.9681      | 0.6694   | 0.3223     - 0.1473\u001b[0m\n",
      "Training 95     - 101    -          0.2239| 0.0662  | 0.2901 -           0.9064    | 0.9664      | 0.6744   | 0.3963     - 1.5448\n",
      "\u001b[1;4mValidati 95     - 23     -          0.4418| 0.2160  | 0.6578 -           0.8876    | 0.9652      | 0.5687   | 0.3048     - 0.1407\u001b[0m\n",
      "Training 96     - 87     -          0.1838| 0.0845  | 0.2683 -           0.9079    | 0.9660      | 0.6826   | 0.3938     - 1.3279\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
