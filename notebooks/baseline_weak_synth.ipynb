{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53, # The model output localisation with a résolution of ~ 18ms --> 53 temporal bins\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak ans synthetic20 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:13<00:00, 558.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: synthetic20\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4251/4251 [00:08<00:00, 517.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_val_subset >>> Loading dataset: validation\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/validation\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "manager.add_subset(\"synthetic20\")\n",
    "manager.add_subset(\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset\n",
    "\n",
    "- We want both the weak and strong ground truth --> the *weak* and *strong* parameters to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, weak=True, strong=True, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, weak=True, strong=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4049, 1058)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "This model is the same than the weak baseline but have an extra output. <br />\n",
    "the loc_output is compose of a single convolution layer with nb_filters == nb_class. <br />\n",
    "Since their is some pooling layer, the *loc_ouput* have a precision of 53 bins (~= 18 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakStrongBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (weak_output): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       "  (strong_output): Sequential(\n",
       "    (0): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakStrongBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_weak_output.Flatten_0                  -         [1, 1696]       -   \n",
      "16_weak_output.Dropout_1                  -         [1, 1696]       -   \n",
      "17_weak_output.Linear_2          [1696, 10]           [1, 10]  16.97k   \n",
      "18_strong_output.Conv2d_0    [32, 10, 1, 1]    [1, 10, 1, 53]   330.0   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_weak_output.Flatten_0             -  \n",
      "16_weak_output.Dropout_1             -  \n",
      "17_weak_output.Linear_2         16.96k  \n",
      "18_strong_output.Conv2d_0       16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             36.308k\n",
      "Trainable params         36.308k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.625696M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom loss function\n",
    "\n",
    "Since not all file have strong truth, it is necessary to remove those files. <br />\n",
    "For that, the strong mask is computed. If the sum of the strong ground truth is equal to 0 then it is a fake one <br />\n",
    "This file strong loss must not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_synth_loss(logits_weak, logits_strong, y_weak, y_strong, reduce: str = \"mean\"):\n",
    "    assert reduce in [\"mean\", \"sum\"], \"support only \\\"mean\\\" and \\\"sum\\\"\"\n",
    "    \n",
    "    #  Reduction function\n",
    "    if reduce == \"mean\":\n",
    "        reduce_fn = torch.mean\n",
    "    elif reduce == \"sum\":\n",
    "        reduce_fn = torch.sum\n",
    "    \n",
    "    # based on Binary Cross Entropy loss\n",
    "    weak_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    strong_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    # calc separate loss function\n",
    "    weak_bce = weak_criterion(logits_weak, y_weak)\n",
    "    strong_bce = strong_criterion(logits_strong, y_strong)\n",
    "    \n",
    "    weak_bce = reduce_fn(weak_bce, dim=1)\n",
    "    strong_bce = reduce_fn(strong_bce, dim=(1, 2))\n",
    "    \n",
    "    # calc strong mask\n",
    "    strong_mask = torch.clamp(torch.sum(y_strong, dim=(1, 2)), 0, 1) # vector of 0 or 1\n",
    "    strong_mask = strong_mask.detach() # declared not to need gradients\n",
    "    \n",
    "    # Output the different loss for logging purpose\n",
    "    weak_loss = reduce_fn(weak_bce)\n",
    "    strong_loss = reduce_fn(strong_mask * strong_bce)\n",
    "    total_loss = reduce_fn(weak_bce + strong_mask * strong_bce)\n",
    "    \n",
    "    return weak_loss, strong_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "weak_binacc_func = BinaryAccuracy()\n",
    "strong_binacc_func = BinaryAccuracy()\n",
    "weak_f_func = FScore()\n",
    "strong_f_func = FScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_metrics():\n",
    "    metrics = [weak_binacc_func, strong_binacc_func, weak_f_func, strong_f_func]\n",
    "    \n",
    "    for m in metrics:\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6}| {:<8.8}| {:<6.6} - {:<9.9} {:<10.10}| {:<12.12}| {:<9.9}| {:<11.11}- {:<6.6}\"\n",
    "\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f}| {:<8.4f}| {:<6.4f} - {:<9.9} {:<10.4f}| {:<12.4f}| {:<9.4f}| {:<11.4f}- {:<6.4f}\"\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"Strong \", \"Total \", \"metrics: \", \"Weak acc \", \"Strong acc \", \"Weak F1 \", \"Strong F1\", \"Time\"\n",
    ")\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X, y) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X = X.cuda().float()\n",
    "        y_weak = y[0].cuda().float()\n",
    "        y_strong = y[1].cuda().float()\n",
    "        \n",
    "        weak_logits, strong_logits = model(X)\n",
    "        \n",
    "        # calc the loss\n",
    "        weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "            weak_logits, strong_logits,\n",
    "            y_weak, y_strong,\n",
    "            reduce=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "\n",
    "\n",
    "            # logs\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"train/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"train/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"train/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"train/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"train/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "        \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y_weak = y[0].cuda().float()\n",
    "            y_strong = y[1].cuda().float()\n",
    "\n",
    "            weak_logits, strong_logits = model(X)\n",
    "\n",
    "            # calc the loss\n",
    "            weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "                weak_logits, strong_logits,\n",
    "                y_weak, y_strong,\n",
    "                reduce=\"mean\"\n",
    "            )\n",
    "            \n",
    "             # calc metrics\n",
    "            weak_pred = torch.sigmoid(weak_logits)\n",
    "            strong_pred = torch.sigmoid(strong_logits)\n",
    "\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(weak_pred, y_weak)\n",
    "            weak_fscore = weak_f_func(weak_pred, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(strong_pred, y_strong)\n",
    "            strong_fscore = strong_f_func(strong_pred, y_strong)\n",
    "\n",
    "            # logs\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"val/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n",
      "\n",
      "Training 1      - 100    -          0.4003| 0.1050  | 0.5054 -           0.8207    | 0.9152      | 0.2971   | 0.0202     - 52.9765\n",
      "\u001b[1;4mValidati 1      - 26     -          0.2930| 0.2575  | 0.5505 -           0.8545    | 0.9185      | 0.2466   | 0.0000     - 13.4928\u001b[0m\n",
      "Training 2      - 100    -          0.4832| 0.1367  | 0.6199 -           0.8306    | 0.9655      | 0.3361   | 0.0023     - 1.9678\n",
      "\u001b[1;4mValidati 2      - 26     -          0.2459| 0.2523  | 0.4981 -           0.8708    | 0.9185      | 0.3607   | 0.0011     - 0.2105\u001b[0m\n",
      "Training 3      - 100    -          0.4249| 0.1602  | 0.5851 -           0.8335    | 0.9657      | 0.3345   | 0.0385     - 1.9455\n",
      "\u001b[1;4mValidati 3      - 26     -          0.2938| 0.2741  | 0.5679 -           0.8699    | 0.9207      | 0.3271   | 0.0541     - 0.2155\u001b[0m\n",
      "Training 4      - 100    -          0.3776| 0.1321  | 0.5096 -           0.8405    | 0.9661      | 0.3658   | 0.1086     - 1.9134\n",
      "\u001b[1;4mValidati 4      - 26     -          0.2954| 0.2579  | 0.5533 -           0.8776    | 0.9241      | 0.4412   | 0.1335     - 0.2381\u001b[0m\n",
      "Training 5      - 100    -          0.3441| 0.0996  | 0.4437 -           0.8451    | 0.9661      | 0.3769   | 0.1364     - 1.8953\n",
      "\u001b[1;4mValidati 5      - 26     -          0.3204| 0.2727  | 0.5931 -           0.8632    | 0.9275      | 0.4686   | 0.2204     - 0.2174\u001b[0m\n",
      "Training 6      - 100    -          0.3221| 0.1255  | 0.4475 -           0.8496    | 0.9662      | 0.3961   | 0.1613     - 1.9023\n",
      "\u001b[1;4mValidati 6      - 26     -          0.2965| 0.2553  | 0.5518 -           0.8835    | 0.9253      | 0.4254   | 0.1629     - 0.2160\u001b[0m\n",
      "Training 7      - 100    -          0.3553| 0.1067  | 0.4620 -           0.8562    | 0.9664      | 0.4243   | 0.1860     - 1.9011\n",
      "\u001b[1;4mValidati 7      - 26     -          0.3327| 0.2893  | 0.6219 -           0.8857    | 0.9281      | 0.4440   | 0.2251     - 0.2134\u001b[0m\n",
      "Training 8      - 100    -          0.3451| 0.0620  | 0.4071 -           0.8576    | 0.9661      | 0.4279   | 0.1882     - 1.9141\n",
      "\u001b[1;4mValidati 8      - 26     -          0.2929| 0.2512  | 0.5441 -           0.8874    | 0.9304      | 0.5091   | 0.2694     - 0.2147\u001b[0m\n",
      "Training 9      - 100    -          0.3660| 0.0971  | 0.4631 -           0.8603    | 0.9661      | 0.4438   | 0.2002     - 1.9012\n",
      "\u001b[1;4mValidati 9      - 26     -          0.3049| 0.2180  | 0.5229 -           0.8887    | 0.9287      | 0.4578   | 0.2260     - 0.2180\u001b[0m\n",
      "Training 10     - 100    -          0.3013| 0.0589  | 0.3603 -           0.8615    | 0.9664      | 0.4430   | 0.2175     - 1.8948\n",
      "\u001b[1;4mValidati 10     - 26     -          0.2983| 0.2292  | 0.5276 -           0.8826    | 0.9268      | 0.4368   | 0.2086     - 0.2179\u001b[0m\n",
      "Training 11     - 100    -          0.3903| 0.1122  | 0.5025 -           0.8633    | 0.9663      | 0.4585   | 0.2295     - 1.9021\n",
      "\u001b[1;4mValidati 11     - 26     -          0.3234| 0.2608  | 0.5842 -           0.8874    | 0.9297      | 0.4520   | 0.2548     - 0.2172\u001b[0m\n",
      "Training 12     - 100    -          0.3967| 0.0922  | 0.4889 -           0.8665    | 0.9666      | 0.4693   | 0.2432     - 1.9079\n",
      "\u001b[1;4mValidati 12     - 26     -          0.3420| 0.2293  | 0.5713 -           0.8845    | 0.9299      | 0.4951   | 0.2599     - 0.2135\u001b[0m\n",
      "Training 13     - 100    -          0.3256| 0.0964  | 0.4219 -           0.8674    | 0.9668      | 0.4761   | 0.2605     - 1.9016\n",
      "\u001b[1;4mValidati 13     - 26     -          0.3027| 0.2122  | 0.5148 -           0.8870    | 0.9283      | 0.4448   | 0.2310     - 0.2176\u001b[0m\n",
      "Training 14     - 100    -          0.4703| 0.1295  | 0.5998 -           0.8709    | 0.9667      | 0.4957   | 0.2760     - 1.8972\n",
      "\u001b[1;4mValidati 14     - 26     -          0.3278| 0.2188  | 0.5466 -           0.8916    | 0.9292      | 0.4863   | 0.2500     - 0.2173\u001b[0m\n",
      "Training 15     - 100    -          0.2530| 0.0853  | 0.3383 -           0.8696    | 0.9671      | 0.4930   | 0.2802     - 1.9180\n",
      "\u001b[1;4mValidati 15     - 26     -          0.3643| 0.2124  | 0.5766 -           0.8857    | 0.9273      | 0.4821   | 0.2166     - 0.2162\u001b[0m\n",
      "Training 16     - 100    -          0.3340| 0.0937  | 0.4277 -           0.8720    | 0.9669      | 0.5054   | 0.2895     - 1.9149\n",
      "\u001b[1;4mValidati 16     - 26     -          0.3513| 0.2107  | 0.5621 -           0.8931    | 0.9313      | 0.5108   | 0.2900     - 0.2181\u001b[0m\n",
      "Training 17     - 100    -          0.3372| 0.1054  | 0.4426 -           0.8727    | 0.9670      | 0.5099   | 0.2971     - 1.9183\n",
      "\u001b[1;4mValidati 17     - 26     -          0.3396| 0.2150  | 0.5546 -           0.8672    | 0.9240      | 0.4560   | 0.2701     - 0.2197\u001b[0m\n",
      "Training 18     - 100    -          0.3384| 0.1017  | 0.4401 -           0.8749    | 0.9668      | 0.5164   | 0.2918     - 1.9090\n",
      "\u001b[1;4mValidati 18     - 26     -          0.2924| 0.2104  | 0.5028 -           0.8937    | 0.9312      | 0.5113   | 0.2841     - 0.2172\u001b[0m\n",
      "Training 19     - 100    -          0.3382| 0.0716  | 0.4098 -           0.8750    | 0.9666      | 0.5212   | 0.3029     - 1.9197\n",
      "\u001b[1;4mValidati 19     - 26     -          0.3470| 0.2104  | 0.5574 -           0.8983    | 0.9326      | 0.5371   | 0.3105     - 0.2160\u001b[0m\n",
      "Training 20     - 100    -          0.3768| 0.0874  | 0.4641 -           0.8774    | 0.9670      | 0.5296   | 0.3083     - 1.9058\n",
      "\u001b[1;4mValidati 20     - 26     -          0.3499| 0.2121  | 0.5620 -           0.8926    | 0.9316      | 0.5275   | 0.2960     - 0.2178\u001b[0m\n",
      "Training 21     - 100    -          0.3620| 0.1059  | 0.4679 -           0.8784    | 0.9670      | 0.5373   | 0.3136     - 1.9273\n",
      "\u001b[1;4mValidati 21     - 26     -          0.3305| 0.2104  | 0.5410 -           0.8985    | 0.9329      | 0.5417   | 0.3193     - 0.2164\u001b[0m\n",
      "Training 22     - 100    -          0.3206| 0.0832  | 0.4038 -           0.8812    | 0.9671      | 0.5497   | 0.3314     - 1.9056\n",
      "\u001b[1;4mValidati 22     - 26     -          0.2953| 0.2064  | 0.5016 -           0.8950    | 0.9327      | 0.5430   | 0.3122     - 0.2087\u001b[0m\n",
      "Training 23     - 100    -          0.2965| 0.0649  | 0.3615 -           0.8837    | 0.9667      | 0.5613   | 0.3308     - 1.9062\n",
      "\u001b[1;4mValidati 23     - 26     -          0.3441| 0.2109  | 0.5551 -           0.8922    | 0.9310      | 0.5186   | 0.2826     - 0.2177\u001b[0m\n",
      "Training 24     - 100    -          0.2358| 0.0493  | 0.2851 -           0.8842    | 0.9671      | 0.5649   | 0.3377     - 1.9222\n",
      "\u001b[1;4mValidati 24     - 26     -          0.3443| 0.2104  | 0.5547 -           0.8948    | 0.9301      | 0.5027   | 0.2614     - 0.2147\u001b[0m\n",
      "Training 25     - 100    -          0.2726| 0.1166  | 0.3892 -           0.8846    | 0.9671      | 0.5682   | 0.3371     - 1.9973\n",
      "\u001b[1;4mValidati 25     - 26     -          0.3327| 0.2075  | 0.5402 -           0.8951    | 0.9302      | 0.5166   | 0.2662     - 0.2202\u001b[0m\n",
      "Training 26     - 100    -          0.3149| 0.0756  | 0.3906 -           0.8835    | 0.9671      | 0.5600   | 0.3386     - 1.9695\n",
      "\u001b[1;4mValidati 26     - 26     -          0.3326| 0.2093  | 0.5420 -           0.8969    | 0.9320      | 0.5446   | 0.3055     - 0.2385\u001b[0m\n",
      "Training 27     - 100    -          0.3192| 0.1210  | 0.4402 -           0.8867    | 0.9667      | 0.5727   | 0.3353     - 1.9191\n",
      "\u001b[1;4mValidati 27     - 26     -          0.3464| 0.2092  | 0.5556 -           0.8941    | 0.9311      | 0.5239   | 0.2856     - 0.2159\u001b[0m\n",
      "Training 28     - 100    -          0.3122| 0.1053  | 0.4176 -           0.8856    | 0.9675      | 0.5678   | 0.3537     - 1.9408\n",
      "\u001b[1;4mValidati 28     - 26     -          0.3271| 0.2134  | 0.5405 -           0.8976    | 0.9313      | 0.5411   | 0.3058     - 0.2202\u001b[0m\n",
      "Training 29     - 100    -          0.2641| 0.0724  | 0.3365 -           0.8867    | 0.9672      | 0.5769   | 0.3563     - 1.9028\n",
      "\u001b[1;4mValidati 29     - 26     -          0.3363| 0.2067  | 0.5430 -           0.8994    | 0.9326      | 0.5422   | 0.3275     - 0.2154\u001b[0m\n",
      "Training 30     - 100    -          0.2487| 0.0513  | 0.3001 -           0.8894    | 0.9671      | 0.5866   | 0.3571     - 1.9016\n",
      "\u001b[1;4mValidati 30     - 26     -          0.3176| 0.2116  | 0.5292 -           0.8895    | 0.9309      | 0.5073   | 0.3128     - 0.2155\u001b[0m\n",
      "Training 31     - 100    -          0.2161| 0.1148  | 0.3309 -           0.8882    | 0.9670      | 0.5828   | 0.3609     - 1.9202\n",
      "\u001b[1;4mValidati 31     - 26     -          0.3556| 0.2060  | 0.5615 -           0.8996    | 0.9320      | 0.5465   | 0.3050     - 0.2113\u001b[0m\n",
      "Training 32     - 100    -          0.2674| 0.0674  | 0.3347 -           0.8895    | 0.9667      | 0.5888   | 0.3554     - 1.9009\n",
      "\u001b[1;4mValidati 32     - 26     -          0.3325| 0.2149  | 0.5474 -           0.8994    | 0.9325      | 0.5452   | 0.3127     - 0.2186\u001b[0m\n",
      "Training 33     - 100    -          0.2401| 0.0655  | 0.3057 -           0.8898    | 0.9671      | 0.5922   | 0.3589     - 1.9151\n",
      "\u001b[1;4mValidati 33     - 26     -          0.3119| 0.2125  | 0.5244 -           0.8951    | 0.9317      | 0.5215   | 0.2989     - 0.2155\u001b[0m\n",
      "Training 34     - 100    -          0.2875| 0.0487  | 0.3361 -           0.8895    | 0.9677      | 0.5870   | 0.3713     - 1.9553\n",
      "\u001b[1;4mValidati 34     - 26     -          0.3537| 0.2109  | 0.5646 -           0.9012    | 0.9326      | 0.5517   | 0.3103     - 0.2166\u001b[0m\n",
      "Training 35     - 100    -          0.3510| 0.1025  | 0.4535 -           0.8911    | 0.9670      | 0.5972   | 0.3672     - 1.9539\n",
      "\u001b[1;4mValidati 35     - 26     -          0.3436| 0.2127  | 0.5563 -           0.8968    | 0.9326      | 0.5443   | 0.3170     - 0.2381\u001b[0m\n",
      "Training 36     - 100    -          0.2590| 0.0744  | 0.3334 -           0.8909    | 0.9669      | 0.5967   | 0.3646     - 1.9166\n",
      "\u001b[1;4mValidati 36     - 26     -          0.3305| 0.2032  | 0.5337 -           0.8902    | 0.9313      | 0.5235   | 0.2977     - 0.2149\u001b[0m\n",
      "Training 37     - 100    -          0.3147| 0.0805  | 0.3952 -           0.8918    | 0.9670      | 0.5988   | 0.3681     - 1.9172\n",
      "\u001b[1;4mValidati 37     - 26     -          0.3299| 0.2070  | 0.5369 -           0.8994    | 0.9316      | 0.5350   | 0.2983     - 0.2164\u001b[0m\n",
      "Training 38     - 100    -          0.2522| 0.0733  | 0.3255 -           0.8946    | 0.9672      | 0.6102   | 0.3701     - 1.9292\n",
      "\u001b[1;4mValidati 38     - 26     -          0.3279| 0.2046  | 0.5325 -           0.8997    | 0.9327      | 0.5477   | 0.3211     - 0.2174\u001b[0m\n",
      "Training 39     - 100    -          0.2905| 0.1046  | 0.3951 -           0.8934    | 0.9671      | 0.6056   | 0.3695     - 1.8993\n",
      "\u001b[1;4mValidati 39     - 26     -          0.3285| 0.2080  | 0.5365 -           0.9002    | 0.9333      | 0.5504   | 0.3311     - 0.2173\u001b[0m\n",
      "Training 40     - 100    -          0.2488| 0.0995  | 0.3483 -           0.8949    | 0.9668      | 0.6149   | 0.3757     - 1.9324\n",
      "\u001b[1;4mValidati 40     - 26     -          0.3236| 0.2071  | 0.5307 -           0.8977    | 0.9304      | 0.5288   | 0.2824     - 0.2064\u001b[0m\n",
      "Training 41     - 100    -          0.2401| 0.0943  | 0.3344 -           0.8946    | 0.9666      | 0.6115   | 0.3741     - 1.9026\n",
      "\u001b[1;4mValidati 41     - 26     -          0.3162| 0.2029  | 0.5191 -           0.8973    | 0.9324      | 0.5447   | 0.3390     - 0.2127\u001b[0m\n",
      "Training 42     - 100    -          0.2813| 0.0980  | 0.3793 -           0.8943    | 0.9664      | 0.6128   | 0.3725     - 1.8994\n",
      "\u001b[1;4mValidati 42     - 26     -          0.3204| 0.2024  | 0.5228 -           0.8998    | 0.9324      | 0.5488   | 0.3218     - 0.2178\u001b[0m\n",
      "Training 43     - 100    -          0.3171| 0.0630  | 0.3802 -           0.8956    | 0.9664      | 0.6177   | 0.3695     - 1.9225\n",
      "\u001b[1;4mValidati 43     - 26     -          0.3281| 0.2183  | 0.5464 -           0.9032    | 0.9339      | 0.5706   | 0.3437     - 0.2195\u001b[0m\n",
      "Training 44     - 100    -          0.3239| 0.0768  | 0.4007 -           0.8959    | 0.9670      | 0.6161   | 0.3790     - 1.9650\n",
      "\u001b[1;4mValidati 44     - 26     -          0.3147| 0.2046  | 0.5193 -           0.8977    | 0.9329      | 0.5461   | 0.3273     - 0.2393\u001b[0m\n",
      "Training 45     - 100    -          0.3751| 0.0673  | 0.4424 -           0.8945    | 0.9667      | 0.6123   | 0.3758     - 1.9512\n",
      "\u001b[1;4mValidati 45     - 26     -          0.3280| 0.2031  | 0.5311 -           0.8972    | 0.9307      | 0.5155   | 0.2827     - 0.2160\u001b[0m\n",
      "Training 46     - 100    -          0.3116| 0.0746  | 0.3862 -           0.8960    | 0.9666      | 0.6200   | 0.3800     - 1.9165\n",
      "\u001b[1;4mValidati 46     - 26     -          0.3241| 0.2045  | 0.5287 -           0.9011    | 0.9327      | 0.5517   | 0.3252     - 0.2155\u001b[0m\n",
      "Training 47     - 100    -          0.2707| 0.0691  | 0.3398 -           0.8982    | 0.9671      | 0.6292   | 0.3869     - 1.9249\n",
      "\u001b[1;4mValidati 47     - 26     -          0.3262| 0.2089  | 0.5352 -           0.9001    | 0.9331      | 0.5487   | 0.3289     - 0.2124\u001b[0m\n",
      "Training 48     - 100    -          0.2873| 0.0765  | 0.3638 -           0.8996    | 0.9664      | 0.6360   | 0.3797     - 1.8992\n",
      "\u001b[1;4mValidati 48     - 26     -          0.3111| 0.2008  | 0.5119 -           0.8994    | 0.9321      | 0.5400   | 0.3116     - 0.2152\u001b[0m\n",
      "Training 49     - 100    -          0.2353| 0.0925  | 0.3277 -           0.8971    | 0.9668      | 0.6259   | 0.3887     - 1.9021\n",
      "\u001b[1;4mValidati 49     - 26     -          0.3112| 0.2024  | 0.5136 -           0.9006    | 0.9318      | 0.5447   | 0.3069     - 0.2242\u001b[0m\n",
      "Training 50     - 100    -          0.2531| 0.0792  | 0.3323 -           0.8978    | 0.9666      | 0.6283   | 0.3806     - 1.9122\n",
      "\u001b[1;4mValidati 50     - 26     -          0.3172| 0.2002  | 0.5174 -           0.8929    | 0.9300      | 0.5343   | 0.3252     - 0.2085\u001b[0m\n",
      "Training 51     - 100    -          0.2986| 0.0819  | 0.3805 -           0.9006    | 0.9665      | 0.6384   | 0.3867     - 1.9195\n",
      "\u001b[1;4mValidati 51     - 26     -          0.3087| 0.1991  | 0.5078 -           0.9050    | 0.9343      | 0.5767   | 0.3519     - 0.2170\u001b[0m\n",
      "Training 52     - 100    -          0.2187| 0.0595  | 0.2782 -           0.8976    | 0.9666      | 0.6277   | 0.3855     - 1.9058\n",
      "\u001b[1;4mValidati 52     - 26     -          0.3115| 0.2039  | 0.5153 -           0.8995    | 0.9310      | 0.5335   | 0.2871     - 0.2185\u001b[0m\n",
      "Training 53     - 100    -          0.3595| 0.0646  | 0.4241 -           0.8987    | 0.9670      | 0.6279   | 0.3896     - 1.9127\n",
      "\u001b[1;4mValidati 53     - 26     -          0.3136| 0.2076  | 0.5212 -           0.8993    | 0.9329      | 0.5465   | 0.3293     - 0.2151\u001b[0m\n",
      "Training 54     - 100    -          0.2552| 0.1009  | 0.3561 -           0.9025    | 0.9666      | 0.6490   | 0.3917     - 1.9493\n",
      "\u001b[1;4mValidati 54     - 26     -          0.3055| 0.1993  | 0.5048 -           0.9011    | 0.9341      | 0.5599   | 0.3572     - 0.2170\u001b[0m\n",
      "Training 55     - 100    -          0.2684| 0.0955  | 0.3639 -           0.8999    | 0.9664      | 0.6374   | 0.3859     - 1.9335\n",
      "\u001b[1;4mValidati 55     - 26     -          0.3056| 0.1991  | 0.5047 -           0.9012    | 0.9335      | 0.5614   | 0.3470     - 0.2445\u001b[0m\n",
      "Training 56     - 100    -          0.2156| 0.0733  | 0.2889 -           0.9015    | 0.9664      | 0.6434   | 0.3885     - 1.9056\n",
      "\u001b[1;4mValidati 56     - 26     -          0.3115| 0.2084  | 0.5198 -           0.9030    | 0.9345      | 0.5691   | 0.3715     - 0.2284\u001b[0m\n",
      "Training 57     - 100    -          0.3106| 0.1390  | 0.4496 -           0.9012    | 0.9665      | 0.6433   | 0.3914     - 1.9175\n",
      "\u001b[1;4mValidati 57     - 26     -          0.3122| 0.2046  | 0.5168 -           0.9009    | 0.9330      | 0.5457   | 0.3258     - 0.2160\u001b[0m\n",
      "Training 58     - 100    -          0.1830| 0.0820  | 0.2650 -           0.9014    | 0.9668      | 0.6410   | 0.3887     - 1.9119\n",
      "\u001b[1;4mValidati 58     - 26     -          0.2954| 0.2011  | 0.4966 -           0.9005    | 0.9325      | 0.5452   | 0.3197     - 0.2214\u001b[0m\n",
      "Training 59     - 100    -          0.2125| 0.0896  | 0.3022 -           0.9006    | 0.9669      | 0.6385   | 0.3985     - 1.9092\n",
      "\u001b[1;4mValidati 59     - 26     -          0.3023| 0.2011  | 0.5034 -           0.9046    | 0.9341      | 0.5637   | 0.3419     - 0.2173\u001b[0m\n",
      "Training 60     - 100    -          0.2080| 0.0999  | 0.3079 -           0.9017    | 0.9662      | 0.6436   | 0.3912     - 1.9325\n",
      "\u001b[1;4mValidati 60     - 26     -          0.2970| 0.1991  | 0.4962 -           0.8916    | 0.9290      | 0.5251   | 0.3291     - 0.2175\u001b[0m\n",
      "Training 61     - 100    -          0.2475| 0.0821  | 0.3296 -           0.9026    | 0.9665      | 0.6485   | 0.3985     - 1.9145\n",
      "\u001b[1;4mValidati 61     - 26     -          0.3008| 0.2013  | 0.5021 -           0.9011    | 0.9313      | 0.5393   | 0.2941     - 0.2090\u001b[0m\n",
      "Training 62     - 100    -          0.2233| 0.0751  | 0.2984 -           0.9013    | 0.9664      | 0.6429   | 0.3939     - 1.9018\n",
      "\u001b[1;4mValidati 62     - 26     -          0.2984| 0.1991  | 0.4974 -           0.9013    | 0.9329      | 0.5505   | 0.3280     - 0.2182\u001b[0m\n",
      "Training 63     - 100    -          0.2597| 0.0943  | 0.3540 -           0.9041    | 0.9664      | 0.6572   | 0.3948     - 1.9031\n",
      "\u001b[1;4mValidati 63     - 26     -          0.2994| 0.1996  | 0.4990 -           0.9003    | 0.9323      | 0.5427   | 0.3211     - 0.2189\u001b[0m\n",
      "Training 64     - 100    -          0.2358| 0.0802  | 0.3160 -           0.9038    | 0.9664      | 0.6571   | 0.3979     - 1.9489\n",
      "\u001b[1;4mValidati 64     - 26     -          0.2968| 0.1999  | 0.4968 -           0.9043    | 0.9334      | 0.5620   | 0.3352     - 0.2143\u001b[0m\n",
      "Training 65     - 100    -          0.1994| 0.0469  | 0.2463 -           0.9040    | 0.9667      | 0.6535   | 0.3995     - 1.9642\n",
      "\u001b[1;4mValidati 65     - 26     -          0.2980| 0.2006  | 0.4985 -           0.9018    | 0.9336      | 0.5630   | 0.3575     - 0.2356\u001b[0m\n",
      "Training 66     - 100    -          0.2155| 0.0906  | 0.3061 -           0.9042    | 0.9664      | 0.6542   | 0.3939     - 1.9150\n",
      "\u001b[1;4mValidati 66     - 26     -          0.3053| 0.2045  | 0.5099 -           0.9079    | 0.9348      | 0.5891   | 0.3651     - 0.2138\u001b[0m\n",
      "Training 67     - 100    -          0.3326| 0.0827  | 0.4152 -           0.9054    | 0.9665      | 0.6608   | 0.4022     - 1.9183\n",
      "\u001b[1;4mValidati 67     - 26     -          0.2929| 0.2006  | 0.4934 -           0.9030    | 0.9338      | 0.5614   | 0.3489     - 0.2168\u001b[0m\n",
      "Training 68     - 100    -          0.3180| 0.0846  | 0.4026 -           0.9039    | 0.9668      | 0.6568   | 0.4087     - 1.9166\n",
      "\u001b[1;4mValidati 68     - 26     -          0.2992| 0.2010  | 0.5002 -           0.9013    | 0.9346      | 0.5707   | 0.3670     - 0.2156\u001b[0m\n",
      "Training 69     - 100    -          0.2647| 0.0912  | 0.3559 -           0.9025    | 0.9666      | 0.6493   | 0.3929     - 1.9251\n",
      "\u001b[1;4mValidati 69     - 26     -          0.2992| 0.2000  | 0.4992 -           0.9040    | 0.9322      | 0.5496   | 0.3095     - 0.2169\u001b[0m\n",
      "Training 70     - 100    -          0.1826| 0.0453  | 0.2279 -           0.9032    | 0.9666      | 0.6514   | 0.4012     - 1.9447\n",
      "\u001b[1;4mValidati 70     - 26     -          0.2991| 0.1997  | 0.4988 -           0.9066    | 0.9347      | 0.5748   | 0.3579     - 0.2204\u001b[0m\n",
      "Training 71     - 100    -          0.2307| 0.0860  | 0.3167 -           0.9040    | 0.9663      | 0.6555   | 0.3954     - 1.9134\n",
      "\u001b[1;4mValidati 71     - 26     -          0.2954| 0.1999  | 0.4954 -           0.9042    | 0.9342      | 0.5668   | 0.3537     - 0.2114\u001b[0m\n",
      "Training 72     - 100    -          0.2445| 0.0806  | 0.3251 -           0.9049    | 0.9665      | 0.6594   | 0.4027     - 1.9043\n",
      "\u001b[1;4mValidati 72     - 26     -          0.2955| 0.2007  | 0.4962 -           0.9038    | 0.9334      | 0.5713   | 0.3419     - 0.2150\u001b[0m\n",
      "Training 73     - 100    -          0.2445| 0.0580  | 0.3025 -           0.9070    | 0.9667      | 0.6668   | 0.4012     - 1.9134\n",
      "\u001b[1;4mValidati 73     - 26     -          0.2925| 0.2001  | 0.4926 -           0.9017    | 0.9340      | 0.5572   | 0.3442     - 0.2173\u001b[0m\n",
      "Training 74     - 100    -          0.3362| 0.1440  | 0.4802 -           0.9053    | 0.9663      | 0.6618   | 0.4019     - 1.9149\n",
      "\u001b[1;4mValidati 74     - 26     -          0.3015| 0.2019  | 0.5035 -           0.8953    | 0.9310      | 0.5446   | 0.3535     - 0.2155\u001b[0m\n",
      "Training 75     - 100    -          0.1933| 0.1002  | 0.2936 -           0.9059    | 0.9665      | 0.6629   | 0.4056     - 1.9511\n",
      "\u001b[1;4mValidati 75     - 26     -          0.3016| 0.2015  | 0.5031 -           0.8995    | 0.9321      | 0.5381   | 0.3243     - 0.2178\u001b[0m\n",
      "Training 76     - 100    -          0.3087| 0.0897  | 0.3984 -           0.9066    | 0.9661      | 0.6664   | 0.3943     - 1.9354\n",
      "\u001b[1;4mValidati 76     - 26     -          0.3048| 0.2061  | 0.5109 -           0.9076    | 0.9339      | 0.5866   | 0.3438     - 0.2329\u001b[0m\n",
      "Training 77     - 100    -          0.2639| 0.0752  | 0.3392 -           0.9066    | 0.9661      | 0.6664   | 0.3925     - 1.9161\n",
      "\u001b[1;4mValidati 77     - 26     -          0.3011| 0.2023  | 0.5034 -           0.9042    | 0.9328      | 0.5630   | 0.3224     - 0.2360\u001b[0m\n",
      "Training 78     - 100    -          0.2299| 0.0651  | 0.2949 -           0.9081    | 0.9662      | 0.6704   | 0.3984     - 1.9153\n",
      "\u001b[1;4mValidati 78     - 26     -          0.3001| 0.2015  | 0.5016 -           0.9051    | 0.9342      | 0.5846   | 0.3617     - 0.2129\u001b[0m\n",
      "Training 79     - 100    -          0.2990| 0.0938  | 0.3928 -           0.9071    | 0.9664      | 0.6709   | 0.4076     - 1.9248\n",
      "\u001b[1;4mValidati 79     - 26     -          0.3034| 0.2037  | 0.5071 -           0.9055    | 0.9343      | 0.5778   | 0.3605     - 0.2154\u001b[0m\n",
      "Training 80     - 100    -          0.2394| 0.0818  | 0.3212 -           0.9081    | 0.9667      | 0.6715   | 0.4017     - 1.9070\n",
      "\u001b[1;4mValidati 80     - 26     -          0.2965| 0.2024  | 0.4989 -           0.9021    | 0.9331      | 0.5529   | 0.3280     - 0.2133\u001b[0m\n",
      "Training 81     - 100    -          0.2544| 0.0904  | 0.3448 -           0.9085    | 0.9667      | 0.6743   | 0.4045     - 1.9101\n",
      "\u001b[1;4mValidati 81     - 26     -          0.3066| 0.2042  | 0.5109 -           0.9006    | 0.9321      | 0.5375   | 0.3203     - 0.2138\u001b[0m\n",
      "Training 82     - 100    -          0.1963| 0.0727  | 0.2690 -           0.9065    | 0.9665      | 0.6644   | 0.4020     - 1.9046\n",
      "\u001b[1;4mValidati 82     - 26     -          0.2993| 0.2016  | 0.5009 -           0.9073    | 0.9339      | 0.5863   | 0.3493     - 0.2135\u001b[0m\n",
      "Training 83     - 100    -          0.2969| 0.0928  | 0.3897 -           0.9065    | 0.9666      | 0.6667   | 0.4067     - 1.9211\n",
      "\u001b[1;4mValidati 83     - 26     -          0.3023| 0.2019  | 0.5042 -           0.9071    | 0.9330      | 0.5776   | 0.3231     - 0.2162\u001b[0m\n",
      "Training 84     - 100    -          0.2190| 0.0548  | 0.2738 -           0.9081    | 0.9666      | 0.6746   | 0.4112     - 1.9096\n",
      "\u001b[1;4mValidati 84     - 26     -          0.2869| 0.2021  | 0.4890 -           0.9054    | 0.9339      | 0.5739   | 0.3442     - 0.2191\u001b[0m\n",
      "Training 85     - 100    -          0.2395| 0.0719  | 0.3113 -           0.9083    | 0.9669      | 0.6742   | 0.4059     - 1.9600\n",
      "\u001b[1;4mValidati 85     - 26     -          0.2978| 0.2020  | 0.4997 -           0.9035    | 0.9332      | 0.5631   | 0.3339     - 0.2208\u001b[0m\n",
      "Training 86     - 100    -          0.1553| 0.0714  | 0.2267 -           0.9100    | 0.9666      | 0.6803   | 0.4065     - 1.9619\n",
      "\u001b[1;4mValidati 86     - 26     -          0.2933| 0.2023  | 0.4956 -           0.9023    | 0.9327      | 0.5602   | 0.3360     - 0.2364\u001b[0m\n",
      "Training 87     - 100    -          0.2639| 0.0867  | 0.3505 -           0.9072    | 0.9666      | 0.6709   | 0.4052     - 1.9180\n",
      "\u001b[1;4mValidati 87     - 26     -          0.2940| 0.2030  | 0.4970 -           0.9028    | 0.9315      | 0.5444   | 0.2957     - 0.2167\u001b[0m\n",
      "Training 88     - 100    -          0.2136| 0.0417  | 0.2553 -           0.9075    | 0.9666      | 0.6689   | 0.3979     - 1.9145\n",
      "\u001b[1;4mValidati 88     - 26     -          0.2892| 0.2026  | 0.4918 -           0.9010    | 0.9316      | 0.5468   | 0.3037     - 0.2160\u001b[0m\n",
      "Training 89     - 100    -          0.2722| 0.0798  | 0.3520 -           0.9080    | 0.9668      | 0.6726   | 0.4067     - 1.9281\n",
      "\u001b[1;4mValidati 89     - 26     -          0.2919| 0.2015  | 0.4934 -           0.9003    | 0.9327      | 0.5461   | 0.3246     - 0.2139\u001b[0m\n",
      "Training 90     - 100    -          0.2827| 0.0770  | 0.3597 -           0.9085    | 0.9665      | 0.6743   | 0.4105     - 1.9427\n",
      "\u001b[1;4mValidati 90     - 26     -          0.2956| 0.2015  | 0.4971 -           0.9036    | 0.9332      | 0.5700   | 0.3652     - 0.2189\u001b[0m\n",
      "Training 91     - 100    -          0.1845| 0.0452  | 0.2298 -           0.9076    | 0.9668      | 0.6716   | 0.4088     - 1.9400\n",
      "\u001b[1;4mValidati 91     - 26     -          0.2989| 0.2029  | 0.5018 -           0.9025    | 0.9314      | 0.5521   | 0.3046     - 0.2208\u001b[0m\n",
      "Training 92     - 100    -          0.2859| 0.0879  | 0.3738 -           0.9112    | 0.9664      | 0.6872   | 0.4106     - 1.9245\n",
      "\u001b[1;4mValidati 92     - 26     -          0.2951| 0.2024  | 0.4975 -           0.9045    | 0.9337      | 0.5697   | 0.3538     - 0.2072\u001b[0m\n",
      "Training 93     - 100    -          0.2495| 0.0929  | 0.3424 -           0.9090    | 0.9667      | 0.6750   | 0.4092     - 1.9027\n",
      "\u001b[1;4mValidati 93     - 26     -          0.2904| 0.2013  | 0.4918 -           0.9077    | 0.9344      | 0.5796   | 0.3517     - 0.2158\u001b[0m\n",
      "Training 94     - 100    -          0.2058| 0.0800  | 0.2858 -           0.9103    | 0.9667      | 0.6805   | 0.4125     - 1.9392\n",
      "\u001b[1;4mValidati 94     - 26     -          0.3001| 0.2051  | 0.5051 -           0.8922    | 0.9293      | 0.5346   | 0.3492     - 0.2190\u001b[0m\n",
      "Training 95     - 100    -          0.2149| 0.0976  | 0.3125 -           0.9099    | 0.9666      | 0.6806   | 0.4035     - 1.9204\n",
      "\u001b[1;4mValidati 95     - 26     -          0.2913| 0.2011  | 0.4924 -           0.9045    | 0.9337      | 0.5679   | 0.3390     - 0.2138\u001b[0m\n",
      "Training 96     - 100    -          0.2309| 0.0757  | 0.3067 -           0.9116    | 0.9664      | 0.6877   | 0.4063     - 1.9409\n",
      "\u001b[1;4mValidati 96     - 26     -          0.2988| 0.2057  | 0.5045 -           0.9026    | 0.9335      | 0.5644   | 0.3458     - 0.2176\u001b[0m\n",
      "Training 97     - 100    -          0.3039| 0.0763  | 0.3802 -           0.9102    | 0.9667      | 0.6804   | 0.4088     - 1.9308\n",
      "\u001b[1;4mValidati 97     - 26     -          0.2948| 0.2010  | 0.4958 -           0.8996    | 0.9328      | 0.5492   | 0.3375     - 0.2520\u001b[0m\n",
      "Training 98     - 100    -          0.2711| 0.0894  | 0.3605 -           0.9102    | 0.9665      | 0.6825   | 0.4096     - 1.8965\n",
      "\u001b[1;4mValidati 98     - 26     -          0.2932| 0.2008  | 0.4940 -           0.9016    | 0.9322      | 0.5575   | 0.3414     - 0.2305\u001b[0m\n",
      "Training 99     - 100    -          0.3036| 0.0759  | 0.3795 -           0.9103    | 0.9663      | 0.6826   | 0.4078     - 1.9146\n",
      "\u001b[1;4mValidati 99     - 26     -          0.2930| 0.2010  | 0.4939 -           0.9044    | 0.9334      | 0.5699   | 0.3418     - 0.2176\u001b[0m\n",
      "Training 100    - 100    -          0.2214| 0.0549  | 0.2763 -           0.9119    | 0.9664      | 0.6894   | 0.4065     - 1.9080\n",
      "\u001b[1;4mValidati 100    - 26     -          0.2970| 0.2019  | 0.4990 -           0.9065    | 0.9341      | 0.5782   | 0.3453     - 0.2144\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
