{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%% Import\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "# utility function & metrics & augmentation\n",
    "from metric_utils.metrics import FScore, BinaryAccuracy\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed\n",
    "\n",
    "# models\n",
    "from dcase2020_task4.baseline.models import WeakBaseline, WeakStrongBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== set the log ====\n",
    "import logging\n",
    "import logging.config\n",
    "from dcase2020.util.log import DEFAULT_LOGGING\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==== reset the seed for reproductability ====\n",
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:15<00:00, 479.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../dataset/DESED/dataset/audio\"\n",
    "# desed_metadata_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"metadata\")\n",
    "# desed_audio_root = os.path.join(\"e:/\", \"Corpus\", \"dcase2020\", \"DESED\", \"dataset\", \"audio\")\n",
    "\n",
    "manager = DESEDManager(\n",
    "    desed_metadata_root, desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    validation_ratio=0.2,\n",
    "    from_disk=False,\n",
    "    nb_vector_bin=53, # The model output localisation with a résolution of ~ 18ms --> 53 temporal bins\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weak ans synthetic20 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "Loading dataset: train, subset: weak\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.add_subset >>> Loading dataset: train, subset: synthetic20\u001b[0m\n",
      "Loading dataset: train, subset: synthetic20\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/synthetic20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.add_subset(\"weak\")\n",
    "manager.add_subset(\"synthetic20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> Creating new train / validation split\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager.split_train_validation >>> validation ratio : 0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "manager.split_train_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%  setup augmentation and create pytorch dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MultipleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(MultipleDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        for d in self.datasets:\n",
    "            assert not isinstance(d, IterableDataset), \"ConcatDataset does not support IterableDataset\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [d[sample_idx] for d in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218, 706)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.filenames), len(val_dataset.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset\n",
    "\n",
    "- We want both the weak and strong ground truth --> the *weak* and *strong* parameters to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    # signal_augmentation.Noise(0.5, target_snr=15),\n",
    "    # signal_augmentation.RandomTimeDropout(0.5, dropout=0.2)\n",
    "]\n",
    "\n",
    "train_dataset = DESEDDataset(manager, train=True, val=False, weak=True, strong=True, augments=augments, cached=True)\n",
    "val_dataset = DESEDDataset(manager, train=False, val=True, weak=True, strong=True, augments=[], cached=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "This model is the same than the weak baseline but have an extra output. <br />\n",
    "the loc_output is compose of a single convolution layer with nb_filters == nb_class. <br />\n",
    "Since their is some pooling layer, the *loc_ouput* have a precision of 53 bins (~= 18 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeakStrongBaseline(\n",
       "  (features): Sequential(\n",
       "    (0): ConvPoolReLU(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "    (2): ConvPoolReLU(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): MaxPool2d(kernel_size=(4, 2), stride=(4, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout2d(p=0.3, inplace=False)\n",
       "      (4): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (weak_output): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1696, out_features=10, bias=True)\n",
       "  )\n",
       "  (strong_output): Sequential(\n",
       "    (0): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WeakStrongBaseline()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================\n",
      "                               Kernel Shape      Output Shape  Params  \\\n",
      "Layer                                                                   \n",
      "0_features.0.Conv2d_0         [1, 32, 3, 3]  [1, 32, 64, 431]   320.0   \n",
      "1_features.0.MaxPool2d_1                  -  [1, 32, 16, 215]       -   \n",
      "2_features.0.BatchNorm2d_2             [32]  [1, 32, 16, 215]    64.0   \n",
      "3_features.0.Dropout2d_3                  -  [1, 32, 16, 215]       -   \n",
      "4_features.0.ReLU6_4                      -  [1, 32, 16, 215]       -   \n",
      "5_features.1.Conv2d_0        [32, 32, 3, 3]  [1, 32, 16, 215]  9.248k   \n",
      "6_features.1.MaxPool2d_1                  -   [1, 32, 4, 107]       -   \n",
      "7_features.1.BatchNorm2d_2             [32]   [1, 32, 4, 107]    64.0   \n",
      "8_features.1.Dropout2d_3                  -   [1, 32, 4, 107]       -   \n",
      "9_features.1.ReLU6_4                      -   [1, 32, 4, 107]       -   \n",
      "10_features.2.Conv2d_0       [32, 32, 3, 3]   [1, 32, 4, 107]  9.248k   \n",
      "11_features.2.MaxPool2d_1                 -    [1, 32, 1, 53]       -   \n",
      "12_features.2.BatchNorm2d_2            [32]    [1, 32, 1, 53]    64.0   \n",
      "13_features.2.Dropout2d_3                 -    [1, 32, 1, 53]       -   \n",
      "14_features.2.ReLU6_4                     -    [1, 32, 1, 53]       -   \n",
      "15_weak_output.Flatten_0                  -         [1, 1696]       -   \n",
      "16_weak_output.Dropout_1                  -         [1, 1696]       -   \n",
      "17_weak_output.Linear_2          [1696, 10]           [1, 10]  16.97k   \n",
      "18_strong_output.Conv2d_0    [32, 10, 1, 1]    [1, 10, 1, 53]   330.0   \n",
      "\n",
      "                             Mult-Adds  \n",
      "Layer                                   \n",
      "0_features.0.Conv2d_0        7.944192M  \n",
      "1_features.0.MaxPool2d_1             -  \n",
      "2_features.0.BatchNorm2d_2        32.0  \n",
      "3_features.0.Dropout2d_3             -  \n",
      "4_features.0.ReLU6_4                 -  \n",
      "5_features.1.Conv2d_0        31.70304M  \n",
      "6_features.1.MaxPool2d_1             -  \n",
      "7_features.1.BatchNorm2d_2        32.0  \n",
      "8_features.1.Dropout2d_3             -  \n",
      "9_features.1.ReLU6_4                 -  \n",
      "10_features.2.Conv2d_0       3.944448M  \n",
      "11_features.2.MaxPool2d_1            -  \n",
      "12_features.2.BatchNorm2d_2       32.0  \n",
      "13_features.2.Dropout2d_3            -  \n",
      "14_features.2.ReLU6_4                -  \n",
      "15_weak_output.Flatten_0             -  \n",
      "16_weak_output.Dropout_1             -  \n",
      "17_weak_output.Linear_2         16.96k  \n",
      "18_strong_output.Conv2d_0       16.96k  \n",
      "----------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params             36.308k\n",
      "Trainable params         36.308k\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             43.625696M\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "input_tensor = torch.zeros((1, 64, 431), dtype=torch.float)\n",
    "input_tensor = input_tensor.cuda()\n",
    "\n",
    "s = summary(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom loss function\n",
    "\n",
    "Since not all file have strong truth, it is necessary to remove those files. <br />\n",
    "For that, the strong mask is computed. If the sum of the strong ground truth is equal to 0 then it is a fake one <br />\n",
    "This file strong loss must not be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_synth_loss(logits_weak, logits_strong, y_weak, y_strong, reduce: str = \"mean\"):\n",
    "    assert reduce in [\"mean\", \"sum\"], \"support only \\\"mean\\\" and \\\"sum\\\"\"\n",
    "    \n",
    "    #  Reduction function\n",
    "    if reduce == \"mean\":\n",
    "        reduce_fn = torch.mean\n",
    "    elif reduce == \"sum\":\n",
    "        reduce_fn = torch.sum\n",
    "    \n",
    "    # based on Binary Cross Entropy loss\n",
    "    weak_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    strong_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    # calc separate loss function\n",
    "    weak_bce = weak_criterion(logits_weak, y_weak)\n",
    "    strong_bce = strong_criterion(logits_strong, y_strong)\n",
    "    \n",
    "    weak_bce = reduce_fn(weak_bce, dim=1)\n",
    "    strong_bce = reduce_fn(strong_bce, dim=(1, 2))\n",
    "    \n",
    "    # calc strong mask\n",
    "    strong_mask = torch.clamp(torch.sum(y_strong, dim=(1, 2)), 0, 1) # vector of 0 or 1\n",
    "    strong_mask = strong_mask.detach() # declared not to need gradients\n",
    "    \n",
    "    # Output the different loss for logging purpose\n",
    "    weak_loss = reduce_fn(weak_bce)\n",
    "    strong_loss = reduce_fn(strong_mask * strong_bce)\n",
    "    total_loss = reduce_fn(weak_bce + strong_mask * strong_bce)\n",
    "    \n",
    "    return weak_loss, strong_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters (crit & callbacks & loaders & metrics)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Setup model and training parameters\n"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "nb_epochs = 100\n",
    "batch_size = 32\n",
    "nb_batch = len(train_dataset) // batch_size\n",
    "\n",
    "optimizers = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# callbacks\n",
    "callbacks = []\n",
    "\n",
    "# tensorboard\n",
    "title = \"WeakBaseline_%s\" % (get_datetime())\n",
    "tensorboard = SummaryWriter(log_dir=Path(\"../tensorboard/%s\" % title), comment=\"weak baseline\")\n",
    "\n",
    "# loaders\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Metrics\n",
    "weak_binacc_func = BinaryAccuracy()\n",
    "strong_binacc_func = BinaryAccuracy()\n",
    "weak_f_func = FScore()\n",
    "strong_f_func = FScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_all_metrics():\n",
    "    metrics = [weak_binacc_func, strong_binacc_func, weak_f_func, strong_f_func]\n",
    "    \n",
    "    for m in metrics:\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n"
     ]
    }
   ],
   "source": [
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6}| {:<8.8}| {:<6.6} - {:<9.9} {:<10.10}| {:<12.12}| {:<9.9}| {:<11.11}- {:<6.6}\"\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.4f}| {:<8.4f}| {:<6.4f} - {:<9.9} {:<10.4f}| {:<12.4f}| {:<9.4f}| {:<11.4f}- {:<6.4f}\"\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"Weak \", \"Strong \", \"Total \", \"metrics: \", \"Weak acc \", \"Strong acc \", \"Weak F1 \", \"Strong F1\", \"Time\"\n",
    ")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% training function\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.train()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    for i, (X, y) in enumerate(training_loader):\n",
    "        # The DESEDDataset return a list of ground truth depending on the selecting option.\n",
    "        # If weak and strong ground truth are selected, the list order is [WEAK, STRONG]\n",
    "        # here there is only one [WEAK]\n",
    "        X = X.cuda().float()\n",
    "        y_weak = y[0].cuda().float()\n",
    "        y_strong = y[1].cuda().float()\n",
    "        \n",
    "        logits_weak, logits_strong = model(X)\n",
    "        \n",
    "        # calc the loss\n",
    "        weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "            logits_weak, logits_strong,\n",
    "            y_weak, y_strong,\n",
    "            reduce=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # calc metrics\n",
    "        # tagging\n",
    "        weak_binacc = weak_binacc_func(logits_weak, y_weak)\n",
    "        weak_fscore = weak_f_func(logits_weak, y_weak)\n",
    "        \n",
    "        # loc\n",
    "        strong_binacc = strong_binacc_func(logits_strong, y_strong)\n",
    "        strong_fscore = strong_f_func(logits_strong, y_strong)\n",
    "        \n",
    "        \n",
    "        # back propagation\n",
    "        optimizers.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        # logs\n",
    "        print(value_form.format(\n",
    "            \"Training: \",\n",
    "            epoch + 1,\n",
    "            int(100 * (i + 1) / nb_batch),\n",
    "            \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "            \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "            time.time() - start_time\n",
    "        ), end=\"\\r\")\n",
    "        \n",
    "    # tensorboard logs\n",
    "    tensorboard.add_scalar(\"train/weak_loss\", weak_loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/strong_loss\", strong_loss.item(), epoch)\n",
    "    tensorboard.add_scalar(\"train/total_loss\", total_loss.item(), epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"train/weak_acc\", weak_binacc, epoch)\n",
    "    tensorboard.add_scalar(\"train/strong_acc\", strong_binacc, epoch)\n",
    "    tensorboard.add_scalar(\"train/weak_f1\", weak_fscore, epoch)\n",
    "    tensorboard.add_scalar(\"train/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% validation function\n"
    }
   },
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "        \n",
    "    reset_all_metrics()\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\") # <-- Force new line\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            y_weak = y[0].cuda().float()\n",
    "            y_strong = y[1].cuda().float()\n",
    "\n",
    "            logits_weak, logits_strong = model(X)\n",
    "\n",
    "            # calc the loss\n",
    "            weak_loss, strong_loss, total_loss = weak_synth_loss(\n",
    "                logits_weak, logits_strong,\n",
    "                y_weak, y_strong,\n",
    "                reduce=\"mean\"\n",
    "            )\n",
    "            \n",
    "            # calc metrics\n",
    "            # tagging\n",
    "            weak_binacc = weak_binacc_func(logits_weak, y_weak)\n",
    "            weak_fscore = weak_f_func(logits_weak, y_weak)\n",
    "\n",
    "            # loc\n",
    "            strong_binacc = strong_binacc_func(logits_strong, y_strong)\n",
    "            strong_fscore = strong_f_func(logits_strong, y_strong)\n",
    "\n",
    "            # logs\n",
    "            print(value_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch + 1,\n",
    "                int(100 * (i + 1) / nb_batch),\n",
    "                \"\", weak_loss.item(), strong_loss.item(), total_loss.item(),\n",
    "                \"\", weak_binacc, strong_binacc, weak_fscore, strong_fscore,\n",
    "                time.time() - start_time\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "        # tensorboard logs\n",
    "        tensorboard.add_scalar(\"val/weak_loss\", weak_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_loss\", strong_loss.item(), epoch)\n",
    "        tensorboard.add_scalar(\"val/total_loss\", total_loss.item(), epoch)\n",
    "\n",
    "        tensorboard.add_scalar(\"val/weak_acc\", weak_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_acc\", strong_binacc, epoch)\n",
    "        tensorboard.add_scalar(\"val/weak_f1\", weak_fscore, epoch)\n",
    "        tensorboard.add_scalar(\"val/strong_f1\", strong_fscore, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Epoch  - %      - Losses:  Weak  | Strong  | Total  - metrics:  Weak acc  | Strong acc  | Weak F1  | Strong F1  - Time  \n",
      "\n",
      "Training 1      - 101    -          0.3985| 0.1005  | 0.4989 -           0.8273    | 50.6695     | nan      | nan        - 1.4933\n",
      "Validati 1      - 23     -          0.5093| 0.2854  | 0.7948 -           0.8292    | 51.0548     | 0.2414   | 0.0000     - 0.1357\n",
      "Training 2      - 101    -          0.4245| 0.1482  | 0.5727 -           0.8340    | 51.1199     | 0.2723   | nan        - 1.4614\n",
      "Validati 2      - 23     -          0.4557| 0.2596  | 0.7153 -           0.8478    | 51.0586     | 0.2845   | nan        - 0.1376\n",
      "Training 3      - 101    -          0.3545| 0.1448  | 0.4994 -           0.8425    | 51.1357     | 0.3248   | nan        - 1.5595\n",
      "Validati 3      - 23     -          0.4908| 0.3169  | 0.8077 -           0.8492    | 51.1289     | 0.3296   | nan        - 0.1534\n",
      "Training 4      - 101    -          0.4345| 0.1078  | 0.5423 -           0.8465    | 51.1549     | 0.3619   | 0.0388     - 1.5713\n",
      "Validati 4      - 23     -          0.5075| 0.2715  | 0.7790 -           0.8537    | 51.1156     | 0.3532   | nan        - 0.1665\n",
      "Training 5      - 101    -          0.3563| 0.1328  | 0.4892 -           0.8488    | 51.1567     | 0.3768   | 0.0549     - 1.5595\n",
      "Validati 5      - 23     -          0.5022| 0.2918  | 0.7939 -           0.8565    | 51.1489     | 0.3647   | nan        - 0.1369\n",
      "Training 6      - 101    -          0.4123| 0.0873  | 0.4996 -           0.8545    | 51.1574     | 0.3890   | 0.0678     - 1.4639\n",
      "Validati 6      - 23     -          0.4628| 0.2748  | 0.7376 -           0.8553    | 51.1268     | 0.3797   | nan        - 0.1348\n",
      "Training 7      - 101    -          0.3889| 0.0972  | 0.4861 -           0.8529    | 51.1676     | 0.4001   | 0.0784     - 1.4978\n",
      "Validati 7      - 23     -          0.5126| 0.2375  | 0.7501 -           0.8592    | 51.1372     | 0.3884   | nan        - 0.1358\n",
      "Training 8      - 101    -          0.3638| 0.0918  | 0.4556 -           0.8581    | 51.1599     | 0.4124   | 0.0913     - 1.5694\n",
      "Validati 8      - 23     -          0.4670| 0.2446  | 0.7116 -           0.8610    | 51.1484     | 0.4019   | nan        - 0.1546\n",
      "Training 9      - 101    -          0.4026| 0.1119  | 0.5144 -           0.8592    | 51.1658     | 0.4262   | 0.1029     - 1.5708\n",
      "Validati 9      - 23     -          0.4194| 0.2386  | 0.6580 -           0.8633    | 51.1325     | 0.4128   | nan        - 0.1527\n",
      "Training 10     - 101    -          0.3563| 0.1086  | 0.4649 -           0.8631    | 51.1656     | 0.4371   | 0.1126     - 1.5139\n",
      "Validati 10     - 23     -          0.4259| 0.2270  | 0.6529 -           0.8698    | 51.1050     | 0.4246   | nan        - 0.1359\n",
      "Training 11     - 101    -          0.3642| 0.0861  | 0.4503 -           0.8630    | 51.1732     | 0.4496   | 0.1232     - 1.4701\n",
      "Validati 11     - 23     -          0.4362| 0.2418  | 0.6780 -           0.8649    | 51.1431     | 0.4345   | nan        - 0.1366\n",
      "Training 12     - 101    -          0.3808| 0.1102  | 0.4910 -           0.8654    | 51.1640     | 0.4592   | 0.1326     - 1.4654\n",
      "Validati 12     - 23     -          0.3850| 0.2280  | 0.6130 -           0.8651    | 51.1579     | 0.4434   | nan        - 0.1346\n",
      "Training 13     - 101    -          0.3501| 0.1244  | 0.4745 -           0.8657    | 51.1679     | 0.4678   | 0.1412     - 1.4819\n",
      "Validati 13     - 23     -          0.4277| 0.2265  | 0.6541 -           0.8690    | 51.1910     | 0.4511   | nan        - 0.1395\n",
      "Training 14     - 101    -          0.3249| 0.0869  | 0.4118 -           0.8668    | 51.1899     | 0.4760   | 0.1498     - 1.4907\n",
      "Validati 14     - 23     -          0.4115| 0.2319  | 0.6434 -           0.8740    | 51.2129     | 0.4598   | nan        - 0.1393\n",
      "Training 15     - 101    -          0.3391| 0.0958  | 0.4350 -           0.8709    | 51.1914     | 0.4854   | 0.1597     - 1.4930\n",
      "Validati 15     - 23     -          0.4221| 0.2271  | 0.6492 -           0.8704    | 51.2356     | 0.4692   | nan        - 0.1371\n",
      "Training 16     - 101    -          0.3256| 0.0965  | 0.4221 -           0.8701    | 51.1784     | 0.4956   | 0.1683     - 1.4991\n",
      "Validati 16     - 23     -          0.4273| 0.2303  | 0.6576 -           0.8731    | 51.2292     | 0.4775   | nan        - 0.1381\n",
      "Training 17     - 101    -          0.3049| 0.0998  | 0.4046 -           0.8705    | 51.2093     | 0.5027   | 0.1761     - 1.5107\n",
      "Validati 17     - 23     -          0.3992| 0.2329  | 0.6321 -           0.8716    | 51.2264     | 0.4844   | nan        - 0.1382\n",
      "Training 18     - 101    -          0.2823| 0.1111  | 0.3934 -           0.8739    | 51.1892     | 0.5101   | 0.1835     - 1.5012\n",
      "Validati 18     - 23     -          0.4561| 0.2118  | 0.6679 -           0.8707    | 51.2503     | 0.4914   | nan        - 0.1486\n",
      "Training 19     - 101    -          0.3091| 0.0852  | 0.3943 -           0.8737    | 51.1952     | 0.5174   | 0.1909     - 1.4909\n",
      "Validati 19     - 23     -          0.4231| 0.2313  | 0.6544 -           0.8701    | 51.2409     | 0.4979   | nan        - 0.1524\n",
      "Training 20     - 101    -          0.2804| 0.0762  | 0.3567 -           0.8743    | 51.2200     | 0.5231   | 0.1979     - 1.5852\n",
      "Validati 20     - 23     -          0.4048| 0.2254  | 0.6302 -           0.8779    | 51.2630     | 0.5044   | nan        - 0.1549\n",
      "Training 21     - 101    -          0.2894| 0.0778  | 0.3671 -           0.8770    | 51.2219     | 0.5317   | 0.2059     - 1.5657\n",
      "Validati 21     - 23     -          0.3948| 0.2138  | 0.6086 -           0.8780    | 51.2375     | 0.5121   | nan        - 0.1730\n",
      "Training 22     - 101    -          0.3261| 0.1004  | 0.4265 -           0.8778    | 51.2020     | 0.5395   | 0.2123     - 1.5644\n",
      "Validati 22     - 23     -          0.3788| 0.2229  | 0.6018 -           0.8796    | 51.2910     | 0.5198   | nan        - 0.1539\n",
      "Training 23     - 101    -          0.2554| 0.0979  | 0.3532 -           0.8762    | 51.2058     | 0.5468   | 0.2184     - 1.5206\n",
      "Validati 23     - 23     -          0.4153| 0.2181  | 0.6334 -           0.8780    | 51.2808     | 0.5260   | nan        - 0.1377\n",
      "Training 24     - 101    -          0.3521| 0.0708  | 0.4229 -           0.8782    | 51.2172     | 0.5528   | 0.2241     - 1.4804\n",
      "Validati 24     - 23     -          0.4276| 0.2180  | 0.6456 -           0.8784    | 51.2751     | 0.5319   | nan        - 0.1369\n",
      "Training 25     - 101    -          0.2535| 0.1069  | 0.3605 -           0.8787    | 51.2021     | 0.5594   | 0.2296     - 1.4697\n",
      "Validati 25     - 23     -          0.3886| 0.2154  | 0.6040 -           0.8817    | 51.2784     | 0.5381   | nan        - 0.1369\n",
      "Training 26     - 101    -          0.3044| 0.1035  | 0.4080 -           0.8789    | 51.2159     | 0.5656   | 0.2349     - 1.4777\n",
      "Validati 26     - 23     -          0.4276| 0.2202  | 0.6478 -           0.8768    | 51.2743     | 0.5436   | nan        - 0.1355\n",
      "Training 27     - 101    -          0.3161| 0.0965  | 0.4126 -           0.8803    | 51.2147     | 0.5711   | 0.2395     - 1.4960\n",
      "Validati 27     - 23     -          0.4398| 0.2133  | 0.6531 -           0.8753    | 51.2451     | 0.5488   | nan        - 0.1363\n",
      "Training 28     - 101    -          0.2894| 0.0811  | 0.3706 -           0.8812    | 51.2112     | 0.5764   | 0.2438     - 1.4966\n",
      "Validati 28     - 23     -          0.4950| 0.2103  | 0.7053 -           0.8803    | 51.2561     | 0.5540   | nan        - 0.1365\n",
      "Training 29     - 101    -          0.2745| 0.0755  | 0.3500 -           0.8812    | 51.2229     | 0.5820   | 0.2484     - 1.4832\n",
      "Validati 29     - 23     -          0.4319| 0.2127  | 0.6446 -           0.8791    | 51.2466     | 0.5592   | nan        - 0.1393\n",
      "Training 30     - 101    -          0.3229| 0.0912  | 0.4140 -           0.8826    | 51.2012     | 0.5871   | 0.2523     - 1.4711\n",
      "Validati 30     - 23     -          0.4274| 0.2202  | 0.6476 -           0.8825    | 51.2755     | 0.5642   | nan        - 0.1361\n",
      "Training 31     - 101    -          0.3250| 0.0949  | 0.4199 -           0.8839    | 51.2345     | 0.5926   | 0.2566     - 1.4940\n",
      "Validati 31     - 23     -          0.4481| 0.2119  | 0.6601 -           0.8827    | 51.2810     | 0.5696   | nan        - 0.1509\n",
      "Training 32     - 101    -          0.3726| 0.1045  | 0.4771 -           0.8824    | 51.2235     | 0.5980   | 0.2612     - 1.5044\n",
      "Validati 32     - 23     -          0.4317| 0.2060  | 0.6378 -           0.8825    | 51.2871     | 0.5743   | nan        - 0.1380\n",
      "Training 33     - 101    -          0.3108| 0.0771  | 0.3878 -           0.8859    | 51.2095     | 0.6032   | 0.2649     - 1.4832\n",
      "Validati 33     - 23     -          0.4288| 0.2099  | 0.6387 -           0.8818    | 51.2654     | 0.5794   | nan        - 0.1386\n",
      "Training 34     - 101    -          0.2105| 0.0545  | 0.2650 -           0.8838    | 51.1999     | 0.6079   | 0.2683     - 1.4900\n",
      "Validati 34     - 23     -          0.4168| 0.2075  | 0.6243 -           0.8802    | 51.2624     | 0.5835   | nan        - 0.1382\n",
      "Training 35     - 101    -          0.2504| 0.0798  | 0.3302 -           0.8872    | 51.2145     | 0.6126   | 0.2720     - 1.4993\n",
      "Validati 35     - 23     -          0.4484| 0.2104  | 0.6587 -           0.8870    | 51.2766     | 0.5886   | nan        - 0.1374\n",
      "Training 36     - 101    -          0.3306| 0.0875  | 0.4181 -           0.8876    | 51.2273     | 0.6181   | 0.2761     - 1.4853\n",
      "Validati 36     - 23     -          0.4302| 0.2079  | 0.6382 -           0.8819    | 51.2895     | 0.5935   | nan        - 0.1396\n",
      "Training 37     - 101    -          0.2601| 0.1308  | 0.3909 -           0.8864    | 51.2164     | 0.6226   | 0.2797     - 1.4958\n",
      "Validati 37     - 23     -          0.4250| 0.2053  | 0.6304 -           0.8885    | 51.2948     | 0.5980   | nan        - 0.1368\n",
      "Training 38     - 101    -          0.3336| 0.0510  | 0.3846 -           0.8883    | 51.2409     | 0.6279   | 0.2837     - 1.4926\n",
      "Validati 38     - 23     -          0.4351| 0.2153  | 0.6504 -           0.8836    | 51.3137     | 0.6029   | nan        - 0.1387\n",
      "Training 39     - 101    -          0.3358| 0.1015  | 0.4373 -           0.8869    | 51.2251     | 0.6326   | 0.2874     - 1.4903\n",
      "Validati 39     - 23     -          0.4302| 0.2064  | 0.6366 -           0.8813    | 51.2861     | 0.6070   | nan        - 0.1353\n",
      "Training 40     - 101    -          0.2601| 0.0709  | 0.3310 -           0.8874    | 51.2280     | 0.6366   | 0.2905     - 1.5059\n",
      "Validati 40     - 23     -          0.4428| 0.2052  | 0.6480 -           0.8872    | 51.2957     | 0.6110   | nan        - 0.1389\n",
      "Training 41     - 101    -          0.2844| 0.1052  | 0.3896 -           0.8878    | 51.2358     | 0.6410   | 0.2942     - 1.5107\n",
      "Validati 41     - 23     -          0.4307| 0.2030  | 0.6337 -           0.8848    | 51.2974     | 0.6150   | nan        - 0.1554\n",
      "Training 42     - 101    -          0.2909| 0.0821  | 0.3730 -           0.8906    | 51.2299     | 0.6451   | 0.2975     - 1.4846\n",
      "Validati 42     - 23     -          0.3995| 0.2046  | 0.6040 -           0.8886    | 51.3010     | 0.6194   | nan        - 0.1355\n",
      "Training 43     - 101    -          0.2659| 0.0925  | 0.3584 -           0.8894    | 51.2342     | 0.6496   | 0.3011     - 1.4763\n",
      "Validati 43     - 23     -          0.4355| 0.2030  | 0.6384 -           0.8876    | 51.2973     | 0.6234   | nan        - 0.1346\n",
      "Training 44     - 101    -          0.3241| 0.0900  | 0.4141 -           0.8905    | 51.2123     | 0.6539   | 0.3042     - 1.4970\n",
      "Validati 44     - 23     -          0.4453| 0.2128  | 0.6581 -           0.8829    | 51.2885     | 0.6272   | nan        - 0.1369\n",
      "Training 45     - 101    -          0.2693| 0.0945  | 0.3638 -           0.8930    | 51.2296     | 0.6578   | 0.3071     - 1.5189\n",
      "Validati 45     - 23     -          0.4350| 0.2095  | 0.6445 -           0.8864    | 51.2867     | 0.6314   | nan        - 0.1482\n",
      "Training 46     - 101    -          0.2987| 0.0716  | 0.3703 -           0.8915    | 51.2161     | 0.6620   | 0.3104     - 1.4953\n",
      "Validati 46     - 23     -          0.4336| 0.2088  | 0.6424 -           0.8852    | 51.3039     | 0.6351   | nan        - 0.1343\n",
      "Training 47     - 101    -          0.2701| 0.1063  | 0.3764 -           0.8916    | 51.2256     | 0.6659   | 0.3133     - 1.4874\n",
      "Validati 47     - 23     -          0.4349| 0.2128  | 0.6477 -           0.8810    | 51.3258     | 0.6385   | nan        - 0.1375\n",
      "Training 48     - 101    -          0.2462| 0.0606  | 0.3068 -           0.8926    | 51.2213     | 0.6694   | 0.3159     - 1.5174\n",
      "Validati 48     - 23     -          0.4358| 0.2051  | 0.6409 -           0.8905    | 51.3173     | 0.6423   | nan        - 0.1375\n",
      "Training 49     - 101    -          0.2596| 0.0838  | 0.3434 -           0.8932    | 51.2061     | 0.6734   | 0.3188     - 1.4933\n",
      "Validati 49     - 23     -          0.4684| 0.2060  | 0.6744 -           0.8889    | 51.2943     | 0.6460   | nan        - 0.1356\n",
      "Training 50     - 101    -          0.2966| 0.0625  | 0.3591 -           0.8926    | 51.2364     | 0.6771   | 0.3213     - 1.5050\n",
      "Validati 50     - 23     -          0.4573| 0.2073  | 0.6645 -           0.8904    | 51.3048     | 0.6494   | nan        - 0.1357\n",
      "Training 51     - 101    -          0.2236| 0.1179  | 0.3415 -           0.8925    | 51.2348     | 0.6807   | 0.3241     - 1.5137\n",
      "Validati 51     - 23     -          0.4338| 0.1984  | 0.6322 -           0.8891    | 51.3175     | 0.6528   | nan        - 0.1367\n",
      "Training 52     - 101    -          0.3176| 0.0526  | 0.3703 -           0.8952    | 51.2207     | 0.6844   | 0.3266     - 1.5033\n",
      "Validati 52     - 23     -          0.4314| 0.1993  | 0.6306 -           0.8851    | 51.2916     | 0.6564   | nan        - 0.1364\n",
      "Training 53     - 101    -          0.2484| 0.0858  | 0.3342 -           0.8938    | 51.2282     | 0.6878   | 0.3290     - 1.5123\n",
      "Validati 53     - 23     -          0.4543| 0.2071  | 0.6614 -           0.8849    | 51.2846     | 0.6595   | nan        - 0.1356\n",
      "Training 54     - 101    -          0.3145| 0.0775  | 0.3920 -           0.8934    | 51.2271     | 0.6910   | 0.3312     - 1.5131\n",
      "Validati 54     - 23     -          0.4447| 0.2051  | 0.6498 -           0.8895    | 51.3026     | 0.6626   | nan        - 0.1367\n",
      "Training 55     - 101    -          0.2438| 0.0792  | 0.3231 -           0.8959    | 51.2469     | 0.6945   | 0.3336     - 1.4953\n",
      "Validati 55     - 23     -          0.4400| 0.2053  | 0.6453 -           0.8921    | 51.3307     | 0.6662   | nan        - 0.1358\n",
      "Training 56     - 101    -          0.2862| 0.0486  | 0.3348 -           0.8935    | 51.2344     | 0.6982   | 0.3360     - 1.4757\n",
      "Validati 56     - 23     -          0.4416| 0.2095  | 0.6510 -           0.8921    | 51.3126     | 0.6692   | nan        - 0.1403\n",
      "Training 57     - 101    -          0.2181| 0.0660  | 0.2841 -           0.8938    | 51.2366     | 0.7012   | 0.3383     - 1.4794\n",
      "Validati 57     - 23     -          0.4359| 0.2074  | 0.6432 -           0.8955    | 51.3435     | 0.6723   | nan        - 0.1381\n",
      "Training 58     - 101    -          0.2896| 0.0621  | 0.3517 -           0.8959    | 51.2261     | 0.7045   | 0.3407     - 1.5280\n",
      "Validati 58     - 23     -          0.4523| 0.2103  | 0.6626 -           0.8908    | 51.2857     | 0.6755   | nan        - 0.1523\n",
      "Training 59     - 101    -          0.3193| 0.0836  | 0.4029 -           0.8950    | 51.2129     | 0.7076   | 0.3427     - 1.5923\n",
      "Validati 59     - 23     -          0.4463| 0.2028  | 0.6491 -           0.8939    | 51.3242     | 0.6784   | nan        - 0.1648\n",
      "Training 60     - 101    -          0.2002| 0.0734  | 0.2736 -           0.8959    | 51.2207     | 0.7106   | 0.3447     - 1.5798\n",
      "Validati 60     - 23     -          0.4300| 0.2009  | 0.6309 -           0.8844    | 51.2764     | 0.6811   | nan        - 0.1518\n",
      "Training 61     - 101    -          0.2330| 0.0586  | 0.2915 -           0.8955    | 51.2364     | 0.7135   | 0.3466     - 1.5323\n",
      "Validati 61     - 23     -          0.4461| 0.2029  | 0.6490 -           0.8899    | 51.2996     | 0.6838   | nan        - 0.1372\n",
      "Training 62     - 101    -          0.1985| 0.1061  | 0.3046 -           0.8979    | 51.2254     | 0.7163   | 0.3486     - 1.4908\n",
      "Validati 62     - 23     -          0.4409| 0.2088  | 0.6497 -           0.8925    | 51.3341     | 0.6867   | nan        - 0.1348\n",
      "Training 63     - 101    -          0.2461| 0.0751  | 0.3212 -           0.8962    | 51.2282     | 0.7192   | 0.3507     - 1.5046\n",
      "Validati 63     - 23     -          0.4082| 0.2037  | 0.6120 -           0.8942    | 51.3173     | 0.6894   | nan        - 0.1374\n",
      "Training 64     - 101    -          0.2444| 0.0682  | 0.3126 -           0.8975    | 51.2138     | 0.7222   | 0.3526     - 1.5267\n",
      "Validati 64     - 23     -          0.4595| 0.1992  | 0.6588 -           0.8909    | 51.3160     | 0.6922   | nan        - 0.1379\n",
      "Training 65     - 101    -          0.3244| 0.1118  | 0.4363 -           0.8960    | 51.2192     | 0.7250   | 0.3544     - 1.4996\n",
      "Validati 65     - 23     -          0.4524| 0.2038  | 0.6563 -           0.8912    | 51.3370     | 0.6948   | nan        - 0.1364\n",
      "Training 66     - 101    -          0.2672| 0.1131  | 0.3803 -           0.8980    | 51.2445     | 0.7277   | 0.3562     - 1.5093\n",
      "Validati 66     - 23     -          0.4281| 0.1986  | 0.6267 -           0.8906    | 51.3420     | 0.6973   | nan        - 0.1363\n",
      "Training 67     - 101    -          0.3876| 0.0843  | 0.4719 -           0.8959    | 51.2413     | 0.7303   | 0.3581     - 1.4921\n",
      "Validati 67     - 23     -          0.4637| 0.1966  | 0.6603 -           0.8924    | 51.3068     | 0.6998   | nan        - 0.1346\n",
      "Training 68     - 101    -          0.3507| 0.0672  | 0.4178 -           0.8974    | 51.2426     | 0.7330   | 0.3599     - 1.5039\n",
      "Validati 68     - 23     -          0.4250| 0.2093  | 0.6343 -           0.8901    | 51.3062     | 0.7024   | nan        - 0.1395\n",
      "Training 69     - 101    -          0.3431| 0.0937  | 0.4368 -           0.9001    | 51.2287     | 0.7356   | 0.3616     - 1.4978\n",
      "Validati 69     - 23     -          0.4357| 0.2028  | 0.6385 -           0.8943    | 51.3231     | 0.7050   | nan        - 0.1385\n",
      "Training 70     - 101    -          0.2793| 0.0763  | 0.3556 -           0.9014    | 51.2269     | 0.7386   | 0.3635     - 1.4724\n",
      "Validati 70     - 23     -          0.4210| 0.2037  | 0.6247 -           0.8950    | 51.3380     | 0.7079   | nan        - 0.1355\n",
      "Training 71     - 101    -          0.2141| 0.0552  | 0.2693 -           0.8986    | 51.2141     | 0.7414   | 0.3653     - 1.5039\n",
      "Validati 71     - 23     -          0.4170| 0.1983  | 0.6153 -           0.8980    | 51.3393     | 0.7105   | nan        - 0.1568\n",
      "Training 72     - 101    -          0.2715| 0.1077  | 0.3792 -           0.8992    | 51.2291     | 0.7443   | 0.3672     - 1.5889\n",
      "Validati 72     - 23     -          0.4565| 0.1996  | 0.6561 -           0.8936    | 51.3337     | 0.7131   | nan        - 0.1356\n",
      "Training 73     - 101    -          0.1831| 0.0648  | 0.2479 -           0.8987    | 51.2009     | 0.7466   | 0.3688     - 1.5199\n",
      "Validati 73     - 23     -          0.4481| 0.2027  | 0.6507 -           0.8935    | 51.3302     | 0.7154   | nan        - 0.1491\n",
      "Training 74     - 101    -          0.2736| 0.0989  | 0.3725 -           0.9002    | 51.2268     | 0.7492   | 0.3703     - 1.5292\n",
      "Validati 74     - 23     -          0.4462| 0.2024  | 0.6485 -           0.8906    | 51.2904     | 0.7178   | nan        - 0.1527\n",
      "Training 75     - 101    -          0.3117| 0.1189  | 0.4306 -           0.9014    | 51.2269     | 0.7518   | 0.3718     - 1.6303\n",
      "Validati 75     - 23     -          0.4296| 0.2009  | 0.6306 -           0.8954    | 51.3196     | 0.7203   | nan        - 0.1563\n",
      "Training 76     - 101    -          0.2391| 0.0815  | 0.3206 -           0.8972    | 51.2055     | 0.7542   | 0.3734     - 1.5824\n",
      "Validati 76     - 23     -          0.4371| 0.2051  | 0.6422 -           0.8933    | 51.3137     | 0.7225   | nan        - 0.1399\n",
      "Training 77     - 101    -          0.2682| 0.0945  | 0.3627 -           0.8978    | 51.2306     | 0.7564   | 0.3749     - 1.4921\n",
      "Validati 77     - 23     -          0.4276| 0.2001  | 0.6276 -           0.8933    | 51.3174     | 0.7246   | nan        - 0.1424\n",
      "Training 78     - 101    -          0.2749| 0.1445  | 0.4194 -           0.8997    | 51.2336     | 0.7586   | 0.3763     - 1.5039\n",
      "Validati 78     - 23     -          0.4337| 0.1983  | 0.6320 -           0.8959    | 51.3213     | 0.7267   | nan        - 0.1403\n",
      "Training 79     - 101    -          0.1815| 0.0994  | 0.2809 -           0.9025    | 51.2220     | 0.7609   | 0.3778     - 1.5235\n",
      "Validati 79     - 23     -          0.4426| 0.2008  | 0.6434 -           0.8944    | 51.3273     | 0.7291   | nan        - 0.1365\n",
      "Training 80     - 101    -          0.1906| 0.0569  | 0.2476 -           0.9019    | 51.2321     | 0.7634   | 0.3793     - 1.4971\n",
      "Validati 80     - 23     -          0.4220| 0.2013  | 0.6233 -           0.8942    | 51.3465     | 0.7314   | nan        - 0.1397\n",
      "Training 81     - 101    -          0.2969| 0.0451  | 0.3420 -           0.9020    | 51.2425     | 0.7658   | 0.3809     - 1.5219\n",
      "Validati 81     - 23     -          0.4125| 0.2060  | 0.6185 -           0.8986    | 51.3261     | 0.7337   | nan        - 0.1375\n",
      "Training 82     - 101    -          0.2331| 0.0790  | 0.3120 -           0.9013    | 51.2303     | 0.7682   | 0.3826     - 1.5187\n",
      "Validati 82     - 23     -          0.4193| 0.1997  | 0.6189 -           0.8929    | 51.3202     | 0.7359   | nan        - 0.1362\n",
      "Training 83     - 101    -          0.2403| 0.1007  | 0.3410 -           0.9022    | 51.2271     | 0.7704   | 0.3839     - 1.5001\n",
      "Validati 83     - 23     -          0.4181| 0.2013  | 0.6194 -           0.8984    | 51.3492     | 0.7381   | nan        - 0.1414\n",
      "Training 84     - 101    -          0.2860| 0.0780  | 0.3640 -           0.8991    | 51.2393     | 0.7726   | 0.3854     - 1.5226\n",
      "Validati 84     - 23     -          0.4501| 0.2066  | 0.6568 -           0.8948    | 51.3409     | 0.7400   | nan        - 0.1373\n",
      "Training 85     - 101    -          0.2157| 0.0701  | 0.2858 -           0.9038    | 51.2286     | 0.7748   | 0.3868     - 1.5221\n",
      "Validati 85     - 23     -          0.4325| 0.2056  | 0.6380 -           0.8963    | 51.3380     | 0.7422   | nan        - 0.1373\n",
      "Training 86     - 101    -          0.1785| 0.0814  | 0.2599 -           0.9012    | 51.2439     | 0.7770   | 0.3881     - 1.5070\n",
      "Validati 86     - 23     -          0.4127| 0.2092  | 0.6219 -           0.8981    | 51.3227     | 0.7442   | nan        - 0.1414\n",
      "Training 87     - 101    -          0.3486| 0.1232  | 0.4718 -           0.9012    | 51.2354     | 0.7790   | 0.3893     - 1.5317\n",
      "Validati 87     - 23     -          0.4492| 0.2038  | 0.6530 -           0.8952    | 51.3564     | 0.7461   | nan        - 0.1375\n",
      "Training 88     - 101    -          0.2348| 0.0973  | 0.3321 -           0.9014    | 51.2267     | 0.7811   | 0.3907     - 1.5990\n",
      "Validati 88     - 23     -          0.4228| 0.2073  | 0.6301 -           0.8992    | 51.3398     | 0.7481   | nan        - 0.1660\n",
      "Training 89     - 101    -          0.2390| 0.0777  | 0.3167 -           0.9027    | 51.2222     | 0.7831   | 0.3918     - 1.5885\n",
      "Validati 89     - 23     -          0.4350| 0.2057  | 0.6408 -           0.8971    | 51.3064     | 0.7501   | nan        - 0.1570\n",
      "Training 90     - 101    -          0.2114| 0.0821  | 0.2935 -           0.9037    | 51.2401     | 0.7853   | 0.3931     - 1.5269\n",
      "Validati 90     - 23     -          0.4556| 0.2149  | 0.6705 -           0.8955    | 51.3311     | 0.7522   | nan        - 0.1569\n",
      "Training 91     - 101    -          0.2779| 0.0590  | 0.3369 -           0.9054    | 51.2366     | 0.7876   | 0.3944     - 1.5687\n",
      "Validati 91     - 23     -          0.4326| 0.2054  | 0.6380 -           0.8976    | 51.3390     | 0.7544   | nan        - 0.1364\n",
      "Training 92     - 101    -          0.1863| 0.0880  | 0.2742 -           0.9045    | 51.2414     | 0.7897   | 0.3957     - 1.5427\n",
      "Validati 92     - 23     -          0.4436| 0.2094  | 0.6531 -           0.8981    | 51.3429     | 0.7564   | nan        - 0.1377\n",
      "Training 93     - 101    -          0.2746| 0.0942  | 0.3688 -           0.9029    | 51.2266     | 0.7918   | 0.3970     - 1.5173\n",
      "Validati 93     - 23     -          0.4311| 0.2072  | 0.6383 -           0.8924    | 51.3393     | 0.7582   | nan        - 0.1393\n",
      "Training 94     - 101    -          0.3315| 0.0809  | 0.4123 -           0.9027    | 51.2468     | 0.7936   | 0.3982     - 1.5485\n",
      "Validati 94     - 23     -          0.4205| 0.2060  | 0.6265 -           0.9020    | 51.3315     | 0.7601   | nan        - 0.1387\n",
      "Training 95     - 101    -          0.3212| 0.0883  | 0.4095 -           0.9058    | 51.2408     | 0.7958   | 0.3996     - 1.5034\n",
      "Validati 95     - 23     -          0.4318| 0.2010  | 0.6327 -           0.8989    | 51.3356     | 0.7622   | nan        - 0.1367\n",
      "Training 96     - 101    -          0.2095| 0.0938  | 0.3034 -           0.9052    | 51.2371     | 0.7979   | 0.4008     - 1.4834\n",
      "Validati 96     - 23     -          0.4447| 0.2058  | 0.6505 -           0.8992    | 51.3519     | 0.7641   | nan        - 0.1383\n",
      "Training 97     - 101    -          0.1620| 0.0743  | 0.2363 -           0.9050    | 51.2567     | 0.7998   | 0.4020     - 1.5175\n",
      "Validati 97     - 23     -          0.4360| 0.2024  | 0.6384 -           0.8935    | 51.3197     | 0.7660   | nan        - 0.1358\n",
      "Training 98     - 101    -          0.2402| 0.0747  | 0.3149 -           0.9024    | 51.2385     | 0.8016   | 0.4031     - 1.4900\n",
      "Validati 98     - 23     -          0.4674| 0.2080  | 0.6754 -           0.9015    | 51.3667     | 0.7677   | nan        - 0.1401\n",
      "Training 99     - 101    -          0.2142| 0.1042  | 0.3184 -           0.9042    | 51.2379     | 0.8035   | 0.4043     - 1.5431\n",
      "Validati 99     - 23     -          0.4262| 0.2091  | 0.6352 -           0.9020    | 51.3450     | 0.7694   | nan        - 0.1526\n",
      "Training 100    - 101    -          0.1889| 0.0986  | 0.2875 -           0.9044    | 51.2324     | 0.8054   | 0.4054     - 1.5740\n",
      "Validati 100    - 23     -          0.4448| 0.2104  | 0.6551 -           0.8996    | 51.3549     | 0.7712   | nan        - 0.1402\r"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "for e in range(nb_epochs):\n",
    "    train(e)\n",
    "    val(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2020",
   "language": "python",
   "name": "dcase2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
