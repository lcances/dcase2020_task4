{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NU M_THREADS\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "import random\n",
    "from random import shuffle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from advertorch.attacks import GradientSignAttack\n",
    "from torch.nn.utils import weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lcances/sync/Documents_sync/Projet/dcase2020_task4/dcase2020_task4/CoTraining/notebooks', '/home/lcances/.miniconda3/envs/dcase2020/lib/python37.zip', '/home/lcances/.miniconda3/envs/dcase2020/lib/python3.7', '/home/lcances/.miniconda3/envs/dcase2020/lib/python3.7/lib-dynload', '', '/home/lcances/.miniconda3/envs/dcase2020/lib/python3.7/site-packages', '/home/lcances/sync/Documents_sync/Projet/augmentation_utils', '/home/lcances/sync/Documents_sync/Projet/dcase2020_task4', '/home/lcances/sync/Documents_sync/Projet/Datasets/dcase2020', '/home/lcances/.miniconda3/envs/dcase2020/lib/python3.7/site-packages/IPython/extensions', '/home/lcances/.ipython', '..', '/home/lcances/sync/Documents_sync/Projet/dcase2020_task4/dcase2020_task4']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# dataset manager\n",
    "from dcase2020.datasetManager import DESEDManager\n",
    "from dcase2020.datasets import DESEDDataset\n",
    "\n",
    "\n",
    "import augmentation_utils.signal_augmentations as sa\n",
    "from metric_utils.metrics import CategoricalAccuracy, BinaryRatio, ContinueAverage, FScore\n",
    "\n",
    "from dcase2020_task4.util.utils import get_datetime, reset_seed, ZipCycle\n",
    "from dcase2020_task4.util.checkpoint import CheckPoint\n",
    "\n",
    "from dcase2020_task4.CoTraining.models import dcase2019_model\n",
    "from dcase2020_task4.CoTraining.losses import loss_cot, loss_diff, loss_diff, p_loss_diff, weak_synth_loss\n",
    "from dcase2020_task4.CoTraining.ramps import Warmup, sigmoid_rampup\n",
    "from dcase2020_task4.CoTraining.samplers import CoTrainingSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments (for compatibility with script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = argparse.ArgumentParser(description='Deep Co-Training for Semi-Supervised Image Recognition')\n",
    "parser.add_argument(\"--model\", default=\"cnn\", type=str, help=\"Model to load, see list of model in models.py\")\n",
    "\n",
    "parser.add_argument(\"--nb_view\", default=2, type=int, help=\"Number of supervised view\")\n",
    "parser.add_argument(\"--ratio\", default=0.1, type=float)\n",
    "parser.add_argument(\"--parser_ratio\", default=None, type=float, help=\"ratio to apply for sampling the S and U data\")\n",
    "parser.add_argument(\"--subsampling\", default=1.0, type=float, help=\"subsampling ratio\")\n",
    "parser.add_argument(\"--subsampling_method\", default=\"balance\", type=str, help=\"method to perform subsampling [random | balance]\")\n",
    "\n",
    "parser.add_argument('--batchsize', '-b', default=100, type=int)\n",
    "parser.add_argument('--epochs', default=150, type=int)\n",
    "\n",
    "parser.add_argument('--lambda_cot_max', default=10, type=int)\n",
    "parser.add_argument('--lambda_diff_max', default=0.5, type=float)\n",
    "parser.add_argument('--warm_up', default=80.0, type=float)\n",
    "parser.add_argument('--momentum', default=0.0, type=float)\n",
    "parser.add_argument('--decay', default=1e-3, type=float)\n",
    "parser.add_argument('--epsilon', default=0.02, type=float)\n",
    "\n",
    "parser.add_argument('--seed', default=1234, type=int)\n",
    "parser.add_argument('--num_class', default=10, type=int)\n",
    "parser.add_argument(\"-T\", '--tensorboard_dir', default='tensorboard/', type=str)\n",
    "parser.add_argument('--checkpoint_dir', default='checkpoint', type=str)\n",
    "parser.add_argument('--base_lr', default=0.05, type=float)\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "parser.add_argument('--dataset', default='cifar10', type=str, help='choose svhn or cifar10, svhn is not implemented yey')\n",
    "parser.add_argument(\"--job_name\", default=\"default\", type=str)\n",
    "parser.add_argument(\"-a\",\"--augments\", action=\"append\", help=\"Augmentation. use as if python script\")\n",
    "parser.add_argument(\"--augment_S\", action=\"store_true\", help=\"Apply augmentation on Supervised part\")\n",
    "parser.add_argument(\"--augment_U\", action=\"store_true\", help=\"Apply augmentation on Unsupervised part\")\n",
    "parser.add_argument(\"--num_workers\", default=0, type=int, help=\"Choose number of worker to train the model\")\n",
    "parser.add_argument(\"--log\", default=\"warning\", help=\"Log level\")\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../../../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../../../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../../../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../../../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager.__init__ >>> ../../../dataset/DESED/dataset/audio/dcase2020_dataset_22050.hdf5\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../../../dataset/DESED/dataset/metadata/train/weak.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../../../dataset/DESED/dataset/metadata/train/unlabel_in_domain.tsv\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._load_metadata >>> Reading metadata: ../../../dataset/DESED/dataset/metadata/train/synthetic20.tsv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ==== load the dataset ====\n",
    "desed_metadata_root = \"../../../dataset/DESED/dataset/metadata\"\n",
    "desed_audio_root = \"../../../dataset/DESED/dataset/audio\"\n",
    "\n",
    "manager_parameters = dict(\n",
    "    metadata_root = desed_metadata_root,\n",
    "    audio_root = desed_audio_root,\n",
    "    sampling_rate = 22050,\n",
    "    from_disk = False,\n",
    "    subsampling = 1.0,\n",
    "    subsampling_method=\"inverse_distribution\",\n",
    "    nb_vector_bin = 431,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "S_manager = DESEDManager(**manager_parameters)\n",
    "U_manager = DESEDManager(**manager_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add all training subsets and validation subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: weak\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/weak\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7582/7582 [00:14<00:00, 506.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: synthetic20\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/synthetic20\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4251/4251 [00:08<00:00, 483.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_val_subset >>> Loading dataset: validation\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/validation\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37mINFO --- datasetManager._add_train_metadata >>> Loading metadata for: unlabel_in_domain\u001b[0m\n",
      "\u001b[1;37mINFO --- datasetManager._add_train_subset >>> Loading dataset: train, subset: unlabel_in_domain\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> hdf_file: <HDF5 file \"dcase2020_dataset_22050.hdf5\" (mode r)>\u001b[0m\n",
      "\u001b[1;34mDEBUG --- datasetManager._hdf_to_dict >>> path: DESED/dataset/audio/train/unlabel_in_domain\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "S_manager.add_subset(\"weak\")\n",
    "S_manager.add_subset(\"synthetic20\")\n",
    "S_manager.add_subset(\"validation\")\n",
    "\n",
    "U_manager.add_subset(\"unlabel_in_domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_augments = [\n",
    "    \n",
    "]\n",
    "\n",
    "U_augments = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the sampler with the specified number of supervised file\n",
    "train_S_dataset = DESEDDataset(S_manager, train=True, val=False, weak=True, strong=True, augments=S_augments, cached=False)\n",
    "train_U_dataset = DESEDDataset(U_manager, train=True, val=False, weak=False, strong=False, augments=U_augments, cached=False)\n",
    "val_dataset = DESEDDataset(S_manager, train=False, val=True, weak=True, strong=True, augments=[], cached=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_S_dataset), len(train_U_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataloader\n",
    "Batch_size must be divided into a supervised minibatch and an unsupervised one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_S_files = len(train_S_dataset)\n",
    "nb_U_files = len(train_U_dataset)\n",
    "\n",
    "ratio = nb_S_files / nb_U_files\n",
    "\n",
    "batch_size = args.batchsize\n",
    "S_batch_size = int(np.floor(batch_size * ratio))\n",
    "U_batch_size = int(np.ceil(batch_size * (1 - ratio)))\n",
    "\n",
    "nb_batch = (nb_S_files + nb_U_files) / batch_size\n",
    "\n",
    "print(\"S_batch_size: \", S_batch_size)\n",
    "print(\"U_batch_size: \", U_batch_size)\n",
    "print(nb_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_S1_loader = torch.utils.data.DataLoader(train_S_dataset, batch_size=S_batch_size, shuffle=True, num_workers=10)\n",
    "train_S2_loader = torch.utils.data.DataLoader(train_S_dataset, batch_size=S_batch_size, shuffle=True, num_workers=10)\n",
    "train_U_loader = torch.utils.data.DataLoader(train_U_dataset, batch_size=U_batch_size, shuffle=True, num_workers=10)\n",
    "train_loader = ZipCycle([train_S1_loader, train_S2_loader, train_U_loader])\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = dcase2019_model\n",
    "\n",
    "m1, m2 = model_func(), model_func()\n",
    "\n",
    "m1 = m1.cuda()\n",
    "m2 = m2.cuda()\n",
    "\n",
    "\n",
    "# Advertorch can't work with multiple output models.\n",
    "def weaker(model):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        weak_output, _ = model(*args, **kwargs)\n",
    "        return weak_output\n",
    "    return wrapper\n",
    "    \n",
    "    \n",
    "    \n",
    "weak_m1 = weaker(m1)\n",
    "weak_m2 = weaker(m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial generation\n",
    "Multilabel adversarial generation paper from 2019: https://arxiv.org/pdf/1901.00546.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# adversarial generation\n",
    "adv_generator_1 = GradientSignAttack(\n",
    "    weak_m1, loss_fn=nn.BCEWithLogitsLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False\n",
    ")\n",
    "\n",
    "adv_generator_2 = GradientSignAttack(\n",
    "    weak_m2, loss_fn=nn.BCEWithLogitsLoss(reduction=\"sum\"),\n",
    "    eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizers, Checkpoint, Warmup & callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(m1.parameters()) + list(m2.parameters())\n",
    "optimizer = optim.SGD(params, lr=args.base_lr, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "checkpoint_m1 = CheckPoint(m1, optimizer, mode=\"max\", name=\"../models/best_dct_m1.torch\")\n",
    "checkpoint_m2 = CheckPoint(m2, optimizer, mode=\"max\", name=\"../models/best_dct_m2.torch\")\n",
    "\n",
    "lr_lambda = lambda epoch: (1.0 + math.cos((epoch-1)*math.pi/args.epochs))\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# define the warmups\n",
    "lambda_cot = Warmup(args.lambda_cot_max, args.warm_up, sigmoid_rampup)\n",
    "lambda_diff = Warmup(args.lambda_diff_max, args.warm_up, sigmoid_rampup)\n",
    "\n",
    "callbacks = [lr_scheduler, lambda_cot, lambda_diff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metrics\n",
    "ratioS = [BinaryRatio(), BinaryRatio()]\n",
    "weak_fscores = [FScore(), FScore()]\n",
    "strong_fscores = [FScore(), FScore()]\n",
    "avg_losses = {\n",
    "    \"l_sup\": ContinueAverage(),\n",
    "    \"l_cot\": ContinueAverage(),\n",
    "    \"l_diff\": ContinueAverage(),\n",
    "    \"total\": ContinueAverage()   \n",
    "}\n",
    "\n",
    "\n",
    "def reset_all_metrics():\n",
    "    all_metrics = [*ratioS, *weak_fscores, *strong_fscores, *avg_losses.values()]\n",
    "    for m in all_metrics:\n",
    "        m.reset()\n",
    "        \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def partial_eval(model):\n",
    "    for c in model.children():\n",
    "        if not isinstance(c, nn.GRU):\n",
    "            c.train(False)\n",
    "            \n",
    "def binarize(tensor, threshold: float = 0.5, apply_sigmoid: bool = False):\n",
    "    if apply_sigmoid:\n",
    "        tensor = torch.sigmoid(tensor)\n",
    "        \n",
    "    tensor[tensor >= threshold] = 1\n",
    "    tensor[tensor < threshold] = 0\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "title = \"%s_%s_%slcm_%sldm_%swl\" % (\n",
    "    get_datetime(),\n",
    "    args.job_name,\n",
    "    args.lambda_cot_max,\n",
    "    args.lambda_diff_max,\n",
    "    args.warm_up,\n",
    ")\n",
    "tensorboard = SummaryWriter(\"%s/%s\" % (args.tensorboard_dir, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak / Strong DCT lossFalse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDERLINE_SEQ = \"\\033[1;4m\"\n",
    "RESET_SEQ = \"\\033[0m\"\n",
    "\n",
    "header_form = \"{:<8.8} {:<6.6} - {:<6.6} - {:<8.8} {:<6.6}| {:<6.6}| {:<8.8}| {:<6.6} - {:<9.9}  {:<9.9}| {:<11.11}- {:<6.6}\"\n",
    "\n",
    "value_form  = \"{:<8.8} {:<6} - {:<6} - {:<8.8} {:<6.3f}| {:<6.3f}| {:<8.3f}| {:<6.3f} - {:<9.9} {:<9.3f}| {:<11.3f}- {:<6.3f}\"\n",
    "\n",
    "train_form = value_form\n",
    "val_form = UNDERLINE_SEQ + value_form + RESET_SEQ\n",
    "\n",
    "header = header_form.format(\n",
    "    \"\", \"Epoch\", \"%\", \"Losses:\", \"sup\", \"cot \", \"diff\", \"Total \", \"metrics:\", \"Weak F1 \", \"Strong F1\", \"Time\"\n",
    ")\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    m1.train()\n",
    "    m2.train()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    ls = 0.0\n",
    "    lc = 0.0 \n",
    "    ld = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"\")\n",
    "    \n",
    "    for batch, (S1, S2, U) in enumerate(train_loader):\n",
    "        # Separate the diff component of the minibatch\n",
    "        X_S1, X_S2, X_U = S1[0], S2[0], U[0]\n",
    "        weak_y_S1, strong_y_S1 = S1[1][0], S1[1][1]\n",
    "        weak_y_S2, strong_y_S2 = S2[1][0], S2[1][1]\n",
    "        \n",
    "        X_S1, X_S2, X_U = X_S1.cuda().float(), X_S2.cuda().float(), X_U.cuda().float()\n",
    "        weak_y_S1, strong_y_S1 = weak_y_S1.cuda().float(), strong_y_S1.cuda().float()\n",
    "        weak_y_S2, strong_y_S2 = weak_y_S2.cuda().float(), strong_y_S2.cuda().float()\n",
    "\n",
    "        # Predict all minibatch component separately\n",
    "        weak_logits_S1, strong_logits_S1 = m1(X_S1)\n",
    "        weak_logits_S2, strong_logits_S2 = m2(X_S2)\n",
    "        weak_logits_U1, strong_logits_U1 = m1(X_U)\n",
    "        weak_logits_U2, strong_logits_U2 = m2(X_U)\n",
    "\n",
    "        weak_pred_S1 = binarize(weak_logits_S1, apply_sigmoid=True)\n",
    "        weak_pred_S2 = binarize(weak_logits_S2, apply_sigmoid=True)\n",
    "        weak_pred_U1 = binarize(weak_logits_U1, apply_sigmoid=True)\n",
    "        weak_pred_U2 = binarize(weak_logits_U2, apply_sigmoid=True)\n",
    "        _, weak_pred_S1 = torch.max(weak_logits_S1, 1)\n",
    "        _, weak_pred_S2 = torch.max(weak_logits_S2, 1)\n",
    "\n",
    "        # pseudo labels of U \n",
    "        _, weak_pred_U1 = torch.max(weak_logits_U1, 1)\n",
    "        _, weak_pred_U2 = torch.max(weak_logits_U2, 1)\n",
    "\n",
    "        ======== Generate adversarial examples ========\n",
    "        # fix batchnorm ----\n",
    "        partial_eval(m1)\n",
    "        partial_eval(m2)\n",
    "#         m1.eval()\n",
    "#         m2.eval()\n",
    "\n",
    "        #generate adversarial examples ----\n",
    "        weak_adv_data_S1 = adv_generator_1.perturb(X_S1, weak_y_S1)\n",
    "        weak_adv_data_U1 = adv_generator_1.perturb(X_U, weak_pred_U1)\n",
    "        weak_adv_data_S2 = adv_generator_2.perturb(X_S2, weak_y_S2)\n",
    "        weak_adv_data_U2 = adv_generator_2.perturb(X_U, weak_pred_U2)\n",
    "\n",
    "        m1.train()\n",
    "        m2.train()\n",
    "\n",
    "        # predict adversarial examples ----\n",
    "        weak_adv_logits_S1, _ = m1(weak_adv_data_S2)\n",
    "        weak_adv_logits_S2, _ = m2(weak_adv_data_S1)\n",
    "\n",
    "        weak_adv_logits_U1, _ = m1(weak_adv_data_U2)\n",
    "        weak_adv_logits_U2, _ = m2(weak_adv_data_U1)\n",
    "\n",
    "        # ======== calculate the differents loss ========\n",
    "        # zero the parameter gradients ----\n",
    "        optimizer.zero_grad()\n",
    "        m1.zero_grad()\n",
    "        m2.zero_grad()\n",
    "\n",
    "        # losses ----\n",
    "        # L_sup. weak_synth_loss already take care of applyinh the mask on the synth component\n",
    "        weak_l_sup_S1, strong_l_sup_S1, total_l_sup_S1 = weak_synth_loss(weak_logits_S1, strong_logits_S1, weak_y_S1, strong_y_S1)\n",
    "        weak_l_sup_S2, strong_l_sup_S2, total_l_sup_S2 = weak_synth_loss(weak_logits_S2, strong_logits_S2, weak_y_S2, strong_y_S2)\n",
    "        l_sup = total_l_sup_S1 + total_l_sup_S2\n",
    "        \n",
    "        # L_cot. Apply on both weak and strong prediction.\n",
    "        # Mask is not needed since there is no label used\n",
    "        weak_l_cot = loss_cot(weak_logits_U1, weak_logits_U2)\n",
    "        strong_l_cot = loss_cot(strong_logits_U1, strong_logits_U2)\n",
    "        l_cot = weak_l_cot + strong_l_cot\n",
    "        \n",
    "        # L_diff. Since the adversarial sample are generated using the weak prediction, then L_diff is compute only using weak anotation\n",
    "        pld_S, pld_U, l_diff = p_loss_diff(\n",
    "            weak_logits_S1, weak_logits_S2, weak_adv_logits_S1, weak_adv_logits_S2,\n",
    "            weak_logits_U1, weak_logits_U2, weak_adv_logits_U1, weak_adv_logits_U2\n",
    "        )\n",
    "        \n",
    "        total_loss = l_sup + lambda_cot() * l_cot # + lambda_diff() * l_diff\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ======== Calc the metrics ========\n",
    "        with torch.no_grad():\n",
    "            # accuracies ----\n",
    "\n",
    "            weak_f1_S1 = weak_fscores[0](weak_logits_S1, weak_y_S1)\n",
    "            weak_f1_S2 = weak_fscores[1](weak_logits_S2, weak_y_S2)\n",
    "            strong_f1_S1 = strong_fscores[0](strong_logits_S1, strong_y_S1)\n",
    "            strong_f1_S2 = strong_fscores[1](strong_logits_S2, strong_y_S2)\n",
    "\n",
    "            # ratios  ----\n",
    "            weak_adv_pred_S1 = binarize(weak_adv_logits_S1, apply_sigmoid=True)\n",
    "            weak_adv_pred_S2 = binarize(weak_adv_logits_S2, apply_sigmoid=True)\n",
    "\n",
    "            ratio_S1 = ratioS[0](weak_adv_pred_S1, weak_y_S1)\n",
    "            ratio_S2 = ratioS[1](weak_adv_pred_S2, weak_y_S2)\n",
    "            # ========\n",
    "\n",
    "            avg_total_loss = avg_losses[\"total\"](total_loss.item())\n",
    "            avg_ls = avg_losses[\"l_sup\"](l_sup.item())\n",
    "            avg_lc = avg_losses[\"l_cot\"](l_cot.item())\n",
    "            avg_ld = avg_losses[\"l_diff\"](l_diff.item())\n",
    "            \n",
    "            # print statistics\n",
    "            print(train_form.format(\n",
    "                \"Training: \",\n",
    "                epoch,\n",
    "                \"%d %%\" % int((batch / nb_batch) * 100),\n",
    "                \"\", avg_ls, avg_lc, avg_ld, avg_total_loss,\n",
    "#                 \"\", avg_ls, avg_lc, 0.0, avg_total_loss,\n",
    "                \"\", weak_f1_S1, strong_f1_S1,\n",
    "                time.time() - start_time,\n",
    "            ), end=\"\\r\")\n",
    "\n",
    "    # using tensorboard to monitor loss and acc\\n\",\n",
    "    tensorboard.add_scalar('train/total_loss', total_loss.item(), epoch)\n",
    "    tensorboard.add_scalar('train/Lsup', l_sup.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Lcot', l_cot.item(), epoch )\n",
    "    tensorboard.add_scalar('train/Ldiff', l_diff.item(), epoch )\n",
    "    tensorboard.add_scalar(\"train/weak_f1_S1\", weak_f1_S1, epoch )\n",
    "    tensorboard.add_scalar(\"train/weak_f1_S2\", weak_f1_S2, epoch )\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_loss/Lsup_S1\", total_l_sup_S1.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Lsup_S2\", total_l_sup_S2.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Ldiff_S\", pld_S.item(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_loss/Ldiff_U\", pld_U.item(), epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_acc/weak_f1_S1\", weak_f1_S1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_acc/weak_f1_S2\", weak_f1_S2, epoch)\n",
    "\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_S1\", ratio_S1, epoch)\n",
    "    tensorboard.add_scalar(\"detail_ratio/ratio_S2\", ratio_S2, epoch)\n",
    "    \n",
    "    # Return the total loss to check for NaN\n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    \n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    \n",
    "    reset_all_metrics()\n",
    "    print(\"\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(val_loader):\n",
    "            X = X.cuda().float()\n",
    "            weak_y = y[0].cuda().float()\n",
    "            strong_y = y[1].cuda().float()\n",
    "\n",
    "            weak_logits_1, strong_logits_1 = m1(X)\n",
    "            weak_logits_2, strong_logits_2 = m2(X)\n",
    "            \n",
    "            weak_f1_S1 = weak_fscores[0](weak_logits_1, weak_y)\n",
    "            weak_f1_S2 = weak_fscores[1](weak_logits_2, weak_y)\n",
    "            \n",
    "            strong_f1_S1 = strong_fscores[0](strong_logits_1, strong_y)\n",
    "            strong_f1_S2 = strong_fscores[1](strong_logits_2, strong_y)\n",
    "\n",
    "            # print statistics\n",
    "            print(val_form.format(\n",
    "                \"Validation: \",\n",
    "                epoch,\n",
    "                int((batch / len(val_loader)) * 100),\n",
    "                \"\", 0.0, 0.0, 0.0, 0.0,\n",
    "                \"\", weak_f1_S1, strong_f1_S1,\n",
    "                time.time() - start_time,\n",
    "            ), end=\"\\r\")\n",
    "    \n",
    "    tensorboard.add_scalar(\"val/weak_f1_S1\", weak_f1_S1, epoch)\n",
    "    tensorboard.add_scalar(\"val/weak_f1_S2\", weak_f1_S2, epoch)\n",
    "    tensorboard.add_scalar(\"val/strong_f1_S1\", strong_f1_S1, epoch)\n",
    "    tensorboard.add_scalar(\"val/strong_f1_S2\", strong_f1_S2, epoch)\n",
    "    \n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_cot\", lambda_cot(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/lambda_diff\", lambda_diff(), epoch)\n",
    "    tensorboard.add_scalar(\"detail_hyperparameters/learning_rate\", get_lr(optimizer), epoch)\n",
    "\n",
    "    # Apply callbacks\n",
    "    for c in callbacks:\n",
    "        c.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(header)\n",
    "for epoch in range(0, args.epochs):\n",
    "    total_loss = train(epoch)\n",
    "    \n",
    "    if np.isnan(total_loss):\n",
    "        print(\"Losses are NaN, stoping the training here\")\n",
    "        break\n",
    "        \n",
    "    test(epoch)\n",
    "\n",
    "# tensorboard.export_scalars_to_json('./' + args.tensorboard_dir + 'output.json')\n",
    "# tensorboard.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫♪.ılılıll|̲̅̅●̲̅̅|̲̅̅=̲̅̅|̲̅̅●̲̅̅|llılılı.♫♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
